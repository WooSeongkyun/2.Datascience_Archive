
## 신경망 언어모델  NNLM(Neural Network Language Model)
- 과거엔 기계가 자연어를 학습하게 하는 방법으로 통계적 방법론을 사용하였으나, 최근 인공 신경망을 사용하는 방법이 자연어처리에서 더 좋은 성능을 얻고 있다. 번역기, 음성 인식등과 같은 자연어 생성NLG(Natural Language Generation) 기반 언어 모델또한 마찬가지이다.
- 신경망 언어 모델의 시초는 피드포워드 신경망 언어모델Feed Foward Neural Network Language이다. 

### 기존 N-gram 언어모델의 한계
- 언어모델은 문장에 확률을 할당하는 모델이며, 주어진 문맥으로부터 아직 모르는 단어를 예측하는 언어 모델링이다. 
- 예: An adorable little boy is spreading ()
	- 4- gram 모델일 경우 앞의 3개의 단어를 참고, 즉 'boy is spreading'을 참고한다
	- $P(w|'boy \,\, is \,\, spreading')=\displaystyle\frac{count('boy \,\, is \,\, spreading'+w)}{count('boy \,\, is \,\, spreading')}$
- 이러한 n-gram 모델은 충분한 데이터를 관측하지 못하면 언어를 정확히 모델링 하지 못하는 희소 문제 sparsity problem이 존재한다. 예를 들어 'boy is spreading smile' 이란 문장은 현실에서 존재가능한 표현이나, 이러한 문장에 대한 훈련데이터가 존재하지 않는다면 이 문장에 대한 예측 확률값은 0일수밖에 없다

### 단어의 의미적 유사성
- 기계가 단어의 유사성을 활용한다면 어느정도 희소문제를 줄일 수 있다. 
	- 예: '톺아보다'는 '샅샅이 살펴보다'와 유사한 의미를 갖는 단어이다
		- $P('톺아보다'|'발표 \,\,자료를')$ 과 $P('냠냠하다'|'발표 \,\,자료를')$ 를 비교하자. 사람이라면 '살펴보다'와 '톺아보다'의 단어의 유사성을 근거로 '톺아보다'를 더 맞는 선택이라고 판단할 것이다. 
		- 기존의 n-gram 언어 모델은 단어의 유사도를 고려하지 못하지만, 이러한 아이디어를 반영한 것이 NNLM과 단어 벡터간 유사도를 구할 수 있는 벡터를 얻어내는 워드 word embedding의 아이디어 이다


### 피드 포워드 신경망 언어 모델 NNLM
- 예: "what will the fast cat sit on"
	- 훈련 과정에서는 'what will the fat cat'이라는 단어 시퀀스가 입력으로 주어지면, 다음 단어 'sit'을 예측하는 방식으로 훈련된다
	- 훈련 코퍼스가 준비된 상태에서 가장 먼저 해야 할 일은 기계가 단어를 인식할 수 있도록 모든 단어를 수치화하는 것이다. 훈련 코퍼스에 7개의 단어만 존재한다고 가정했을 때 위 단어들을 다음과 같이 원-핫 인코딩 할 수 있다
	```python
what = [1, 0, 0, 0, 0, 0, 0] 
will = [0, 1, 0, 0, 0, 0, 0]
the = [0, 0, 1, 0, 0, 0, 0]
fat = [0, 0, 0, 1, 0, 0, 0]
cat = [0, 0, 0, 0, 1, 0, 0]
sit = [0, 0, 0, 0, 0, 1, 0]
on = [0, 0, 0, 0, 0, 0, 1]
```
	- NNLM은 보통
		- 입력층 Input Layer
		- 투사층 Projection Layer(linear)
		- 은닉층 Hidden Layer(nonlinear)
		- 출력층Out Layer
		- 의 4층으로 이루어진 인공 신경망이다. 이때 투사층은 일반 은닉층과는 다르게 가중치행렬과의 행렬곱은 이루어지지만 활성화함수를 거치지 않는다.
	- ![](Pasted%20image%2020220828124218.png)
		- 단어 집합의 크기를 $V$, 투사층의 크기를 $M$ 이라 하자 그런 경우 가중치 행렬의 크기는 $W \in \mathbf{M}(V \times M)$ 이 된다. 
		- 이때 이 연산에 사용되는 데이터셋의 크기가 $N$ 이라 하자 그러면,  데이터행렬 크기는 $X \in\mathbf{M}(N \times V)$ 가 된다. 각 행이 원-핫 벡터로 이루어진 이 데이터 행렬 $X$ 는,   $X$ 의 $j$ 번째 행과 와 $W$ 를 곱했을 때 $W$의  $j$ 번째 행이 출력된다. 이런 결과 때문에 이 작업을 lookup table이라고 부른다
		- 이 과정을 거쳐 변환된 벡터를 임베딩 벡터 embedding vector라고 부르며 다음과 같은 특징을 갖는다
			- 실수로 구성되어 있다
			- 원 핫 인코딩에 비해 훨씬 작은 공간을 갖는다
		- 투사층에서 모든 임베딩 벡터의 값들을 연결한다. 예를 들어 5차원 벡터 4개를 연결한다면 20차원 벡터 1개를 만드는 형식이다
		- 은닉층을 지나 활성화 층을 지난다.
			- 은닉층: $h^{layer}=tanh(W_hp^{layer}+b-h)$
		- 활성화 층에선 활성화 함수로 소프트맥스함수를 활용하는데, 이때 벡터의 각 원소는 0과 1사이 실수값을 가지며 총 합이 1이 되는 상태가 된다
			- 손실 함수로 크로스 엔트로피(cross-entropy) 함수를 사용한다
			- 출력층: $\hat{y}=softmax(W_yh^{layer}+b_y)$

![피드포워드](피드포워드.png)

- 










> 레퍼런스: 다양체 가설 Manifold hypothesis
- 높은 차원에 존재하는 데이터의 경우 실제로는 해당 데이터를 아우르는 더 낮은 차원의 다양체 역시 존재한다는 가설이 있다고 함
- 뭔 말인지 잘 모르겠는데, 데이터의 손실없이 차원을 얼마나 축소시킬 수 있는지를 이야기하는거 같음..?