## 머신러닝의 기초
- 데이터 준비하기
	- 머신 러닝을 위한 데이터를 준비했다면 기계를 학습하기 전 해당 데이터를 훈련용, 검증용, 테스트용 이렇게 세 가지로 분리하는 것이 일반적임  
	  - 훈련 데이터: 머신 러닝 모델을 학습하는 용도  
	  - 테스트 데이터: 학습한 머신 러닝 모델의 성능을 평가하기 위한 용도  
	  - 검증용 데이터: 모델의 성능을 평가하기 위한 용도가 아니라 모델의 성능을 조정하기 위한 용도. 즉, 모델이 훈련 데이터에 과적합(overfitting) 이 되고 있는지 판단하거나 하이퍼파라미터의 조정을 위한 용도  
- 모델 관련 용어
	- 하이퍼파라미터(초매개변수) : 모델의 성능에 영향을 주는 사람이 값을 지정하는 변수.  
	-  매개변수 : 가중치와 편향. 학습을 하는 동안 값이 계속해서 변하는 수.  
		-  하이퍼파라미터는 보통 사용자가 직접 정해줄 수 있는 변수임 (뒤의 선형 회귀에서 배우게 되는 경사 하강법에서 학습률(learning rate)이나, 딥 러닝에서 뉴런의 수나 층의 수와 같은 것들이 대표적인 하이퍼파라미터)  
	    -  반면, 가중치와 편향과 같은 매개변수는 사용자가 결정해주는 값이 아니라 모델이 학습하는 과정에서 얻어지는 값(훈련용 데이터로 훈련을 모두 시킨 모델은 검증용 데이터를 사용하여 정확도를 검증하며 하이퍼파라미터를 튜닝(tuning) - 검증용 데이터에 대해서 높은 정확도를 얻도록 하이퍼파라미터의 값을 바꿔 정확도를 높이는 방향으로 점차적으로 수정됨)  


## 머신러닝의 구분(종속변수가 연속형인가 이산형인가?) 

### 분류(Classification)와 회귀(Regression)  
- 머신 러닝의 많은 문제는 분류 또는 회귀 문제에 속함  
- 머신 러닝 기법 중 선형 회귀(Lineare Regression)과 로지스틱 회귀(Logistic Rgression)를 다룸 (선형 회귀는 대표적인 회귀 문제에 속하고, 로지스틱 회귀는 (이름은 회귀이지만) 대표적인 분류 문제에 속함)  
- 분류는 또한 이진 분류(Binary Classification)과 다중 클래스 분류(Multi-Class Classification)로 나뉨(엄밀히는 다중 레이블 분류(Multi-lable Classification)라는 또 다른 문제가 존재하지만, 이 책에서는 이진 분류와 다중 클래스 분류만을 다룸)  
  
- 이진 분류 문제(Binary Classification)
	- 이진 분류는 주어진 입력에 대해서 두 개의 선택지 중 하나의 답을 선택해야 하는 경우  
	- 예
	  - 종합 시험 성적표를 보고 최종적으로 합격, 불합격인지 판단하는 문제,   
	  - 메일을 보고나서 정상 메일, 스팸 메일인지를 판단하는 문제 등  
  
- 다중 클래스 분류(Multi-class Classification)  
	- 주어진 입력에 대해서 세 개 이상의 선택지 중에서 답을 선택해야 하는 경우  
	- 예 
	  - 과학, 영어, IT, 학습지, 만화라는 레이블이 붙어있는 5개의 책장에 새 책이 입고되면, 이 책은 다섯 개의 책장 중에서 분야에 맞는 적절한 책장에 책을 넣어야 함  
	  
-  회귀 문제(Regression)  
	- 어떠한 연속적인 값의 범위 내에서 예측값이 나오는 경우  
	- 예:  
	  - 역과의 거리, 인구 밀도, 방의 개수 등을 입력하면 부동산 가격을 예측하는 머신 러닝 모델  
	      - 머신 러닝 모델이 부동산 가격을 7억 8,456만 3,450원으로 예측하는 경우도 있을 것이고, 8억 1257만 300원으로 예측하는 경우도 있을 수 있음(특정 값의 범위 내에서는 어떤 숫자도 나올 수 있음)  
	  - 시계열 데이터(Time Series Data)를 이용한 주가 예측, 생산량 예측, 지수 예측 등


  ## 머신러닝의 구분(레이블 데이터가 있는가 없는가?)
  
  ### 지도 학습과 비지도 학습
- 머신 러닝은 크게 지도 학습, 비지도 학습, 강화 학습으로 나눔  
- 강화 학습은 이 책의 범위를 벗어나므로 설명하지 않음  
- 큰 갈래로서는 자주 언급 되지는 않지만 딥 러닝 자연어 처리에서 중요한 학습 방법 중 하나인 자기지도 학습(Self-Supervised Learning, SSL)에 대해서도 언급할 것임  
  
- 지도 학습(Supervised Learning)
	- 지도 학습이란 레이블(Label)이라는 정답과 함께 학습하는 것  
	- 자연어 처리는 대부분 지도 학습에 속함  
	- 자연어 처리의 많은 문제들은 레이블이 존재하는 경우가 많기 때문  
	  - 레이블이라는 말 외에도 y, 실제값 등으로 부름  
	  - 기계는 예측값과 실제값의 차이인 오차를 줄이는 방식으로 학습을 하게 되는데 예측값은 ^y 과 같이 표현하기도 함  
	  
- 비지도 학습(Unsupervised Learning)  
	- 비지도 학습은 데이터에 별도의 레이블이 없이 학습하는 것  
	- 예 
	    - 텍스트 처리 분야의 토픽 모델링 알고리즘인 LSA나 LDA는 비지도 학습에 속함  
		
- 자기지도 학습(Self-Supervised Learning, SSL)  
	- 레이블이 없는 데이터가 주어지면, 모델이 학습을 위해서 스스로 데이터로부터 레이블을 만들어서 학습  
	- 예:  
	  - Word2Vec과 같은 워드 임베딩 알고리즘이나,   
	  - BERT와 같은 언어 모델의 학습 방법


## 평가 척도
- 머신 러닝에서는 다음과 같은 네 가지 케이스에 대해서 각각 TP, FP, FN, TN을 정의  
  - True Positive(TP) : 실제 True인 정답을 True라고 예측 (정답)  
  - False Positive(FP) : 실제 False인 정답을 True라고 예측 (오답)  
  - False Negative(FN) : 실제 True인 정답을 False라고 예측 (오답)  
  - True Negative(TN) : 실제 False인 정답을 False라고 예측 (정답)  
	-  이 개념을 사용하여 정밀도(Precision)와 재현율(Recall)을 구함  
  
- 정밀도(Precision)  
	- 정밀도란 모델이 True라고 분류한 것 중에서 실제 True인 것의 비율  
	- 정밀도 = $\displaystyle\frac{TP}{TP + FP}$
  
- 재현율(Recall)  
	- 재현율이란 실제 True인 것 중에서 모델이 True라고 예측한 것의 비율  
	- 재현율 = $\displaystyle\frac{TP}{TP + FN}$  
	- Precision이나 Recall은 모두 실제 True인 정답을 모델이 True라고 예측한 경우. 즉, TP에 관심이 있음 (두 식 모두 분자가 TP)  
  
- 정확도(Accuracy)  
	- 정확도(Accuracy)는 우리가 일반적으로 실생활에서도 가장 많이 사용하는 지표임  
	- 전체 예측한 데이터 중에서 정답을 맞춘 것에 대한 비율  
	- 정확도 = $\displaystyle\frac{TP + TN}{TP + FN + FP + TN}$
   
- 그런데 Accuracy로 성능을 예측하는 것이 적절하지 않은 때가 있음  
- 예 
	- 비가 오는 날을 예측하는 모델   
		 - 200일 동안 총 6일만 비가 왔음  
	      - 이 모델은 200일 내내 날씨가 맑았다고 예측  
	      - 200번 중 총 6회 틀렸으므로 194/200=0.97이므로 정확도는 97%  
	      - 하지만 정작 비가 온 날은 하나도 못 맞춘 셈  
	  - 스팸 메일 분류기  
	      - 메일 100개 중 스팸 메일은 5개  
	      - 모두 정상 메일이라고 해도 정확도는 95%  
	      - 정작 스팸 메일은 하나도 못 찾아낸 셈  
- 실질적으로 더 중요한 경우에 대한 데이터가 전체 데이터에서 너무 적은 비율을 차지한다면 정확도는 좋은 측정 지표가 될 수 없음  
- 이런 경우에는 F1-Score를 사용 (이에 대해서는 개체명 인식 챕터에서 설명)


### 과적합과 과소적합
-  과적합(Overfitting)과 과소 적합(Underfitting)  
	- 머신 러닝에서 과적합(Overfitting) 이란 훈련 데이터를 과하게 학습한 경우  
	- 훈련 데이터에 대해서만 과하게 학습하면 성능 측정을 위한 데이터인 테스트 데이터나 실제 서비스에서는 정확도가 좋지 않은 현상이 발생  
	- 과적합 상황에서는 훈련 데이터에 대해서는 오차가 낮지만, 테스트 데이터에 대해서는 오차가 커짐  
	- 아래의 그래프는 훈련 횟수에 따른 훈련 데이터의 오차와 테스트 데이터의 오차(또는 손실이라고도 부름)의 변화를 보여줌


## 선형회귀의 종류

- 회귀란 무엇인가?
    - 정량적인 종속변수를 예측하는데 쓰이는 도구이다.
    - 현대 통계학습중 일반화된 선형회귀법이 많은 부분을 차지한다
- 정의
    - 독립변수 $X_1,X_2,...,X_n$과 종속변수 $Y$, 계수 $\beta_0,\beta_1,...,\beta_n$, 오차항 $\epsilon_1,\epsilon_2,...,\epsilon_n$이 있다하자
    - 다음과 같은 가정이 성립한다 하자
        - 비확률변수$nonstochastic$ :독립변수 $X_1,X_2,...,X_n$는 비확률변수이다(상수취급한다)
        - 선형성$linearity$: 계수와 종속변수 사이 선형성을 갖는다
        - 독립성:$independence$ 독립변수가 다른 독립변수에 영향을 주지 않는다
        - 등분산성$homoskedasticity$: $Var(\epsilon_i)=\sigma^2$$\,\,for\,\,i=1,2,...$
        - 정규성$normality$: $\epsilon\sim \mathcal{N}(0,\sigma^2)$
        - $i\ne j$일때 $Cov(\epsilon_i,\epsilon_j)=0$ 이다
- 정의
    - 모델형태가  $Y=\sum_{i=1}^{n}\beta_{i}X_i+\beta_0+\epsilon$ 이라고 가정하자. 그 계수값 $\beta_0,\beta_1,\beta_2,...,\beta_n$ 을 구한뒤, 해당 모델로 종속변수를 예측하는 일련의 방법론을 선형회귀라고 부른다
	
- 단순 선형 회귀 분석(Simple Linear Regression Analysis)  
	- $y = wx + b$
	- 위의 수식은 단순 선형 회귀의 수식  
	- 독립 변수 $x$와 곱해지는 값 $w$를 머신 러닝에서는 가중치(weight), 별도로 더해지는 값 $b$를 편향(bias)이라고 함  
	- 직선의 방정식에서는 각각 직선의 기울기와 절편을 의미  
  
- 다중 선형 회귀 분석(Multiple Linear Regression Analysis)  
	- $y = w_{1}x_{1} + w_{2}x_{2} + ... + w_{n}x_{n} + b$
	- 집의 매매 가격은 단순히 집의 평수가 크다고 결정되는 게 아니라 집의 층의 수, 방의 개수, 지하철 역과의 거리와도 영향이 있음  
	- 다수의 요소를 가지고 집의 매매 가격을 예측  
	- $y$는 여전히 1개이지만 이제 $x$는 1개가 아니라 여러 개임  
	- -> 다중 선형 회귀 분석이라 함


## 목적함수와 옵티마이저
- 비용 함수(Cost function) : 평균 제곱 오차(MSE)  
	- 머신 러닝은 w와 b를 찾기 위해서 실제값과 가설로부터 얻은 예측값의 오차를 계산하는 식을 세우고, 이 식의 값을 최소화하는 최적의 w와 b를 찾아낸다
	- 이때 실제값과 예측값에 대한 오차에 대한 식을 목적 함수(Objective function) 또는 비용 함수(Cost function) 또는 손실 함수(Loss function) 라고 한다
	  - 함수의 값을 최소화하거나, 최대화하거나 하는 목적을 가진 함수를 목적 함수(Objective function)라고 한다  
	  - 값을 최소화하려고 하면 이를 비용 함수(Cost function) 또는 손실 함수(Loss function)라고 한다  
	- 비용 함수는 단순히 실제값과 예측값에 대한 오차를 표현하면 되는 것이 아니라, 예측값의 오차를 줄이는 일에 최적화 된 식이어야 한다 
	  - 머신러닝, 딥 러닝에는 다양한 문제들이 있고, 각 문제들에는 적합한 비용 함수들이 있다  
	  - 회귀 문제의 경우에는 주로 평균 제곱 오차(Mean Squared Error, MSE)가 사용된다
	  
-  옵티마이저(Optimizer) : 경사하강법(Gradient Descent)  
	- 선형 회귀를 포함한 수많은 머신 러닝, 딥 러닝의 학습은 결국 비용 함수를 최소화하는 매개 변수인 w와 b을 찾기 위한 작업을 수행한다
	- 이때 사용되는 알고리즘을 옵티마이저(Optimizer) 또는 최적화 알고리즘이라 한다  
	- 옵티마이저를 통해 적절한 w와 b를 찾아내는 과정을 머신 러닝에서 훈련(training) 또는 학습(learning)이라고 부른다 
	- 기본적인 옵티마이저 알고리즘인 경사 하강법(Gradient Descent)이 있다  
	- 경사 하강법을 이해하기 위해서 cost와 기울기 w와의 관계를 이해해야 한다
	- w는 머신 러닝 용어로는 가중치라고 불리지만, 직선의 방정식 관점에서 보면 직선의 기울기를 의미 한다 
	- 아래의 그래프는 기울기 w가 지나치게 높거나, 낮을 떄 오차가 커지는 모습을 보여준다