## RNN 언어모델 RNNLM(Recurrent Neural Network Language Model)
- 시점 time step의 개념이 도입된 RNN으로 언어모델을 만들면 입력 길이를 고정하지 않아도 된다. 이렇게 RNN으로 만든 언어 모델을 RNNLM이라고 한다
- 예
	- 'what will the fat cat sit on'
	- ![](Pasted%20image%2020220828224839.png)
	- 테스트 동안
		- RNNLM은 기본적으로 예측과정에서 이전 시점의 출력을 현재 시점의 입력으로 활용한다. 
		- what을 입력한다면 will을 예측하고, 이 will은 다음 시점의 입력이 되어 the를 예측한다
		- 결과적으로 세번째 시점에서 fat은 앞서 나온 what, will, the라는 시퀀스로 인해 결정된 단어이며, 네번째 시점의 cat은 앞서 나온 what, will, the, fat이라는 시퀀스로 인해 결정된 단어이다
	- 훈련 동안
		-  what will the fat cat sit on라는 훈련 샘플이 있다면, what will the fat cat sit 시퀀스를 모델의 입력으로 넣으면, will the fat cat sit on를 예측하도록 훈련된다
		- 교사 강요 teacher forcing 
			- 모델이 t 시점에서 예측한 값을 t+1 시점에 입력으로 사용하지 않고, t 시점의 레이블. 즉, 실제 알고있는 정답을 t+1 시점의 입력으로 사용하는 기법
			- 훈련 과정에서도 이전 시점의 출력을 다음 시점의 입력으로 사용하면서 훈련 시킬 수도 있지만 이는 한 번 잘못 예측하면 뒤에서의 예측까지 영향을 미쳐 훈련 시간이 느려지게 되므로 교사 강요를 사용하여 RNN을 좀 더 빠르고 효과적으로 훈련시킬 수 있다
	- 훈련 과정
		- 현 시점의 입력 단어의 원-핫 벡터 xt를 입력 받은 RNNLM은 우선 임베딩층(embedding layer)을 지난다
		- 단어 집합의 크기가 V일 때, 임베딩 벡터의 크기를 M으로 설정하면, 각 입력 단어들은 임베딩층에서 V × M 크기의 임베딩 행렬과 곱해진다. 여기서 V는 단어 집합의 크기를 의미합니다. 만약 원-핫 벡터의 차원이 7이고, M이 5라면 임베딩 행렬은 7 × 5 행렬이 된다
		- 임베딩 벡터는 은닉층에서 이전 시점의 은닉 상태인 $h_{t−1}$과 함께 다음의 연산을 하여 현재 시점의 은닉 상태 $h_t$를 계산하게 된다
		- 은닉층 : $h_t=tanh(W_xe_t+W_hh_{t−1}+b)$
		- 출력층 : $\hat{y}_t=softmax(W_yh_t+b)$
		- 룩업 테이블의 대상이 되는 테이블인 임베딩 행렬을 $E$ 라고 하였을 때, 결과적으로 RNNLM에서 학습 과정에서 학습되는 가중치 행렬은 다음의 $E,W_x,W_h,W_y$ 4개 이다
		