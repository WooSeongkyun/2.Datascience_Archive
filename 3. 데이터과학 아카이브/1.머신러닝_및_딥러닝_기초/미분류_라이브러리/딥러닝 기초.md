
## 1. 데이터를 텐서로  표현하기
- 다양한 예들
	- 주식가격 데이터 
		- (numbers of recorded dates, minutes of one trading day, cases of price:3)
		- 1분마다의 현재 주식가격- 최고가, 최소가,현재가를 저장한다. 하루의 거래시간은 390분이므로 하루에 발생되는 데이터 크기는 (390,3)이며, 1년치인경우 (365,390,3)이다
	- 트윗 데이터
		- (number of tweets ,size of a tweet,size of alphabets set)
		- 128개의 알파벳으로 이루어진 280개의 문자 시퀀스이다. 따라서 하나의 트윗은 (280,128)의 텐서이며, 100만개의 트윗으로 구성된 데이터셋이라면 (1000000,280,128)이다
	- 256* 256 이미지
		- (samples,height,width,color_depth)
		- 흑백의 경우 (128,256,256,1)의 텐서를
		- 컬러의 경우 (128,256,256,3) 크기 텐서에 저장될 수 있다(아마 128는 고정된 값은 아닌거 같음)
	- 동영상 데이터
		- (samples,frames,height,width,depth)
		
## 2. 신경망의 구조

### 1. 신경망의 요소
- 네트워크를 구성하는 층 layer
- 입력nput과 그에 상응하는 타겟target
- 학습 피드백에 사용될 손실함수 loss function
- 학습 진행 방식을 결정하는 옵티마이저 optimizer

### 2. 층: 딥러닝의 구성 단위
- 하나 이상의 텐서를 입력으로 받아 하나 이상의 텐서를 출력하는 데이터 처리 모듈
- 각 데이터 및 텐서 형식마다 적절한 층이 존재한다
	- 단순한 2D 벡터 데이터는 완전 연결층 fully connected layer 혹은 밀집층 dense layer
	- 시퀀스 3D 데이터는 LSTM과 같은 순환층 recurrent layer
	- 4D 텐서로 구성된 이미지 데이터는 2D 합성곱 층 convolution layer에 의해 주로 처리된다

### 3. 손실함수와 옵티마이저: 학습과정의 키
- 손실함수 loss function: 훈련 과정동안 최소화될 값으로서 제시되는 지표
- 옵티마이저: 손실함수를 기반으로 네트워크가 어떻게 업데이트 될지 결정하는 알고리즘


### 4. 신경망의 엔진: 그래디언트 기반 최적화
- 기초적인 신경망 구조
	- `output=relu(dot(W,input)+b)`
	- 여기서 `W` 는 가중지weight로 훈련되는 파라미터trainable parameter라고 불린다
	- `b`는 편향bias라고 불린다
	- 초기 `W` 가중치행렬은 작은 난수로 이루어져 있다
- 훈련 반복 루프 training loop
	1. 훈련 샘플 X와 이에 상응하는 타깃 y의 배치를 추출한다
	2. X를 사용하여 네트워크를 실행하고, 예측 y_pred를 구한다
	3. y_pred와 y의 차이를 측정하여, 이 배치에 대한 네트워크의 손실을 계산한다
	4. 배치에 대한 손실이 조금 감소하도록 네트워크의 모든 가중치를 업데이트 한다
	 - 4번 가중치를 얼만큼 업데이트할 것인지는 어려운 문제이다. 관심있는 독립변수 하나만 값을 변동시키고, 나머지 값들을 고정시키면서 손실함수값의 변화를 살펴보는 방법도 있지만, 이는 엄청난 양의 계산을 요하므로 비효율적이다. 그에 비해 gradient를 계산하는 것이 훨씬 좋은 방법이다

### 경사하강법의 정의
- $x_{i,j}= x_{i,j+1} - \alpha \nabla f(x_{i,j})$=$x_{i,j+1}- \alpha\cfrac{\partial{f}}{\partial{\textbf{x}}}$  
- $i$ 번째 변수의 $j$번째 값을 갱신할때 다음과 같은 공식을 활용한다. 이때 $\alpha$는 hyperparameter이다  
- 비용함수에서의 경사하강법  
	- $J(\theta)= \cfrac{1}{N}*\sum_{i}(y_i-x_i\theta+\phi_i)^2$ 
	- $\cfrac{\partial{J}}{\partial{\theta}}=\cfrac{2}{N}(y_i-\theta)$
	- $\theta_{i,j+1}= \theta_{i,j}-\alpha\cfrac{2}{N}\cdot\sum_{i}x_i(y_i-x_i\theta_i)$
	- $\phi_{i,j+1}= \phi_{i,j}-\alpha\cfrac{2}{N}\cdot\sum_{i}(y_i-x_i\theta_i)$

### 확률적 경사하강법
- 확률적stochastic이 붙은 이유: 각 배치 데이터를 무작위로 선택하였기 때문이다

### **그외의 최적화 방법: 옵티마이저optimizer**
- 손실함수를 최소화하기 위하여 업데이트할 가중치를 계산할 때 사용되는 알고리즘
![옵티마이저](옵티마이저.png)


### 5. Epoch와 Batch size
- epoch
	 - 전체 train set이 학습하는데 활용된 횟수
- batch size
	- train set을 여러개의 작은 그룹으로 쪼개었을때, 하나의 작은 그룹(batch)에 속하는 데이터의 수
- iteration
	- 하나의 epoch가 시행되기 위해서 필요한 batch의 수 
	- 하나의 epoch 가 마칠때까지 시행되는 파라미터 업데이트의 수

### 6. 텐서연산
- `keras.layer.Dense(512,activation='relu')` 의 의미
	- `output=relu(dot(W,input)+b)` 
	- `relu`는 연소별 연산element-wise operation을 지원한다



### 3. 활성화 함수
#### 1. ReLU: Rectified Linear Unit
- 정의
$$

\begin{equation*}
f(x)=\begin{cases}
          0\,\,\,\, if\,\,x<0 \\
          x\,\,\,\, if\,\,x\ge0 
     \end{cases}
\end{equation*}
=\,\,max(x,0)
$$

- ReLu의 특징
	- Sparase activation: 0 이하의 입력에 대해 0으로 출력함으로서 부분적으로 활성화시킬 수 있다
	- Efficient gradient propagation: gradient의 vanishing이 없으며, gradient가 exploding이 되지 않는다
	- Efficient computation:선형함수이므로 미분이 간단하다

