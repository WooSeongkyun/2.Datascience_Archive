### 순방향 신경망의 구조 
- 현대에 들어 다층 퍼셉트론을 순방향 신경망, 퍼셉트론은 인공 뉴런 artificial neuron이라 불린다
	- 순방향 신경모델은 데이터가 한 방향으로 전달되는 순방향feedfoward 연결을 갖는 구조가 되고 있다
- 순방향 신경망의 구조
	- 순방향 신경망의 계층은 입력 계층, 은닉 계층, 출력 계층으로 구분된다
	- 입력 계층은 외부로부터 데이터를 받고, 은닉 계층은 데이터의 특징을 추출하며, 출력 계층은 추출된 특징을 기반으로 추론한 결과를 외부에 출력한다
- 완전 연결 계층과 뉴런의 역할
	- 순방향 신경망은 모든 계층이 완전 연결 계층 fully connected layer로 이루어져 있다. 완전 연결 계층이란 이전 층의 뉴런과 다음층의 모든 뉴런이 연결되어 있는 구조이다. 
	- 각 뉴런은 이전 계층에서 출력한 데이터를 동일하게 전달받기 때문에, 같은 입력 데이터에서 뉴런마다 서로 다른 특징을 추출한다
	- 따라서 데이터에 특징이 많을수록, 뉴런의 수가 충분히 늘어나야 데이터에 내재된 특징을 모두 추출할 수 있다
- 특징을 추출하는 뉴런의 구조
	- 가중 합산: 다른 뉴런의 input에 가중치를 곱하여 합하는 과정이며, 이는 추출할 특특징에 중요한 영향을 미치는 데이터를 선택하는 과정이다
		- 가중치는 특징을 추출할 때 영향이 클 데이터를 선택하는 역할을 한다. 이때 편향 $\mathbf{b}$ 를 활용하는 이유는 특징을 공간상 임의의 위치에 표현하기 위해서이다. 편향이 없다면 특징의 위치는 원점을 지나는 연속함수로 결정되므로, 공간 어느 위치에서든지 존재할 수있게 하려면 원점으로부터 오프셋을 지정해야한다.
	- 활성 함수: 원하는 형태로 특징을 추출하기 위해 데이터를 비선형적으로 변환하는 과정
		- 예) ReLU(Rectified Linear Unit) 함수: 입력값이 0보다 크면 그대로, 0보다 작거나 같으면 0을 출력하는 함수

### 범용 함수 근사기로써의 신경망
- 실함수 real-valued function
	- $f:\mathbb{R}^{n}\to \mathbb{R}$ 형태의 함수를 의미한다
	- 뉴런은 입력과 가중치벡터가 각각 $\mathbf{x}=\begin{bmatrix} x_1,x_2,\cdots,x_{n} \end{bmatrix},\mathbf{\beta}=\begin{bmatrix} \beta_{1},\beta_{2},\cdots,\beta_n \end{bmatrix}$ 으로, 둘을 내적하고 편향 bias b를 더한 가중합산의 결과 $\mathbf{\beta}  ^{T} \mathbf{x} + b$ 가 실함수가 된다
	- 이때 가중합산한 값을 활성함수$activation$ 의 input으로 넣어줌으로서 $f(\mathbf{x})=activation(\mathbf{\beta}  ^{T}\mathbf{x}+\mathbf{b})$ 가 뉴런의 최종 output이 된다
- 벡터함수인 계층 
	- 이전 계층의 뉴런이 $n$ 개이고, 현재 계층의 뉴런이 $m$ 개일때, 입력은 $\mathbf{x}=\begin{bmatrix} x _{1}, x _{2}, \cdots ,x _{n}\end{bmatrix}$ 이고 출력은 $f(x)=\begin{bmatrix} f _{1}(\mathbf{x}), f _{2}(\mathbf{x}) ,\cdots,f _{m}(\mathbf{x}) \end{bmatrix}$ 인 벡터이다
	- 가중치행렬은 다음과 같이 정의된다
		- $\mathbf{\beta}= \begin{bmatrix} \mathbf{\beta }_{1}  & \mathbf{\beta}_{2}  &  \cdots & \mathbf{\beta}_{m} \end{bmatrix}=\begin{bmatrix} \beta _{11} & \beta _{21}  & \cdots  & \beta _{m1} \\ \beta _{12} & \beta _{22}  & \cdots  & \beta _{m2} \\ \cdots & \cdots & \cdots & \cdots \\ \beta _{1n} & \beta _{2n}  & \cdots & \beta _{mn} \end{bmatrix}$ 
		- 이때 $\beta _{ij}$ 에서 $i$번째 output neuron과 j번째 input neuron 를 잇는 연결망의 가중치이다 
	- 계층의 가중합산은 $\mathbf{\beta}  ^{T} \mathbf{x} + \mathbf{b}$ 가 된다
- 신경망의 output
	- 만일 신경망의 계층 수가 $L$ 이라면 다음과 같은 합성함수로 정의된다
		- $\mathbf{y}=f  ^{L}(\cdots f  ^{2}( f  ^{1}(\mathbf{x})))$
	- 신경망은 입력 $\mathbf{x}$ 를 출력 $\mathbf{y}$  로 매핑하는 $\mathbf{y}=f(\mathbf{x};\mathbf{\theta})$ 의 파라미터 함수이며, 이때 $\theta$ 는 뉴런의 가중치와 편향을 포함한 함수의 파라미터이다
	- 신경망은 학습할 때 미분을 사용하기 때문에 신경망이 표현하는 함수는 미분가능한 함수 differentiable function이여야 한다

### 범용 근사정리 Universal Approximation Theorem
- 1개의 은닉층을 갖는 순방향 신경망에서 은닉 뉴런을 충분히 사용하고 검증된 활성함수를 사용한다면 $n$ 차원의 임의의 연속함수를 원하는 정도의 정확도로 근사할 수 있다
- 아주 복잡한 함수를 정확히 근사하려면 은닉 뉴런을 충분히 사용하여야 한다. 그러나 뉴런수가 커지면 과적합이 쉽게 발생하여 모델의 성능이 낮아질 수 있다
	- 이 문제를 위해 증명된 확장된 정리는 은닉 계층의 뉴런 수를 제한하더라도 신경망의 깊이에 제한을 두지 않는 다면 범용 근사가 가능하다는 점이다
	- 따라서 입력이 $n$ 차원이고 $ReLU$ 를 사용할 때 은닉 계층의 뉴런수를 $n+4$ 로 제한해도 르베그 적분가능함수Lebesgue Intergrable Function를 근사할 수 있음이 증명되었다


## 분류와 회귀문제
- 분류 classification
	- 데이터의 클래스class 또는 카테고리category를 예측하는 문제이다
	- 두개 클래스로 분류하는가, 아니면 세개 이상으로 분류하는 가에 따라 이진분류binary classifcation과 다중분류 multiclass classification으로 나눠진다
	- 대상이 어떤 클래스에 속할 것인가, 아니면 각 클래스에 속할 확률이 얼마일 것인가를 예측하는가에 따라 판별 함수 discriminative function과 확률 모델stochastic model로 나뉜다
- 회귀 regression
	- 여러 독립변수와 종속변수의 관계를 연속함수 형태로 분석하는 문제이다


### 이진분류모델
-  $x$ 가 사건이 일어난 횟수, $\mu$ 가 사건이 일어날 확률로 정의하자
	- 베르누이 분포는 $P(x;p)=p  ^{x}(1-p)  ^{1-x}$
- 로지스틱 함수(시그모이드 함수)
	- $\sigma(x)=\displaystyle\frac{1}{1+e^{-x}}$
	- 시그모이드 함수는 입력된 값을 $[0,1]$ 의 고정된 범위로 변환하는 스쿼싱 squashing 함수이다
	- 따라서 신경망의 출력 계층에서 실수값을 확률값으로 변환할 때 주로 사용한다
	- 시그모이드 함수는 S형 함수를 총칭하는 것이나, 보통 아무말이 없을경우 로지스틱함수를 칭하는 것이다
	- 승산비 $odd \,\, ratio=\displaystyle\frac{p}{1-p}$ 로 부터 로짓이 정의되는데 $logit(x)=\log_{}{\displaystyle\frac{x}{1-x}}$ 이다
		- 로지스틱함수를 로짓의 역함수로 정의하는데 이로서 로지스틱 함수가 유도된다 
			- $y=\log_{}{\displaystyle\frac{x}{1-x}}$
			- $e^{y}=\displaystyle\frac{x}{1-x}$
			- $e^{y}(1-x)=x$
			- $e^{y}=(e^{y}+1)x$
			- $x=\displaystyle\frac{e^{y}}{e^{y}+1}=\displaystyle\frac{1}{1+e^{-y}}$
		- $\sigma(x)=\displaystyle\frac{1}{1+e^{-x}}$


### 다중분류모델
- 카테고리 분포는 베르누이 분포를 일반화한 형태로서, 여러 종류의 사건이 발생될 확률을 나타내며 다음과 같이 정의한다.
	- $p(\mathbf{x}|\mathbf{\mu})=\prod_{k=1}^{K}{p _{k} ^{x _{k}}}$
	- $\mathbf{p}=\begin{bmatrix} p _{1},p _{2}, \cdots,p _{K} \end{bmatrix}^{T}, \displaystyle\sum_{k=1}^{K}{\mu _{k}=1}$
	- $\mathbf{x}=\begin{bmatrix} x _{1}, x _{2} , \cdots, x _{K} \end{bmatrix}^{T}\,\,\, ,x_{k}=1 \,\,if \,\,k=i \,\,else \,\,0 \,\,,i \in \{ 1,2,\cdots,K \}$
	- $K$ 개는 사건의 갯수, $p _{k}$ 는 사건 $k$ 가 발생할 확률
- 소프트맥스 함수
	- $softmax(y _{i})=\displaystyle\frac{e^{y _{i}}}{\displaystyle\sum_{j=1}^{K}{e^{y _{j}}}}$
	- 실수벡터를 확률벡터로 변환하는 함수로, 실수벡터의 각 요소를 $[0,1]$ 범위로, 각 요소의 합이 1이 되도록 제한시킨다. 따라서 각 실수벡터를 카테고리의 확률분포의 확률벡터로 변환할 때 사용한다
	- 소프트맥스 함수는 입력값이 클수록 1에 가깝게, 입력값이 작을수록 0에 가깝게 만들어주기 때문에 Max의 같은 역활을 하지만, 부드러운 곡선 형태이므로 이를 합쳐 소프트맥스라 부른다


## 회귀모델
- 회귀모델은 여러 독립변수와 종속 변수의 관계를 연속 함수 형태로 분석하는 방법이다
- 가우시안 분포
	- $N(x|\mu,\sigma  ^{2})=\displaystyle\frac{1}{\sigma \sqrt{2 \pi}}exp[{-\displaystyle\frac{(x-\mu)  ^{2}}{2 \sigma^2}}]$
	- $x$ 는 확률변수, $\mu$ 는 평균, $\sigma  ^{2}$ 는 분산이다. 가우시안 분포는 관측 데이터의 분포를 근사하는 데 자주 쓰인다
- 회귀 모델 정의
	- 관측데이터는 $\mathcal{D}=\{ (\mathbf{x} _{i},t _{i}):1,2,\cdots,N \}$ 로 $N$ 개의 샘플로 이루어져있다고 하자. 
	- 입력데이터 $\mathbf{x} _{i}$ 는 같은 분포에서 독립적으로 샘플링되어 $i.i.d$ 를 만족한다고 하자
	- 타깃 $t _{i}$ 는 모델 예측값 $y(\mathbf{x} _{i},\theta)$ 와 관측 오차 $\epsilon$ 의 차이가 존재하며, 관측오차는 가우시안 분포 $\mathcal{N}(\epsilon|0,\beta  ^{-1})$ 를 따른다. 이때 $\beta  ^{-1}$ 는 정밀도 precision $\beta$ 의 역수이다
		- $t _{i}=y(\mathbf{x} _{i};\theta)+\epsilon$ , $\epsilon \sim \mathcal{N}(\epsilon|0,\beta  ^{-1})$
- 예: 집값 예측하기
	- 방의 개수, 면적, 집 종류, 역과의 거리 등의 데이터를 활용하여 집값을 예측하는 회귀 모델을 만든다고 하자. 우선 집값의 평균인 $\mu$  를 계산한다,그리고 회귀모델에 경우 예측된 평균과 분산이 바뀌면 안되기 위해 항등 함수 identity function을 활성 함수로 활용한다

## 활성함수
- 종류
	- 시그모이드
		- $\sigma(x)=\displaystyle\frac{1}{1+e^{-x}}$
	- 하이퍼볼릭 탄젠트
		- $tanh(x)=\displaystyle\frac{e^{x}-e^{-x}}{e^{x}+e^{x}}$
	- ReLU
		- $f(x)=x \,\,if \,\,x \ge 0 \,\,else \,\,0$
	- Leaky ReLU
		- $f(x)=x \,\,if \,\,x \ge 0 \,\,else \,\,0.01x$
	- maxout
		- $max(\beta _{i}x +b_{i})$
	- ELU
		- $x \,\,if \,\,x \ge 0\,\,else  \,\,a(e^{x}-1)$
- 역사적으로 1984~1986년에  역전파 알고리즘이 등장하면서 시그모이드 계열 활성함수가 등장하였고, 딥러닝 시대에 열린 이후 2011년 부터 ReLU 계열 활성 함수들이 나타나기 시작하였다. ReLU 계열은 선형성을 갖고 있어 연산 속도가 매우 빠르고, 학습 과정을 안정적으로 만들어 주므로 은닉계층에서 이점이 많다
- 시그모이드 계열은 그레디언트 소실의 원인이 되어 신경망 학습에는 좋지 않지만, 값을 고정 범위로 만들어주는 스쿼싱 squashing 기능이 필요한 구조에서 다양하게 활용된다
- 계단함수
	- $f(x)=1 \,\,if \,\,x \ge 0 \,\,else \,\,0$
	- 매킬러 - 피츠 모델과 퍼셉트론에서 활용된 활성 함수로 생체 뉴런의 발화 방식을 모방하여 만든 것이다. 다만 현대의 신경망에서는 활용되지 않는다. 계단 함수는 모든 구간에서 미분값이 0 이기 때문에 역전파 알고리즘에 쓰일 수 없기 때문이다.
	- 역전파 알고리즘을 만들어낼 당시 계단 함수를 대체할 미분가능한 함수가 필요하였고, 그 결과 찾아낸 활성 함수가 부드러운 계단 함수 모양인 시그모이드 함수이다
- 시그모이드 함수
	- $f(x)=\displaystyle\frac{1}{1+e^{-x}}$
	- 모든 구간에서 미분 가능하고, 증가함수이므로 미분값이 항상 양수이다
	- 시그모이드는 오랜 기간동안 활성함수로 활용되었지만 다음과 같은 문제점이 존재한다
		- 함수 정의에 지수 함수가 포함되어 연산 비용이 많이 든다
		- 그레디언트 포화가 발생하여 학습이 중단될 수 있다
		- 양수만 출력하므로 학습이 진동하면서 학습속도가 느려질 수 있다
			- 그레디언트 포화gradient saturation란 시그모이드 함수 끝부분에서 미분값이 0으로 포화되는 상태이다. 포화saturation이란 입력값이 변화해도 함수값이 변화하지 않는 상태를 의미한다. 
- 하이퍼볼릭 탄젠트 함수
	- $f(x)=\displaystyle\frac{e^{x}-e^{-x}}{e^{x}+e^{-x}}$
	- $tanh(x)=2 \sigma(2x)-1$
	- 함수값이 $[-1,1]$ 범위에 있는 S형 함수이다
	- 시그모이드는 항상 양수만을 출력하기 때문에 최적화가 비효율적으로 진행되는 문제를 해결하고자 쓰이기 시작하였다
- $ReLU$ 함수
	- $f(x)=x \,\, if \,\, x \ge 0 \,\,else \,\,0$
	- 0보다 큰 입력이 들어오면 그대로 통과시키고, 0보다 작은 입력이 들어오면 0을 출력하는 함수이다
	- '대뇌 피질의 뉴런은 신호가 커지더라도 포화하지 않기 때문에, 인공 뉴런의 활성 함수도 정류기rectifier로 근사할 수 있다' 는 신경과학 분야의 고찰을 통하여 만들어졌다
	- ReLU는 시그모이드 계열보다 학습속도가 빠르고 안정적으로 학습할 수 있다는 것이 밝혀졌는데 이는 연산이 거의 없기 때문이다. 
	- 하지만 죽은 ReLU 라는 현상, 뉴런이 계속하여 0을 출력하는 상태를 야기할 수도 있다. 이는 가중치 초기화가 잘못되었거나 학습률이 매우 클 때 발생될 수 있는 문제로 뉴런의 10~20%가 죽은 ReLU가 되면 학습에 문제가 생길 수도 있다 
	- 이 문제를 해결하기 위하여 음수 구간에서 0이 되지 않도록 약간의 기울기를 주는데 이를 leaky ReLU라고 한다. 다만 leaky ReLU는 기울기가 고정되어 있기 때문에 최적의 성능을 내지 못할 수 있는데, 기울기를 학습하도록 만든 방식이 PReLU(Parametric ReLU)이다.
- 맥스아웃 함수
	- 맥스아웃은 활성함수를 구간 선형함수로 가정하고, 각 뉴런에 최적화된 활성함수를 학습을 통하여 찾아낸다. 맥스아웃은 ReLU의 일반화된 형태라고 할 수 있으며 성능이 뛰어나다
	- 맥스아웃은 선형노드 갯수에 따라 다른 형태의 볼록함수 convex function을 근사할 수 있다. n개의 선형노드면 n차 함수를 근사할 수 있다.
- Swish 함수
	- $f(x)=x \cdot \sigma(x)$
	- 구글 브레인에서 AutoML로  찾은 최적의 활성함수로 SiLU Sigmoid linear unit으로 불리기도 한다

### 신경망의 크기
- 깊이 depth : layer의 수
- 너비 wdith: layer별 뉴런의 수
