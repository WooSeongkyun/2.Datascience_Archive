- 신경망 학습
	- 신경망 학습엔 입력 데이터와 타깃 데이터만 주어질 뿐, 추론을 위한 규칙은 제공되지 않는다. 따라서 신경망은 그 규칙을 학습 데이터를 활용하여 스스로 찾아내야 한다
	- 학습전엔 신경망의 구조에 대해 정의해두고, 학습 과정에서 모델 파라미터의 값을 찾는다. 마치 생체 신경망에서 뉴런의 형태는 이미 정해져 있지만, 학습에 따라 뉴런의 연결 강도가 달라지는 것과 같이, 인공 신경망또한 학습 과정에서 인공뉴런의 연결강도 즉 모델 파라미터를 조절한다
	- 신경망이 모델의 파라미터값을 찾기 위하여 최적화 기법optimization 을 활용한다. 최적화 기법은 함수의 해를 근사적으로 찾는 방법을 의미한다.
- 최적화
	- 유한한 방정식의 정확한 해를 구할 수 없을 때 근사적으로 해를 구하는 방법이다
	- 다양한 제약 조건을 걸어, 목적 함수를 최대 또는 최소화 하는 해를 반복하여 조금씩 접근해나가는 방식으로 찾는 방법이다
	- 실생활에서 발생하는 대부분의 문제들은 대부분 제약 조건이 많고, 정확히 해를 구하기 어려운 문제들이므로 최적화 문제들로 다루어질 수 있다
	- 정의
		- 표준 최적화 문제는 '변수 $x$ 에 대한 등식과 부등식으로 표현되는 여러 제약조건을 만족하면서 목적 함수인 $f(x)$를 최소화 하는 $x$값을 찾는 문제' 로 이야기할 수 있다
		- 최적화 문제는 목적함수 objective function과 제약조건 constraints 로 나뉜다. 또 제약조건은 부등식형태와 등식형태로 나뉜다.
		- 최적화를 통해 찾은 $x$ 값을 최적해 optimal solution이라고 하며, 최적해에 점점 다가가는 상태를 수렴한다converge고 표현한다.
	- 예 : 필수 영양을 만족하면서 가장 저렴한 식단을 구성하기
		- $min _{x} \,\, \mathbf{c}^{T}\mathbf{x}$ : 식단의 가격 
		- $subject \,\,to$ $\mathbf{Dx} \ge \mathbf{d}$ : 필수 영양을 만족하는가에 대한 조건
		- $\mathbf{x} \ge 0$ 음식의 양
		- $ref$
			- $x _{j}$  : 식단을 구성하는 음식 $j$ 의 양
			- $c _{j}$ 음식 $j$ 의 단위 가격
			- $\mathbf{D} _{ij}$ : 음식 $j$ 의 단위별 영양소 $i$ 의 함유량
			- $\mathbf{d} _{i}$ : 영양소 $i$ 의 최소 섭취량

### 신경망 학습을 위한 최적화 문제 정의
- $J(\theta)=\displaystyle\frac{1}{N}\displaystyle\sum_{i=1}^{N}{\| t _{i}-y(\mathbf{x} _{i};\theta) \|^2}=\displaystyle\frac{1}{N}\langle {\mathbf{y}-\mathbf{\theta} ^{T}\mathbf{x}},{\mathbf{y}-\mathbf{\theta} ^{T}\mathbf{x}} \rangle$
- 분류문제를 최적화 문제로 정의한다면 '관측 확률분포와 예측 확률분포의 차이를 최소화 하는 파라미터를 찾으라'는 최적화 문제로 정의될 수 있다
- $min _{\theta}-\displaystyle\frac{1}{N}\displaystyle\sum_{i=1}^{N}{\displaystyle\sum_{k=1}^{K}{t _{ik}\cdot \log_{}{p(\mathbf{x }_{i}; \theta)_{k}}}}$
	- 분류문제에서 손실함수는 크로스 엔트로피 cross entropy로 정의되며, 타깃의 확률분포와 모델의 예측 확률분포 사이의 차이를 나타낸다. 기본 분류 문제의 경우 제약 조건은 없지만 정규화 기법이 적용되거나 문제가 확장되면 제약 조건이 추가될 수 있다
- 최적화를 통한 신경망 학습
	- 최적화 문제가 정의되었다면, 최적화를 통하여 신경망 학습을 수행한다. 최적화 학습은 손실 함수의 최소지점을 찾아가는 과정이다. 최적해를 찾기 전엔 어디서 출발해야 좋을 지 알 수 없다. 따라서 최적화 알고리즘은 어느 위치에서 시작하던 최솟값을 찾을 수 있어야 한다. 
	- 신경망의 손실함수는 차원이 매우 높고 복잡한 모양을 하고 있어 최적화가 어렵다. 손실 함수는 하나의 global minimum과 수많은 local minimum들로 이루어져 있다. global minimum을 찾으려는 것은 곡면 전체의 모양을 확인 해야 하므로 계산 비용이 많이 들거나 때로는 불가능 할 수 있다. 따라서 대부분 최적화 알고리즘의 목표는 local minimum을 찾는 것이다. 단 좋은 local minimum을 찾기 위해 여러개의 해를 찾아 그중에 가장 좋은 해를 선택한다
	- 일반적으로 최적화 문제는 관측변수가 많고 닫힌 형태closed form으로 정의되지 않으므로, 함수를 미분하여 최대 최소를 구할 수 없다
		- 닫힌 형태란 유한개의 방정식으로 명확한 해를 표현할 수 있는 문제, 해를 해석적analytic으로 표현할 수 있는 식의 형태라고 말할 수 있다
	- 최적화 알고리즘은 미분에 차수에 따라 1차 미분, 1.5차 미분, 2차 미분 방식으로 나뉘는데 이중 1차 미분 방식이 경사하강법과 그 변형방식들 (SGD,모멘텀,AdaGrad,RMSProp,Adam) 등이다

### 경사하강법 Gradient Descent
- 경사하강법은 손실 함수의 경사를 구하고, 경사의 반대되는 방향으로 이동하는 것을 극값에 도달할 때 까지 반복하는 것이다
- $\theta  ^{+} = \theta - \alpha \cfrac{\partial {J}}{\partial {\theta}}$ , $J$ 는 손실함수
	- $\alpha$ : 학습률 learning rate
	- $- \cfrac{\partial {J}}{\partial {\theta}}$ : 경사의 음수방향을 가르킨다, 경사하강법에선 $\theta-\theta  ^{+}$ 가 임계치 이하가 되면 최소지점에 도달한 것으로 간주하고 이동을 멈춘다
- 그레디언트란
	- $\mathbb{R}  ^{n} \to \mathbb{R}$ 형태의 함수의 미분
	- $\mathbf{x}=\begin{bmatrix} x _{1}, x _{2}, \cdots,x _{n} \end{bmatrix} \in \mathbb{R}  ^{n}$ 이라 하자. $f( \mathbf{x})$ 에 대하여 그레디언트는
	- $\nabla f(\mathbf{x})=\begin{bmatrix} \cfrac{\partial {f}}{\partial {x_1}} \\ \cfrac{\partial {f}}{\partial {x _{2}}}  \\ \cdots \\ \cfrac{\partial {f}}{\partial {x_n}} \end{bmatrix}$

### 신경망에 경사하강법 적용
- 2계층 신경망 회귀 모델
- 은닉 계층의 활성화 함수는 ReLU , 출력 계층의 활성 함수는항등함수, 손실함수 $J(y,t)$ 는 평균제곱오차라 가정하자
- 표기 규약
	- $w _{nm}  ^{1}$ :  $n$ 번째 입력 뉴런 인덱스, $m$ 번째 은닉 뉴런 인덱스를 의미한다. 윗첨자 1는 계층 번호를 의미한다
- 파라미터 업데이트 식
	- $w _{nm}  ^{+1}= w _{nm}  ^{1} - \alpha \cfrac{\partial {J}}{\partial {w _{nm}  ^{1}}}$
	- 손실함수 $J=\displaystyle\frac{1}{N}\displaystyle\sum_{(x,t) \in \mathcal{D}}^{N}{\| t-y \|^2}$ 에서 가중치 변수 $w _{nm} ^{1}$ 는 은닉 뉴런에 정의되며, 출력 뉴런을 거쳐 손실 함수에 간접적으로 영향을 미친다. 이 영향력을 찾기 위해 연쇄법칙을 활용 한다
		- $\cfrac{\partial {J}}{\partial {w _{nm} ^{1}}}=\cfrac{\partial {J}}{\partial {y}} \cdot \cfrac{\partial {y}}{\partial {z  ^{2}}} \cdot \cfrac{\partial {z  ^{2}}}{\partial {a _{m} ^{1}}} \cdot \cfrac{\partial {a _{m} ^{1}}}{\partial {z _{m}  ^{1}}} \cdot \cfrac{\partial {z _{m}  ^{1}}}{\partial {w _{nm}  ^{1}}}$
	- 그리고 다음과 같은 사실들을 활용하자
		- $z _{m}  ^{1}=w _{jm}  ^{1}x _{j}$
			- $\cfrac{\partial {z _{m}  ^{1}}}{\partial {w _{nm} ^{1}}}=x _{n}$
		- $a _{m}  ^{1}=ReLU(z _{m}  ^{1})$
			- $\cfrac{\partial {a _{m} ^{1}}}{\partial {z _{m} ^{1}}}=ReLU'(z _{m}  ^{1})$
		- $z  ^{2}=w _{j}  ^{2} a _{j}  ^{1}$
			- $\cfrac{\partial {z  ^{2}}}{\partial {a _{m}  ^{1}}}= w _{m}  ^{2}$
		- $y=Identity(z  ^{2})= z  ^{2}$
		- $J(y)=\displaystyle\frac{1}{N}\displaystyle\sum_{(x,t) \in \mathcal{D}}^{N}{\| t-y \|^2}$ 
			- $\cfrac{\partial {J}}{\partial {y}}=-\displaystyle\frac{1}{N}\displaystyle\sum_{(x,t) \in \mathcal{D}}^{N}{2(t-y)}$
	- 그 결과
		- $\cfrac{\partial {J}}{\partial {w _{nm} ^{1}}}=-\displaystyle\frac{1}{N}\displaystyle\sum_{(x,t) \in \mathcal{D}}^{N}{2(t-y)} \cdot 1 \cdot w _{m}  ^{2} \cdot ReLU'(z _{m}  ^{1}) \cdot x _{n}$
		- $w _{nm}  ^{1+} = w _{nm}  ^{1}-\alpha \cfrac{\partial {J}}{\partial {w _{nm} ^{1}}}=w _{nm}  ^{1}-\alpha(-\displaystyle\frac{1}{N}\displaystyle\sum_{(x,t) \in \mathcal{D}}^{N}{2(t-y)} \cdot 1 \cdot w _{m}  ^{2} \cdot ReLU'(z _{m}  ^{1}) \cdot x _{n})$

## 역전파 알고리즘
- 신경망에 경사하강법을 적용할 때 손실 함수에서 각 가중치까지의 신경망의 역방향으로 실행했던 함수를 따라가며 미분을 계산하였다. 만일 이를 모든 파라미터에 대해 개별적으로 실행한다면, 같은 미분을 여러번 반복하는 비효율적인 방법이 될 것이다. 이러한 문제를 해결하고자 제안된 방법이 오차의 역전파 알고리즘 backpropagation algorithm이다

- $\cfrac{\partial {J}}{\partial {w _{nm} ^{1}}}=\cfrac{\partial {J}}{\partial {y}} \cdot \cfrac{\partial {y}}{\partial {z  ^{2}}} \cdot \cfrac{\partial {z  ^{2}}}{\partial {h _{m}  ^{1}}} \cdot \cfrac{\partial {h _{m}  ^{1}}}{\partial {z _{m}  ^{1}}} \cdot \cfrac{\partial {z _{m}  ^{1}}}{\partial {w _{nm}  ^{1}}}$
- $\cfrac{\partial {J}}{\partial {w _{n-1,m} ^{1}}}=\cfrac{\partial {J}}{\partial {y}} \cdot \cfrac{\partial {y}}{\partial {z  ^{2}}} \cdot \cfrac{\partial {z  ^{2}}}{\partial {h _{m}  ^{1}}} \cdot \cfrac{\partial {h _{m}  ^{1}}}{\partial {z _{m}  ^{1}}} \cdot \cfrac{\partial {z _{m}  ^{1}}}{\partial {w _{n-1,m}  ^{1}}}$
	- 입력층과 첫번째 은닉층 사이를 잇는 가중치 파라미터 $w _{ij}$ 를 업데이트 할때마다 공통부분이 존재한다. 이 예제의 경우 $\cfrac{\partial {J}}{\partial {z _{m} ^{1}}}$ 까지가 공통부분으로써 존재한다
	- $\cfrac{\partial {J}}{\partial {w _{nm} ^{1}}}=\cfrac{\partial {J}}{\partial {z _{m}  ^{1}}}\cdot \cfrac{\partial {z _{m}  ^{1}}}{\partial {w _{nm} ^{1}}}$
	- $\cfrac{\partial {J}}{\partial {w _{n-1,m} ^{1}}}=\cfrac{\partial {J}}{\partial {z _{m}  ^{1}}}\cdot \cfrac{\partial {z _{m}  ^{1}}}{\partial {w _{n-1,m} ^{1}}}$
	- 