## RNN은 무엇인가?
- 관련 용어
	- 시퀀스
		- 어떤 순서를 갖고 있는 데이터
		- 예: 시계열 time series, 자연어 NL
	- 메모리셀
		- 타임 스텝에 걸쳐 어떤 상태를 보존하는 신경망의 구조

- 기본적인 신경망은 은닉층에서 활성화함수를 지나 나온 값은 출력층 방향(다음 은닉층 또는 출력층)으로만 향했다. 이런 형태의 신경망을 피드포워드 신경망 Feedforward Neural Network 라고 부른다.
- 그러나 RNN(Recursive Nerual Network)의 경우 활성화 함수를 지나 나온 값이 출력층 방향(다음 은닉층 또는 출력층)으로 보내짐과 동시에 다시 원래의 은닉층 계산의 입력값으로 보내지는 특징을 가지고 있다
- 이러한 특징 때문에 시퀀스 데이터를 다루기 위해선 RNN 계열 모델을 활용해야 한다

![RNN2](RNN2.png)
-  입력-출력에 따른 RNN 활용들
![RNN](RNN.png)

### RNN의 수학적 표현
- 용어 정의
	- $t$ 현재 시점
	- $h_t$ 은닉 상태 값
	- $W_x$: 입력층의 가중치 행렬
	- $W_h$ 이전 시점 $h_{t-1}$의  가중치 행렬
	 - $f$: 비선형 활성화함수 
- 개념 정의
	-   은닉층 : $h_t=tanh(W_xx_t+W_hh_{t−1}+b)$
	-   출력층 : $y_t=f(W_yh_t+b)$
	- $h_t$ 를 계산할 때 [주로 하이퍼볼릭 탄젠트 함수를 사용한다](https://jun-story.tistory.com/19)

- 각 벡터와 행렬의 크기
	- $x_{t}\in\textbf{M}(d \times1)$
	- $W_{x}\in\textbf{M}(D_{h} \times d)$
	- $W_{h}\in\textbf{M}(D_{h} \times D_{h})$
	- $h_{t-1}\in\textbf{M}(D_{h} \times 1)$
	-  $b \in\textbf{M}(D_{h} \times 1)$
- ![](RNN사이즈.png)

### 케라스에서의 RNN 모듈
- `SimpleRNN` (둘중 하나로 표기)
	- `model.add(SimpleRNN(hidden_units, input_shape=(timesteps, input_dim)))`
	- `model.add(SimpleRNN(hidden_units, input_length=M, input_dim=N))`
	- 파라미터
		- `hidden_units` = 은닉 상태의 크기를 정의.메모리 셀이 다음 시점의 메모리 셀과 출력층으로 보내는 값의 크기(`output_dim`)와도 동일. RNN의 용량(capacity)을 늘린다고 보면 되며, 중소형 모델의 경우 보통 128, 256, 512, 1024 등의 값을 가진다.  
		- `timesteps` = 입력 시퀀스의 길이(`input_length`) 또는 시점의 수
		- `input_dim` = 입력의 크기(RNN 레이어로 한번에 들어가는 데이터의 갯수)
	- 유의사항
		- RNN는 `(batch_size, timesteps, input_dim)` 의 3D 텐서를 입력으로 받는다
		- `return_sequences=False` 를 택하면 마지막 시점`time step`에 대한 은닉 상태값만 출력된다
			- ![return_sequences](return_sequences.png)


### 양방향 순환 신경망 Bidirectional Recurrent Neural Network
- 시점 $t$에서의 출력값을 예측할 때 이전 시점에 대한 입력 뿐만 아니라, 이후 시점의 입력또한 예측에 활용한다
	- 예: ' 운동을 열심히 하는 것은 ( ) 을 늘리는데 효과적이다'
		1. 근육
		2. 지방
		3. 스트레스
		- 해당 빈칸에 알맞는 단어를 넣기 위해선 문장 앞뒤를 읽을 필요가 있다
- 양방향 RNN은 두개의 메모리셀이 존재한다. 첫번째 메모리셀은 앞시점 은닉상태Foward State, 두번째 메모리셀은 뒤 시점 은닉상태Backward State를 전달 받아 현재의 은닉상태를 계산한다. 그리고 이 두 값 모두 현재 시첨의 출력층에서 출력값을 예측 하는데 활용된다


## LSTM란 무엇인가?
- RNN는 비교적 짧은 시퀀스sequence에 대해서만 효과를 보인다. 그 이유는 바닐라 RNN의 time step이 길어질수록, 앞의 정보가 뒤로 충분히 전달되지 못하는 현상이 발생되기 때문이다
- LSTM은 은닉층 메모리셀에 입력,망각,출력 게이트를 추가하여 불필요한 기억을 지우고 기억할 것을 정하는 기능을 추가한 RNN이다. 
 ![](LSTM_세부구조.png)
- 삭제 게이트 forget gate 
	- $f_t$는 과거 정보를 얼마나 기억할 것인지 결정하는 게이트다. 시그모이드 함수의 출력 범위는 0 ~ 1 이기 때문에 그 값이 0이라면 이전 상태의 정보는 잊고, 1이라면 이전 상태의 정보를 온전히 기억하게 된다
	- 수식
		- $f_t=σ(W_{xf}x_t+W_{hf}h_{t−1}+b_f)$
-  입력 게이트 input gate
	- $i_t$는 현재 정보를 얼마나 기억할 것인지 결정하는 게이트이다. 이 값은 시그모이드 이므로 0 ~ 1 이지만 hadamard product를 하는 $\tilde{C}_t$는 hyperbolic tangent 결과이므로 -1 ~ 1 이 된다. 따라서 결과는 음수가 될 수도 있다.
	- 수식
		- $i_t=σ(W_{xi}x_t+W_{hi}h_{t−1}+b_i)$  
		- $g_t=tanh(W_{xg}x_t+W_{hg}h_{t−1}+b_g)$
- 셀 상태 cell state
	- 수식
		$C_t=f_{t}\bigcirc C_{t−1}+i_{t}\bigcirc g_t$
- 출력 게이트 output gate
	- 최종적으로 얻은 cell state를 얼마나 출력시킬지 결정하는 게이트이다
	- $o_t$는 최종 결과 $h_t$를 위한 게이트이며, cell state의 hyperbolic tangent를 hadamard product한 값이 LSTM의 최종 결과가 된다.
	- 수식  
		- $o_t=σ(W_{xo}x_t+W_{ho}h_{t−1}+b_o)$
		- $h_t=o_t∘tanh(c_t)$


> 레퍼런스
- 아다마르곱 $\bigcirc$ hadamard product/component-wise product
	- 같은 크기의 행렬 $\textbf{A,B}\in \textbf{M}(m,n)$ 이 있다고 하자
	-  $\textbf{C}=\textbf{A}\bigcirc\textbf{B}$  일때 $C_{ij}=A_{ij}B_{ij}$



- $x_t$ : 현재 정보, input 값
- $h_{t-1}$: 과거 정보, 은닉층 값


##  게이트 순환 유닛 GRU (Gated Recurrent Unit)
- 2014년 뉴욕대 조경현 교수가 집필한 논문에서 제안된 모델
- LSTM의 장기 의존성 문제를 해결하면서, 은닉 상태를 업데이트하는 계산을 줄인 모델이다. 즉 GRU는 성능은 LSTM과 유사하면서, 복잡했던 LSTM의 구조를 단순화 시킨 것이다
- 경험적으로 데이터 양이 적을 때는 매개 변수의 양이 적은 GRU가 조금 더 낫고, 데이터 양이 더 많으면 LSTM이 더 낫다고도 한다. GRU보다 LSTM에 대한 연구나 사용량이 더 많은데, 이는 LSTM이 더 먼저 나온 구조이기 때문이다
-  [![../../Resources/kiee/KIEEP.2020.69.4.253/fig2.png](http://journal.auric.kr/Resources/kiee/KIEEP.2020.69.4.253/fig2.png)](http://journal.auric.kr/Resources/kiee/KIEEP.2020.69.4.253/fig2.png)
  