# 텍스트 전처리
- 풀고자 하는 문제의 용도에 맞게 텍스트 데이터를 사전 처리하는 과정


## 토큰화 Tokenization
- 말뭉치(corpus, 복수형: corpora): 자연언어 연구를 위해 특정한 목적을 가지고 언어의 표본을 추출한 집합
- 주어진 코퍼스(corpus)에서 토큰(token)이라 불리는 단위로 나누는 작업이다
- 보통 의미있는 단위로 토큰을 정의한다
- 토큰의 기준을 단어로 하는 경우를 단어 토큰화(Word tokenization)라고하며   코퍼스 내에서 문장 단위로 구분하는 작업은 문장 토큰화(Sentence Tokenization)라 한다.

### 토큰화시 유의사항
- 구두점이나 특수문자를 단순 제외해선 안된다
	- 예:Ph.D 나 01/02/06, $45.55
- 줄임말과 단어 내 띄어쓰기가 있는 경우를 유의해야 한다
	- 예: New York, rock 'n' roll

### 문장 토큰화 Sentence Tokenization
- 직관적으로 생각해봤을 때는 ?나 마침표(.)나 ! 기준으로 문장을 잘라내면 되지 않을까라고 생각할 수 있지만, 꼭 그렇지만은 않는다
	- 예
		- IP 192.168.56.31 서버에 들어가서 로그 파일 저장해서 aaa@gmail.com로 결과 좀 보내줘. 그 후 점심 먹으러 가자.
		- Since I'm actively looking for Ph.D. students, I get the same question a dozen times every year.

### 한국어 토큰화시 어려움
- 교착어의 특성
	- 한국어에 그(he/him)라는 주어나 목적어가 들어간 문장이 있다고 하자 이 경우, 그라는 단어 하나에도 '그가', '그에게', '그를', '그와', '그는'과 같이 다양한 조사가 '그'라는 글자 뒤에 띄어쓰기 없이 바로 붙게된다. 자연어 처리를 하다보면 같은 단어임에도 서로 다른 조사가 붙어서 다른 단어로 인식이 되면 자연어 처리가 힘들고 번거로워지는 경우가 많다
	- 한국어는 단어가 아닌 형태소 morpheme( 의미를 가지는 요소로서는 더 이상 분석할 수 없는 가장 작은 말의 단위)단위로 토큰화를 해주어야 한다
		-  자립 형태소 : 접사, 어미, 조사와 상관없이 자립하여 사용할 수 있는 형태소. 그 자체로 단어가 된다. 체언(명사, 대명사, 수사), 수식언(관형사, 부사), 감탄사 등이 있다. 
		- 의존 형태소 : 다른 형태소와 결합하여 사용되는 형태소. 접사, 어미, 조사, 어간를 말한다.
- 한국어는 띄어쓰기가 영어보다 잘 지켜지지 않는다
	- 한국어는 영어권 언어와 비교하여 띄어쓰기가 어렵고 잘 지켜지지 않는 경향이 있다. 그 이유는 여러 견해가 있으나, 가장 기본적인 견해는 한국어의 경우 띄어쓰기가 지켜지지 않아도 글을 쉽게 이해할 수 있는 언어이기 때문이다
	- 예
		- 제가이렇게띄어쓰기를전혀하지않고글을썼다고하더라도글을이해할수있습니다.
		- Tobeornottobethatisthequestion


## 품사 태깅 part-of-speech tagging
-  단어는 표기는 같지만 품사에 따라서 단어의 의미가 달라질수도 있기 때문에 토큰화 과정에서 각 단어가 어떤 품사로 쓰였는지를 구분해놓기도 하는데, 이 작업을 품사 태깅이라고 한다
- 예
	- 영어 단어 'fly'는 동사로는 '날다'라는 의미를 갖지만, 명사로는 '파리'라는 의미를 갖고 있다
	- '못'이라는 단어는 명사로서는 망치를 사용해서 목재 따위를 고정하는 물건을 의미한다. 하지만 부사로서의 '못'은 '먹는다', '달린다'와 같은 동작 동사를 할 수 없다는 의미로 쓰인다
- 관련 모듈
	- 영어: NLTK에서는 Penn Treebank POS Tags라는 기준을 사용하여 품사를 태깅한다
	- 한국어:KoNLPy(코엔엘파이)라는 파이썬 패키지를 사용할 수 있다. 코엔엘파이를 통해서 사용할 수 있는 형태소 분석기로 Okt(Open Korea Text), 메캅(Mecab), 코모란(Komoran), 한나눔(Hannanum), 꼬꼬마(Kkma)가 있다.


## 정제 및 정규화
- 정제(cleaning) : 갖고 있는 코퍼스로부터 노이즈 데이터를 제거한다. 
- 정규화(normalization) : 표현 방법이 다른 단어들을 통합시켜서 같은 단어로 만들어준다.

### 정제
- 불필요한 단어 제거
	- 등장 빈도가 적은 단어 제거
	- 길이가 짧은 단어 제거
		- 영어권 언어에서 길이가 짧은 단어들은 대부분 불용어에 해당된다
		- 단정적으로 말할 수는 없지만, 영어 단어의 평균 길이는 6~7 정도이며, 한국어 단어의 평균 길이는 2~3 정도로 추정된다
		- 예
			- 길이가 1인 단어: 관사 'a'와 주어로 쓰이는 'I'가 제거
			- 길이가 2인 단어: it, at, to, on, in, by 등과 같은 대부분 불용어에 해당되는 단어들이 제거
			-  길이가 3인 단어:  이 경우 fox, dog, car 등 길이가 3인 명사들이 제거 되기시작하므로 사용하고자 하는 데이터에서 해당 방법을 사용해도 되는지에 대한 고민이 필요

### 정규화
- 대소문자 통합
	- 영어권 언어에서 대, 소문자를 통합하는 것은 단어의 개수를 줄일 수 있는 또 다른 정규화 방법이다. 영어권 언어에서 대문자는 문장의 맨 앞 등과 같은 특정 상황에서만 쓰이고, 대부분의 글은 소문자로 작성되기 때문에 대, 소문자 통합 작업은 대부분 대문자를 소문자로 변환하는 소문자 변환작업으로 이루어지게 된다


## 어간 추출 Stemming 및 표제어 추출Lemmatization
> 형태학(morphology)이란 형태소로부터 단어들을 만들어가는 학문. 형태소의 종류로 어간(stem)과 접사(affix)가 존재합니다.
- 표제어 추출
	- 표제어(Lemma)는 사전에 등재된 단어를 의미한다
	- 표제어 추출은 단어들로부터 표제어를 찾아가는 과정이다
	- 형태학적 파싱을 통해 찾아낸다
	- 예
		- am, are, is는 서로 다른 스펠링이지만 그 뿌리 단어는 be라고 볼 수  있다 이때, 이 단어들의 표제어는 be라고 한다
		
	- 형태학(morphology)이란 형태소로부터 단어들을 만들어가는 학문. 형태소의 종류로 어간(stem)과 접사(affix)가 존재한다
		- 어간(stem)  : 단어의 의미를 담고 있는 단어의 핵심 부분.
		- 접사(affix)  : 단어에 추가적인 의미를 주는 부분
		
- 어간 추출
	- 어간(Stem)을 추출하는 작업을 어간 추출(stemming)이라고 한다. 어간 추출은 형태학적 분석을 단순화한 버전이라고 볼 수도 있고, 정해진 규칙만 보고 단어의 어미를 자르는 어림짐작의 작업이라고 볼 수도 있다
	- 이 작업은 섬세한 작업이 아니기 때문에 어간 추출 후에 나오는 결과 단어는 사전에 존재하지 않는 단어일 수도 있다

## 불용어 Stopword
- 불용어: 의미를 해석하는데 충분히 유의미하다고 여겨지지 않는 단어들 
- 관련 모듈
	- 영어: NLTK에서는 위와 같은 100여개 이상의 영어 단어들을 불용어로 패키지 내에서 미리 정의하고 있다
	- 한국어 레퍼런스
		- [레퍼런스1]( https://www.ranks.nl/stopwords/korean)
		- [레퍼런스2](https://bab2min.tistory.com/544)

