## 딥러닝 Deep Learning
- 머신러닝의 특정 한 분야로 인공 신경망 Artificial Neural Network 층을 연속적으로 깊이 쌓아올려 데이터를 학습시키는 방법이다

### 퍼셉트론 Perceptron
![](퍼셉트론.png)
>  $x:input\,\,value$, $\omega:weight$, $y:output \,\,value$
- 1957년 프랭크 로젠블라트 Frank Rosenblatt가 제안한 초기형태 인공신경망이다
- 다수의 입력으로부터 하나의 결과를 내보내는 알고리즘이다
- 입력값을 그에 해당되는 가중치와 곱하여 계단함수에 보내고, 이를 출력하는 형태이다
- 관련 용어
	-  계단 함수 Step function
		- 입력값이 임계치(threshold)를 넘으면  1을 출력하고, 그렇지 않을 경우에는 0을 출력하는 함수
	- 층 layer
		- 값을 보내는 단계과 값을 받아서 출력하는 단계, 그 중간 단계 각각을 지칭하는 용어
		- 첫번째를 입력층 input layer, 두번째를 출력층 output layer, 세번째를 은닉층 hidden layer라고 한다

-  단층 퍼셉트론 Single-Layer Perceptron
	- 입력층과 출력층만 존재하는 퍼셉트론
	- 단층 퍼셉트론은 AND, NAND(두개 입력값이 1일때만 0 그외는 1), OR가 구현 가능하다
	-  단층 퍼셉트론은 XOR 게이트는 (입력값 두 개가 서로 다른 값을 갖고 있을때에만 출력값이 1이 되고, 입력값 두 개가 서로 같은 값을 가지면 출력값이 0이 되는 게이트)는 구현 불가능하다 

- 다층 퍼셉트론 MultiLayer Perceptron
	- 입력층과 출력층과 더불어 은닉층이 존재하는 퍼셉트론
- 심층 신경망 Deep Neural Network
	- 은닉층이 2개 이상으로 구성된 신경망(퍼셉트론 뿐만 아니라 여러 변형된 다양한 신경망들도 은닉층 2개 이상이면 이와같이 부름)

## 인공신경망 훑어보기 

## 관련 용어들
- 피드 포워드 신경망 Feed-Foward Neural Network
	- 입력층에서 출력층 방향으로만 연산이 전개되는 신경망
- 전결합층 Fully- connected layer / Dense Layer
	- 어떤 층의 모든 뉴런이 다음층의 모든 뉴런과 연결되어 있는 층
	- 다층 퍼셉트론의 모든 은닉층과 출력층은 전결합층이다
- 활성화 함수 Activation Function
	- 비선형 함수라는 특징을 갖는다
		- 예: 활성화 함수가 선형함수라고 하자. $f(x)=wx$ 의 형태를 갖는다. 그렇다면 연속되어 은닉층을 쌓는경우 $f(f(f(x)))=w(w(w(x)))=w^3x=kx$ 이다. 이 경우 은닉층을 여러번 추가한 것이 1회 추가한 효과와 다름이 없게 된다
	- 예
		- 스텝 함수 Step function
		- $$\begin{equation}

Step\_Function(x)=

\begin{cases}

1 & \text{if } x \ge 0\\

0 & \text{if } x <0

\end{cases}

\end{equation}$$
		- ![스텝함수](스텝함수.png)
		- 시그모이드 함수 Sigmoid function
			- $$

Sigmoid(x)=\cfrac{1}{1+e^{-x}}$$
			- ![시그모이드](시그모이드.png)
			- $x=0$을 경계로 좌우 대칭인 모양이며, 도함수는 $Sigmoid'(x)=\cfrac{e^{-x}}{(1+e^{-x})^2}$ 로  $x=0$ 에서 기울기가 0.25이고 $\lim \,\,{x \to \infty}$ ,  $\lim \,\,{x \to -\infty}$ 에서 기울기가 0이 되는 모습을 보여준다
			- 시그모이드 함수를 활성화 함수로 하는 인공신경망을 쌓는다면 가중치와 편향을 업데이트 하는 역전파 과정에서 0에 가까운 값을 누적해서 곱하게 되면서 앞단에는 기울기(미분값)이 잘 전달되지 않게 된다. 이러한 현상을 기울기 소실 Vanishing Gradient라 부른다
			- 결론적으로 시그모이드 함수의 은닉층에서의 사용은 지양된다. 시그모이드는 주로 이진 분류를 위해 출력층에서 주로 사용된다
			
		- 하이퍼볼릭 탄젠트 함수 Hyperbolic tangent function
			- $$tanh(x)=\cfrac{e^{x}-e^{-x}}{e^x+e^{-x}}$$
			- ![tanh](tanh.jpg)
			- $x=0$을 경계로 좌우 대칭인 모양이며, 도함수는 $tanh'(x)=1-\cfrac{(e^{x}-e^{-x})^2}{(e^{x}+e^{-x})^2}=1-tanh^2(x)$ 로 $x=0$ 에서 기울기가 1이고   $\lim \,\,{x \to \infty}$ ,  $\lim \,\,{x \to -\infty}$ 에서 기울기가 0이 되는 모습을 보여준다
			- 즉 미분하였을 때 시그모이드 함수보다 전반적으로 큰 값이 나오므로, 시그모이드 함수보다 기울기 소실 증상이 적은 편에 속해 은닉층에서 상대적으로 선호된다
		- ReLu 함수
			- $$f(x)=max(0,x)$$
			- ![ReLU함수](ReLU함수.png)
			- Deep Neural Network의 은닉층에서 시그모이드 함수보다 훨씬 더 잘 작동한다
			- 시그모이드나 하이퍼볼릭탄젠트 함수와 같이 어떤 연산이 필요한 것이 아니라 단순 임계값만 지정해준 것이므로 연산 속도도 훨신 빠르다
			- 그러나 문제는 입력값이 음수면 기울기, 즉 미분값이 0이 된다. 이  뉴런은 다시 회생하기 매우 어렵다. 이러한 문제를 죽은 렐루 dying ReLU라고 부른다

		- 리키 렐루 함수 Leaky ReLU
			 - $$ f(x)=max(ax,x)\,\,,a<< 1$$
			-  ![LeakyReLU함수](LeakyReLU함수.png)
			- 죽은 렐루를 보완하기 위해 만든 변형 함수이다. $x$ 가 음수가 들어올 경우 0이 아니라 다른 값을 반환시키기 위하여 위와 같이 정의한다. 이때 $a$는 하이퍼파라미터로 0.01정도의 작은 수를 갖는다
		
		- 소프트맥스 함수 Softmax 
			- 소프트맥스 함수는 $k$ 개의 레이블이 있을 때 $k$차원의 벡터를 입력받아 각 클래스에 대한 확률을 추정한다.  $j$ 번째 레이블일 확률을 다음과 같이 계산한다(다중분류 모델에 사용된다)
			- $$p_j=\cfrac{exp(z_j)}{\sum_{i=1}^{k}exp(z_i)}$$
				- 이때 $z_i$는 벡터의 $i$ 번째 성분, $exp(x)=e^x$ 
				
			- $Softmax(z)=[\cfrac{exp(z_1)}{\sum_{i=1}^{k}exp(z_i)},\cfrac{exp(z_2)}{\sum_{i=1}^{k}exp(z_i)},...,\cfrac{exp(z_k)}{\sum_{i=1}^{k}exp(z_i)}]$	 
			- 즉 소프트맥스 함수는 각각의 레이블이 정답일 확률을 담은 벡터를 반환하고, 이때 이 확률값들을 모두 합하면 1이 된다
			
			