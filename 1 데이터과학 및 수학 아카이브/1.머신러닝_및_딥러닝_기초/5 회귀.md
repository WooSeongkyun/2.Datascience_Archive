## CH5 회귀
- 회귀란 무엇인가?
    - 정량적인 종속변수를 예측하는데 쓰이는 도구이다.
    - 현대 통계학습중 일반화된 선형회귀법이 많은 부분을 차지한다
- 정의
    - 독립변수 $X_1,X_2,...,X_n$과 종속변수 $Y$, 계수 $\beta_0,\beta_1,...,\beta_n$, 오차항 $\epsilon_1,\epsilon_2,...,\epsilon_n$이 있다하자
    - 다음과 같은 가정이 성립한다 하자
        - 비확률변수$nonstochastic$ :독립변수 $X_1,X_2,...,X_n$는 비확률변수이다(상수취급한다)
        - 선형성$linearity$: 계수와 종속변수 사이 선형성을 갖는다
        - 독립성:$independence$ 독립변수가 다른 독립변수에 영향을 주지 않는다
        - 등분산성$homoskedasticity$: $Var(\epsilon_i)=\sigma^2$$\,\,for\,\,i=1,2,...$
        - 정규성$normality$: $\epsilon\sim \mathscr{N}(0,\sigma^2)$
        - $i\ne j$일때 $Cov(\epsilon_i,\epsilon_j)=0$ 이다
- 정의
    - 모델형태가  $Y=\sum_{i=1}^{n}\beta_{i}X_i+\beta_0+\epsilon$ 이라고 가정하자. 그 계수값 $\beta_0,\beta_1,\beta_2,...,\beta_n$ 을 구한뒤, 해당 모델로 종속변수를 예측하는 일련의 방법론을 선형회귀라고 부른다

## 1. 단순선형회귀 $Simple\,\,Linear\,\,Regression$

###  회귀계수의 이론적 추정
- 조건
    - 독립변수 $X$와 종속변수 $Y$, 계수(파라미터) $\beta_0,\beta_1$이 있다하자
    - 훈련 데이터셋 $\{(x_1,y_1),(x_2,y_2),...,(x_n,y_n)\}$이 있다하자
    - 예측값 $\hat{y_i}=\hat{\beta_0}+\hat{\beta_1}x_i$라고 하자
- 순서
    - $y_i-\hat{y_i}$가 작을수록 $\hat{\beta_0},\hat{\beta_1}$이 잘 추정되었다고 볼 수 있다
    - 그렇기 때문에 각 $i$에 대하여 $e_i=y_i-\hat{y_i}$ 값을 측정하자. 그리고 제곱을 하면 $e_i^2=|y_i-\hat{y_i}|^2$, 종속변수의 관측치와 예측치 사이 거리의 제곱이 된다.
    - 이 값을 $i$에 대하여 모으자. 모은값이 작으면 작을수록 계수 추정이 잘 되었다고 해석할 수 있다
    - $RSS=\sum_{i=1}^n|y_i-\hat{y_i}|^2=\sum_{i=1}^n(y_i-(\hat{\beta_0}+\hat{\beta_1}x_i))^2$
	
-   *미분을 이용하여 $RSS$를 최소화하는 계수값을 찾기*
	- $\cfrac{\partial{(RSS)}}{\partial{\hat{\beta_0}}}=-2\sum_{i}(y_i-\hat{\beta_0}-\hat{\beta_1}x_i)=0$
	- $\cfrac{\partial{(RSS)}}{\partial{\hat{\beta_1}}}=-2\sum_{i}(y_i-\beta_0-\beta_1x_i)x_i=0$
	- $n\hat{\beta_0}+\hat{\beta_1}\sum_{i}x_i=\sum_{i}y_i$
	- $\hat{\beta_0}\sum_{i}x_i+\hat{\beta_1}\sum_{i}x_i^2=\sum_{i}x_iy_i$
	- $\begin{bmatrix} \hat{\beta_0} \\ \hat{\beta_1} \end{bmatrix}=\cfrac{1}{n\sum_{i}x_i^2-(\sum_{i}x_i)^2} \begin{bmatrix} \sum_{i}x_i^2 & -\sum_{i}x_i \\ -\sum_{i}x_i &n \end{bmatrix}\begin{bmatrix} \sum_{i}y_i \\ \sum_{i} x_iy_i\end{bmatrix}$
	- $n\bar{x}=\sum_{i}x_i, n\bar{y}=\sum_{i}y_i$로 두자
	- $\begin{bmatrix} \hat{\beta_0} \\ \hat{\beta_1} \end{bmatrix}=\cfrac{1}{n(\sum_{i}x_i^2-n\bar{x}^2)} \begin{bmatrix} \sum_{i}x_i^2 & -n\bar{x}\\ -n\bar{x} &n \end{bmatrix}\begin{bmatrix} n\bar{y}\\ \sum_{i} x_iy_i\end{bmatrix}=\cfrac{1}{n(\sum_{i}(x_i-\bar{x_i})^2)} \begin{bmatrix} n\bar{y}\sum_{i}x_i^2 -n\bar{x}\sum_{i}x_iy_i\\ -n^2\bar{x}\bar{y} + n\sum_{i}x_iy_i \end{bmatrix}$
	- $\begin{bmatrix} \hat{\beta_0} \\ \hat{\beta_1} \end{bmatrix}=\cfrac{1}{\sum_{i}(x_i-\bar{x})^2}\begin{bmatrix} \bar{y}\sum_{i}x_i^2 -\bar{x}\sum_{i}x_iy_i\\   \sum_{i}x_iy_i-n\bar{x}\bar{y} \end{bmatrix}$
	- $\begin{bmatrix} \hat{\beta_0} \\ \hat{\beta_1} \end{bmatrix}=\cfrac{1}{\sum_{i}(x_i-\bar{x})^2}\begin{bmatrix} \bar{y}\sum_{i}x_i^2 -\bar{x}\sum_{i}x_iy_i\\   \sum_{i}(x_i-\bar{x})(y_i-\bar{y}) \end{bmatrix}$
	- $\sum_{i}(x_i-\bar{x})^2\hat{\beta_1}+n\bar{x}\bar{y}=\sum_{i}x_iy_i$
	- $\bar{y}\sum_{i}x_i^2-\bar{x}(\sum_{i}(x_i-\bar{x})^2\hat{\beta_1}+n\bar{x}\bar{y})$
	- $=\bar{y}(\sum_{i}x_i^2-n\bar{x}^2)-\hat{\beta_1}\bar{x}\sum_{i}(x_i-\bar{x})^2$
	- $\therefore \hat{\beta_0}=\bar{y}-\hat{\beta_1}\bar{x}$
- 계수 계산하기(최종 요약)
    - $\begin{bmatrix} \hat{\beta_0} \\ \hat{\beta_1} \end{bmatrix}= \begin{bmatrix} \bar{y}-\hat{\beta_1}\bar{x}\\ \cfrac{\sum_{i=1}^{n}(x_i-\bar{x})(y_i-\bar{y})}{\sum_{i=1}^{n}(x_i-\bar{x})^2}:\cfrac{S(x,y)}{S(x,x)} \end{bmatrix}$
	

## 2. 응용 선형회귀: 규제Regularization을 추가하여
- 릿지 Ridge 회귀: 선형회귀에 L2 규제를 추가한 회귀모델. 
- 라쏘 Lasso 회귀: 선형회귀에 L1 규제를 추가한 회귀모델. L2가 회귀 계수의 크기를 줄이는 것에 비해, L1규제는 예측 영향력이 작은 피처 계수를 0으로 만든다
- 엘라스틱넷 ElasticNet: L1,L2 규제 모두를 결합한 모델
- 로지스틱 회귀 Logistic Regression: 회귀란 이름이 붙어있지만 사실 분류에 사용되는 선형모델. 일반적 이진분류뿐만 아니라, 희소영역 분류, 텍스트 분류와 같은 영역에서 뛰어난 성능을 보인다


## 3. 경사하강법 알고리즘
- 경사하강법의 정의  
	- $x_{i,j+1}= x_{i,j} - \alpha \nabla f(x_{i,j})$=$x_{i,j}- \alpha\cfrac{\partial{f}}{\partial{\textbf{x}}}$  
	- $i$ 번째 변수의 $j$번째 값을 갱신할때 다음과 같은 공식을 활용한다. 이때 $\alpha$는 hyperparameter이다  
- 비용함수에서의 경사하강법  
	- $J(\theta)= \cfrac{1}{N}*\sum_{i}(y_i-x_i\theta+\phi_i)^2$ 
	- $\cfrac{\partial{J}}{\partial{\theta}}=\cfrac{2}{N}(y_i-\theta)$
	- $\theta_{i,j+1}= \theta_{i,j}-\alpha\cfrac{2}{N}\cdot\sum_{i}x_i(y_i-x_i\theta_i)$
	- $\phi_{i,j+1}= \phi_{i,j}-\alpha\cfrac{2}{N}\cdot\sum_{i}(y_i-x_i\theta_i)$
	- (여기서 $\theta$는 가중치weight, $\phi$는 편향bias이다)
	
- 경사하강법 모듈
```python
import numpy as np  
  
# 계수를 1회 업데이트 하는 함수  
def get_weight_updates(coef,const,X,y,learning_rate=0.01):  
    N =len(y)  
    coef_update= np.zeros_like(coef)  
    const_update= np.zeros_like(const)  
    y_pred= np.matmul(X,coef.T) +const  
    diff = y-y_pred  
  
    coef_update= -(2/N) * learning_rate*(np.matmul(X.T,diff))  
    const_update= -(2/N) * learning_rate* (np.matmul(np.ones((N,1)).T,diff))  
  
    return coef_update, const_update  
  
#일반적 경사하강법  
def gradient_descent_steps(X,y,iters=10000):  
    coef= np.zeros((1,1))  
    const=np.zeros((1,1))  
  
    #인자로 주어진 iter만큼 반복적으로 get_weight_updates를 호출하기  
    for i in range(iters):  
        coef_update, const_update = get_weight_updates(coef,const,X,y,learning_rate=0.01)  
        coef= coef - coef_update  
        const= const - const_update  
  
    return coef, const  
  
#확률적 경사하강법  
def stochastic_gradient_steps(X,y,batch_size=10,iters=1000):  
    coef= np.zeros((1,1))  
    const= np.zeros((1,1))  
  
    prev_const= 100000  
    iter_index= 0  
  
    for i in range(iters):  
        np.random.seed(i)  
        #전체 X,y 데이터에서 랜덤하게 batch_size만큼 데이터를 추출하여 sample_X,sample_y로 저장한다  
        stochastic_random_index= np.random.permutation(X.shape[0])  
        sample_X=X[stochastic_random_index[0:batch_size]]  
        sample_y=y[stochastic_random_index[0:batch_size]]  
        #랜덤하게 batch_size만큼 추출된 데이터 기반으로 coef_update, const_update 계산 후 업데이트  
        coef_update, const_update= get_weight_updates(coef,const,sample_X,sample_y,learning_rate=0.01)  
        coef= coef- coef_update  
        const= const - const_update  
  
    return coef,const  
  
#비용함수 계산  
def f_cost(y,y_pred):  
    N= len(y)  
    cost= np.matmul((y-y_pred).T,(y-y_pred))/N  
    return cost  
  

```

- 사이킷런의 LinearRegression
	- `sklearn.linear_model import LinearRegression` 을 통해 사용한다
	- 입력 파라미터
		- `fit_intercept`: 절편값을 계산할 것인지 `True or False`로 정한다
		- `normalize`: 회귀를 수행하기전 데이터 셋을 정규화할지 `True or False`로 정한다
	- 속성 
		- `coef_`: 회귀 계수가 배열 형태로 출력된다
		- `intecept_`:  intercept 값


## 4. 회귀 평가 지표
- *Scoring  함수에 회귀 평가 지표를 적용할 때는 score값이 클수록 좋다고 평가하기 때문에 아래의 지표들에 -1을 곱해주는 과정이 필요하다*
- MAE : Mean Absolute Error 
	- $MAE= \cfrac{1}{N}\sum_{i=1}^{N} |Y_i-\hat{Y_i}|$
	- 사이킷런 라이브러리:  `mean_absolute_error` 
- MSE: Mean Squared Error
	- $MSE= \cfrac{1}{N}\sum_{i=1}^{N}(Y_i-\hat{Y_i})^2$
	- 사이킷런 라이브러리:`mean_squared_error`
- RMSE: Root Mean Sqaured Error
	- $RMSE= \sqrt{\cfrac{1}{N}\sum_{i=1}^{N}(Y_i-\hat{Y_i})^2}$
	- 사이킷런 라이브러리: `mean_squared_error`에서 `sqaured=False`로 지정
- $R^2$
	- 분산 기반으로 예측 성능을 평가하는 것으로, 1에 가까울수록 예측 정확도가 높다
	- $R^2=\cfrac{Var_{predict}}{Var_{measured}}$
	- 사이킷런 라이브러리: `r2_score`


	## 5. 다항회귀 multiple regression
	- $y= \textbf{X}\theta$ 의 식이 있다하자
		- 선형회귀에서 선형성 $linearlity$ 을 갖춰야 하는 것은 독립변수 $X$가 아니라, 계수 $\theta$ 이다.
		- $y= \theta_0x^0+\theta_1 x^1+\theta_2 x^2+...+\theta_n x^n$ 의 형태, power of x의 선형조합으로 이루어진 함수를 다항회귀라고 부른다 
	- 다중선형회귀의 예
```python
# 다항 회귀를 이용한 과소적합 및 과적합의 이해  
import numpy as np  
import matplotlib.pyplot as plt  
from sklearn.pipeline import Pipeline  
from sklearn.preprocessing import PolynomialFeatures  
from sklearn.linear_model import LinearRegression  
from sklearn.model_selection import cross_val_score  
  
#임의의 값으로 구성된 X에 대하여 cosine 변환값을 제공하기  
def true_fun(X):  
    return np.cos(1.5*np.pi *X)  
  
#X는 0부터 1까지의 숫자를 샘플링한 숫자이다  
np.random.seed(0)  
n_samples=30  
X=np.sort(np.random.rand(n_samples))  
  
#y값은 코사인 기반의 true_fun()에서 약간의 노이즈 변동값을 더한 값이다  
y= true_fun(X) + np.random.randn(n_samples) *0.1  
  
plt.figure(figsize=(14,5))  
degrees=[1,4,15]  
  
#다항함수의 차수degree를 1,4,15로 변화시키면서 비교한다  
for i in range(len(degrees)):  
    ax= plt.subplot(1,len(degrees),i+1)  
    plt.setp(ax,xticks=(),yticks=())  
  
    #개별 degree별로 Polynomial 변환시키기  
    polynomial_feature= PolynomialFeatures(degree=degrees[i],include_bias=False)  
    linear_regression= LinearRegression()  
    pipeline= Pipeline([('polynomial_feature',polynomial_feature),('linear_regression',linear_regression)])  
    pipeline.fit(X.reshape(-1,1),y)  
  
    #교차검증으로 다항회귀를 평가하기  
    scroes= cross_val_score(pipeline,X.reshape(-1,1),y,scoring='neg_mean_squared_error',cv=10)  
    #Pipeline를 구성하는 세부 객체를 접근하는 named_steps['객체명']을 이용하여 회귀계수를 추출한다  
    coefficients= pipeline.named_steps['linear_regression'].coef_  
    print(f'\n Degree{degrees[i]}의 회귀계수는 {np.round(coefficients,2)}입니다')  
    print(f'\n Degree{degrees[i]}의 MSE는 {-1*np.mean(scroes):.3f}입니다')  
  
    #0부터 1까지 테스트 데이터 셋 100개를 나눠 예측을 시행한다  
    #테스트 데이터셋에 회귀 예측을 수행하고, 실제곡선도 함께 그려서 비교한다  
    X_test= np.linspace(0,1,100)  
    #예측값 곡선  
    plt.plot(X_test,pipeline.predict(X_test[:,np.newaxis]),label='Model')  
    #실제값 곡선  
    plt.plot(X_test,true_fun(X_test),'--',label='True function')  
    plt.scatter(X,y,edgecolors='b',s=20,label='samples')  
  
    plt.xlabel('x'); plt.ylabel('y'); plt.xlim(0,1); plt.ylim(-2,2); plt.legend(loc='best');  
    plt.title(f'Degree ({degrees[i]})\nMSE = {-scroes.mean():.3f}(+/-){scroes.std():.3f}')  
  
plt.show()
```
![Polynomial_regression](Polynomial_regression.png)

## 6. 편향Bias- 분산Variance Trade off 
![Bias_Variance](Bias_Variance.png)]
- 편향 Bias: 추정값의 중심이 실제값의 중심으로부터 떨어져 있는 정도
- 분산 Variance:평균으로부터 데이터 값들이 떨어져있는 정도
- 일반적으로 편향과 분산은 한쪽이 높아지면 다른 한쪽이 낮아지는 경향이 존재한다


## 7. 규제선형모델- 릿지,라쏘,엘라스틱넷
- 왜 규제선형모델을 사용하게 되었는가?
	- 좋은 회귀모델이란 훈련 데이터셋을 제대로 설명할 수 없을 만큼 단순한 모델이여서도 안되고, 반대로 지나치게 훈련 데이터셋에만 들어맞아 그외에 데이터에선 저열한 성능을 갖는 복잡한 모델이여선 안된다
	- RSS(Root-Mean-Squared) 값을 비용함수로 사용하게 되면, 학습 데이터에만 지나치게 맞추게 되어 회귀계수가 쉽게 커지는 현상이 발생한다. 
	- 그렇기 때문에 과적합을 개선하기 위해선 회귀 계수의 크기를 제어하기 위해 비용함수를 좀 더 다르게 정의할 필요가 있다.  기존 비용함수 항에 페널티값 $\alpha$ 을 추가하는데, 이러한 방식을 규제*Regularization* 이라고 부른다

### 7-1. 릿지 회귀 Ridge Regression
- $J_{ridge}=(𝑦−𝑋𝛽)^T(𝑦−𝑋𝛽)+\alpha𝛽^T𝛽$
	- $\beta$는 계수벡터
- 비용함수를 최소화하는 $\beta$ 값을 유도하기 
	- $\cfrac{\partial}{\partial{\beta}}[(𝑌−𝛽^𝑇𝑋)^𝑇(𝑌−𝛽^𝑇𝑋)+\alpha𝛽^𝑇𝛽]$
	- $=\cfrac{∂(𝑌−𝛽^𝑇𝑋)^𝑇(𝑌−𝛽^𝑇𝑋)}{∂𝛽}+\cfrac{∂(\alpha𝛽^𝑇𝛽)}{∂𝛽}$
	- $=−2𝑋^𝑇(𝑌−𝛽^𝑇𝑋)+2\alpha𝛽=0$
	- 𝛽=$(𝑋^𝑇𝑋+\alpha𝐼)^{−1}𝑋^𝑇𝑌$
-  릿지회귀의 예
```python
#릿지에 사용될 alpha 값을 정의하기  
alphas=[0,0.1,1,10,100]  
  
#alpha_list 값을 반복하면서 alpha에 따른 평균 rmse 를 구한다  
for alpha in alphas:  
    ridge= Ridge(alpha=alpha)  
  
    #cross_val_score 를 이용하여 5폴드의 평균 RMSE 값을 계산한다  
    neg_mse_scores= cross_val_score(ridge,X_data,y_target,scoring='neg_mean_squared_error',cv=5)  
    avg_rmse=np.mean(np.sqrt(-1*neg_mse_scores))  
    print(f'alpha :{alpha} 일때 5folds의 평균 RMSE는 {avg_rmse:.3f}')  
  
"""  
alpha :0 일때 5folds의 평균 RMSE는 5.829
alpha :0.1 일때 5folds의 평균 RMSE는 5.788
alpha :1 일때 5folds의 평균 RMSE는 5.653
alpha :10 일때 5folds의 평균 RMSE는 5.518
alpha :100 일때 5folds의 평균 RMSE는 5.330
"""  
  
#각 alpha에 따른 회귀 계수 값을 시각화하기 위해 5개의 열로 된 맷플롯립 축을 생성한다  
fig,axs= plt.subplots(figsize=(18,6),nrows=1,ncols=5)  
#각 alpha에 따른 회귀 계수 값을 데이터로 저장하기 위한 dataframe을 생성한다  
coef_df= pd.DataFrame()  
  
#alpha 리스트값을 차례로 입력하여 회귀 계수 값 시각화 및 데이터를 저장한다. pos는 axis의 위치를 지정한다  
for pos,alpha in enumerate(alphas):  
    ridge= Ridge(alpha=alpha)  
    ridge.fit(X_data,y_target)  
    #alpha값에 따른 피처별로 회귀 계수 값을 Series로 변환하고 이를 DataFrame의 컬럼으로 추가  
    coef= pd.Series(data=ridge.coef_,index=X_data.columns)  
    colname='alpha'+str(alpha)  
    coef_df[colname]=coef  
    #막대 그래프로 각 alpha값에서의 회귀 계수를 시각화. 회귀 계수가 높은 순으로 표현한다  
    coef= coef.sort_values(ascending=False)  
    axs[pos].set_title(colname)  
    axs[pos].set_xlim(-3,6)  
    sns.barplot(x=coef.values,y=coef.index,ax=axs[pos])  
    #for 문 바깥에서 맷플롯립의 show 호출 및 alpha에 따른 피쳐벌 회귀 계수를 데이터프레임으로 표시  
plt.show()
```
![ridge_graph](ridge_graph.png)


### 7-2. 라쏘 회귀 Lasso Regression
- $J_{lasso}=(𝑦−𝑋𝛽)^T(𝑦−𝑋𝛽)+\alpha\sum_{i=1}^{n}|\beta_i|$
	- $\beta$는 계수벡터
- Ridge($L_2$) 규제가 회귀 계수의 크기를 감소시키는 것이 목적인 것에 반해 Lasso($L_1$) 규제는 불필요한 회귀 계수를 급격하게 감소시켜 0으로 만들고 제거한다.


### 7-3. 엘라스틱넷 회귀 ElasticNet Regression
- $L_1$ 규제와 $L_2$ 규제를 결합한 회귀 방식이다
- $J_{elastic\_net}=(𝑦−𝑋𝛽)^T(𝑦−𝑋𝛽)+r\alpha\sum_{i=1}^{n}|\beta_i|+(1-r)\alpha𝛽^T𝛽$
	- $\beta$ 는 계수벡터, $r$ 는 두 규제항의 혼합비율
- 라쏘 회귀가 중요 피처들을 셀렉션하고 다른 피처들의 회귀계수를 0으로 만드는 과정에서 $\alpha$ 값에 따라 회귀계수값이 급격하게 변동할 수 있는데, 이를 완화하기 위해 $L_2$ 규제를 추가한 것이다.
- $L_1$, $L_2$ 규제가 결합되어 수행시간이 상대적으로 오래걸린다


## 8. 선형 회귀 모델을 위한 데이터 전처리
- 선형 회귀 모델
	- 독립변수와 종속변수간 선형의 관계가 있다는 것을 가정한다. 
	- 종속 변수 𝑦가 독립 변수 𝑥의 선형 조합으로 결정되는 기댓값과 고정된 분산 $𝜎^2$을 가지는 가우시안 정규 분포라고 가정한다  ($y \sim \mathscr{N}(\beta^Tx,\sigma^2)$)
- 타깃값이 정규 분포가 아니라 특정 값의 분포가 치우친 왜곡skew된 형태의 분포일 경우 예측 성능에 부정적인 영향을 미칠 수 있다
- sklearn의 스케일링 작업
	- `StandardScaler`: 평균이 0, 분산이 1인 표준 정규 분포를 가진 데이터셋으로 변환시키는 방법
	- `MinMaxScaler`: 최솟값이0이고 최댓값이1인 데이터셋으로 변환시키는 방법
- 그외 스케일링 작업
	- log transformation: 종속변수 대신, 종속변수의 로그값에 대한 선형회귀를 구하는 방식으로 일반적으로 왜도와 첨도를 줄이고, 정규성을 높인다
		- 이때 일반적으로 `np.log(x)` 함수 대신 `np.log1p(x)=np.log(x+1)` 함수를 사용하여, $x\sim 0$ 에서 언더 플로우되는 문제를 예방한다


## 9. 로지스틱 회귀
- 선형회귀방식을 분류에 적용한 알고리즘이다
- 많은 자연, 사회현상에서 특정 변수의 확률값은 시그모이드 함수의 형태 $y=\cfrac{1}{1+e^{-x}}$ 의 형태를 띄고 있다.  그러므로 다음과 같은 모델로 가정하여 회귀를 수행한다
	- $log(\cfrac{P(y=1|X)}{1-P(y=1|X)})=\beta_0+\sum_{j=1}^{p}\beta_jx_j$
	- $P(y=1|x)=\cfrac{exp(\beta_0+\sum_{j=1}\beta_jx_j)}{1+exp(\beta_0+\sum_{j=1}\beta_jx_j)}$
-   `sklearn` :   `LogisticRegression` 의 다양한 최적화 방법
	- `lbfgs`: `solver`의 기본 설정값. 메모리 공간이 절약되고, CPU 코어수가 많다면 병렬로 수행할 수 있다
	- `liblineaer`: `sklearn` 버전 0.21 까지의 `solver` 기본 설정값. 다차원이고, 적은 데이터 셋에서 효과적으로 동작하지만 국소 최적화에 이슈가 있고, 병렬로 최적화할 수 없다
	- `lewton-cg`: 좀 더 정교한 최적화가 가능하지만 대용량 데이터에서 속도가 많이 느린 방법
	- `sag`: Stochastic Average Gradient 방식으로 경사 하강법 기반 최적화이다. 대용량 데이터에서 빠르게 최적화가 가능하다
	- `saga`: `sag`와 유사한 최적화 방식이며 `L1` 정규화를 가능하게 해준다


## 10. 회귀 트리
- 독립변수 벡터공간$\{x_1,x_2,...,x_n\}$이 있다고 하자. 분기별로  $x_j>val_j$ 의 조건으로 영역을 쪼개는데, 이는 독립변수 벡터공간을 격자로 분할하여 각 분할된 영역에 대한 평균 종속변수값을 예측하는 방식이다 

