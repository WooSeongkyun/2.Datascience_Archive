w## CH4  분류
- 지도학습의 대표적 유형중 하나로, 데이터를 통해 학습된 모델이 새로운 데이터 값이 주어졌을 때 그것의 레이블 값을 예측하는 것, 그 기술 및 이론
- 분류의 머신러닝 알고리즘 종류   
	- 나이브 베이즈 Naive Bayes: 베이즈 통계와 생성모델에 기반한 알고리즘 
	- 로지스틱 회귀 Logisitc Regression: 독립변수와 종속변수의 선형 관계성의 기반한 회귀 알고리즘
	- 결정 트리 Decision Tree: 데이터 균일도에 따른 규칙 기반의 알고리즘
	- 서포트 벡터 머신 Support Vector Machine:개별 클래스 간의 최대 분류 마진을 효과적으로 찾아주는 알고리즘
	- 최소 근접 알고리즘 Nearest Neighbor: 근접 거리를 기준으로 하는 최소 근접 알고리즘
	- 신경망 Neural Network 
	- 앙상블 Ensemble: 서로 다른 머신러닝 알고리즘을 결합하여 경합시킨 알고리즘 형태
		- 이 책(파이썬 머신러닝 완벽가이드: 권철민 저 2판)에서는 다양한 알고리즘 중 *앙상블 방법*을 집중적으로 다룬다.비정형데이터에선 딥러닝이 머신러닝계를 선도하지만 이를 제외한 정형 데이터의 예측 분석에선 앙상블이 매우 높은 예측 성능을 가지고 있기 때문이다.
	- 앙상블의 방식
		- 배깅 Bagging: 대표적으로 랜덤 포레스트 Random Forest 방식이 있다. 뛰어난 예측 성능, 상대적으로 빠른 수행 시간, 유연성 등으로 많은 분석가등으로 애용되고 있다
		- 부스팅 Boosting: 대표적으로 그래디언트 부스팅 Gradient Boosting등이 있으나 이 알고리즘은 수행 시간이 너무 오래걸리는 단점을 가지고 있었다. 그 후 XGBoost, LightGBM등으로 기존 그래디언트부스팅의 예측 성능을 발전시키면서도 수행 시간을 단축시킨 알고리즘이 개발되고 있다

## 2. 결정 트리
- 데이터에 있는 규칙을 학습하여, 트리 기반의 분류규칙을 생성하는 알고리즘이다
- 트리 관련 용어
    - 루트$root$
        - 트리의 가장 위쪽에 있는 노드
    - 리프 $leaf$ / 단말 노드 $terminal\,\,node$ / 외부 노드 $external\,\,node$
        - 트리의 가장 아래쪽에 있는 노드
		- *결정 트리 알고리즘에선 결정된 클래스 값*을 의미한다
    - 비단말 노드 $non-terminal\,\,node$/ 내부 노드$internal\,\,node$ 
        - 리프를 제외한 모든 노드(루트를 포함한다)
		- *결정 트리 알고리즘에선 규칙 노드(Decision Node)* 라고 칭한다
    - 노드의 가족관계
        - 어떤 노드 $y$ 가 가지로 연결된 아래쪽 노드 $x_{1},x_{2},...,x_{n}$을 가지고 있다면
            - $y$를 $x_{1},x_{2},...x_n$의 부모 $parent$라고 부른다
            - $x_1,x_2,...,x_n$을 $y$ 의 자식 $child$이라 부른다
            - $x_1,x_2,...,x_n$는 서로 형제 $sibling$이라 부른다
            - $y$는 $n$개의 자식을 가지고 있고, 이를 다시 말해 차수 $degree=n$이라고 표현한다
            - $y$에서 위쪽 방향으로만 가지를 타고 이동하였을 때 접하는 노드를 $y$의 조상$ancestor$라고 부른다
            - $y$에서 아래쪽 방향으로만 가지를 타고 이동하였을 때 접하는 노드를 $y$의 자손$descendant$라 부른다
            - $y$와 $y$의 자손으로 구성된 트리를 서브트리$subtree$라 부른다
    - 레벨 $level$
        - 어떤 노드 $y$가 루트를 향하여 위쪽 방향으로만 가지를 타고 이동하였을 때 몇번만에 도착하였는지를 측정한 값
    - 높이 $height$
        - 리프에서 루트까지의 레벨
    - 빈 트리 $null\,\,tree$
        - 노드와 가지가 전혀 없는 트리를 빈 트리라고 부른다
    - 순서 트리$ordered\,\,tree$와 무순서 트리$unordered\,\,tree$
        - 형제 노드의 순서관계를 구별하면 순서트리, 순서를 구별하지 않으면 무순서 트리라고 부른다

## 평가 기준: 엔트로피와 지니 계수

### 엔트로피
- 확률분포가 가지는 정보량을 수치로 표현한 것이다. 
- 확률분포에서 특정한 값이 나올 확률이 높아지고, 다른 값이 나올 확률이 낮을수록 엔트로피의 값이 작고 반대로 대부분이 비슷한 경우 엔트로피는 높아지는 경향성을 가진다
- 엔트로피의 정의
	- 이산확률변수인 경우 
		- $H[Y]= -\sum_{k=1}^{K}p(y_k)log_2p(y_k)$ 
			- 이때 $K$ 는 $X$ 가 가질 수 있는 클래스의 총 수, $p(y_k)$ 는 확률질량함수이다
	- 연속확률변수 인경우 
		- $H[Y]=-\int_{-\infty}^{\infty}p(y)log_2p(y)dy$
		- 이떄 $P(y)$는 확률밀도함수이다
- 엔트로피의 성질
	- 엔트로피는 확률분포함수를 입력으로 받아 숫자로 출력하는 범함수 $functional$ 이다.
	- 엔트로피 계산에서 $p(y)=0$ 인경우 로그값이 정의되지 않으므로 다음과 같은 극한값을 로피탈의 정리를 통하여 구한다. $lim_{p\rightarrow 0} \,\, plog_2 p=0$

### 지니 계수
- 불순도를 측정하는 지표로서 데이터의 통계적 분산정도를 수치로 표현한 것이다
- 지니계수의 정의
	- $G(S)= 1- \sum_{i=1}^{C}p_i^2$
	- $S$ 는 이미 발생한 사건의 모음, $C$는 사건의 갯수를 의미한다
	- [레퍼런스](https://leedakyeong.tistory.com/entry/의사결정나무Decision-Tree-CART-알고리즘-지니계수Gini-Index란)

### 결정트리의 선택 : 결정 트리 파라미터를 결정
- `min_samples_split`
	- 노트를 분할하기 위한 최소한의 샘플 데이터 수를 정하는 파라미터로 과적합을 제어하는데에 사용한다.
	- 디폴트 값은 2이다
- `min_samples_leaf`
	- 말단 노드가 되기 위한 최소 샘플 데이터 수를 정하는 파라미터로 과적합을 제어하는데 사용한다 .
	- 그러나 비대칭적 데이터의 경우 특정 클레스의 데이터가 극도로 적을 수 있으니 이 경우는 유의해야 한다
- `max_features`
	- 최적의 분할을 위해 고려할 최대 피처 갯수를 정한다. 
	- 디폴트는 None으로 데이터의 모든 피처를 활용한다
	- int 로 지정하면 대상 피처의 갯수를, float으로 지정하면 전체 피처중 대상 피처의 퍼센트를 활용한다
- `max_depth`
	- 트리의 최대 깊이를 정한다
	- 디폴트는 None으로 완벽한 클래스 결정 값이 될 때까지 깊이를 계속 키운다
- `max_leaf_nodes`
	- 말단 노드(leafts)의 최대 갯수를 정한다

### 결정 트리 모델의 시각화
```python
#DecisionTreeClassifier 생성하기
dt_clf= DecisionTreeClassifier(random_state=156)


#붗꽃 데이터를 로딩하고, 학습과 테스트 데이터 셋으로 분리하기
iris_data= load_iris()
X_train, X_test, y_train, y_test= train_test_split(iris_data.data,iris_data.target,test_size=0.2,random_state=11)

#DecisionTreeClassifier 학습
dt_clf.fit(X_train,y_train)

#graphviz 모듈을 이용하여 시각화하기

from sklearn.tree import export_graphviz

export_graphviz(dt_clf, out_file='tree.dot', class_names= iris_data.target_names,\
feature_names=iris_data.feature_names, impurity=True, filled=True)

import graphviz
with open('tree.dot') as f:
dot_graph= f.read()
graphviz.Source(dot_graph)

______________________________
result)
```
<iframe title="Embedded cell output" src="https://embed.deepnote.com/59f71f48-e620-40de-82b6-60c121211e54/f4cd2ad2-fb90-4aa8-85a9-e83c1ffe6d7c/2e94912ba5fb4e9c8f75d3ab89526ac3?height=1200" height="500" width="700"/>
-  결정 트리 알고리즘의 중요도를 추출 및 그래프 시각화하기
 <iframe title="Embedded cell output" src="https://embed.deepnote.com/59f71f48-e620-40de-82b6-60c121211e54/f4cd2ad2-fb90-4aa8-85a9-e83c1ffe6d7c/f65d4b0c3839482cb89ae8f04c4d2420?height=975.5625" height="800" width="700"/>
- 2개의 피처값을 나타낸 그래프상에 3개의 클래스가 어떻게 분류되었는지 시각화하기
<iframe title="Embedded cell output" src="https://embed.deepnote.com/59f71f48-e620-40de-82b6-60c121211e54/f4cd2ad2-fb90-4aa8-85a9-e83c1ffe6d7c/907a92188dde4b90bd10374bc0b6222a?height=830" height="800" width="700"/>
### 결정 트리 실습- 사용자 행동 인식 데이터 셋


## 3.  앙상블 학습
- 앙상블 학습이란?
	- 여러개의 분류모델을 만들고, 각 모델들의 예측결과들을 결합하여 보다 정확한 예측결과를 도출해내는 기법이다
	- 이미지,영상,음성등의 비정형 데이터의 분류는 딥러능이 뛰어난 성능을 보이고 있지만, 대부분의 정형 데이터 분류에선 앙상블 학습이 뛰어난 성능을 보이고 있다
	- 현재 Kaggle에서 XGBoost,LightGBM등을 비롯한 다양한 유형의 앙상블 학습등이 머신러닝의 선도 알고리즘으로 인기를 끌고 있다
	- 앙상블 학습의 방식으로 보팅 Voting, 배깅 Bagging, 부스팅 Boosting 이 세가지가 대표적으로 존재한다
	
### 보팅
- 보팅의 유형: 하드 보팅과 소프트 보팅
	- 하드 보팅 Hard Voting: 다수결의 원칙에 따라 결정. 예측한 결과값중 다수의 분류기가 결정한 예측값을 최종 보팅 결과값으로 선정한다
	- 소프트 보팅 Soft Voting: 각 분류기의, 각 레이블에 대한 예측 확률들을 평균내어 가장 높은 값을 최종 보팅 결과값으로 결정하는 방식

### 배깅
- 배깅은 같은 알고리즘이지만, 다른 데이터셋을 갖는 classifier이 개별적으로 학습을 수행한 뒤 보팅으로 최종 결정하는 알고리즘이다. 이때 개별 트리의 데이터셋은 전체 데이터 셋의 일부가 중첩되게 샘플링된다(부트스트래핑bootstrapping 분할 방식)
- 배깅의 유형: 대표적으로 랜덤 포레스트가 있다
	- 일반적으로 가장 앙상블 알고리즘 중 가장 빠른 수행속도를 가지고 있으며 다양한 영역에서 높은 예측 성능을 갖고 있다	
	- 랜덤 포레스트
		- [랜덤 포레스트 예시](https://embed.deepnote.com/b04f8dfd-b00d-42eb-95d2-206667455040/f4cd2ad2-fb90-4aa8-85a9-e83c1ffe6d7c/16d181282a64436e9cceb5a432166d02?height=920)
		- [랜덤 포레스트-하이퍼파라미터 튜닝](https://embed.deepnote.com/b04f8dfd-b00d-42eb-95d2-206667455040/f4cd2ad2-fb90-4aa8-85a9-e83c1ffe6d7c/b2bf40adec034559b3a2fafc2e1123ff?height=493)
		- [Feature_importances sorting 시각화](https://embed.deepnote.com/b04f8dfd-b00d-42eb-95d2-206667455040/f4cd2ad2-fb90-4aa8-85a9-e83c1ffe6d7c/29ed2d766de4462ca9be6692b962a4c5?height=539.890625)
- 랜덤 포레스트의 하이퍼파라미터
	- `n_estimators`: 결정 트리의 개수를 지정한다. default는 10개로 정해져있다. 많이 설정할 수록 좋은 성능을 기대할 수도 있지만 계속 증가시킨다고 해서 성능이 무조건 향상되지는 않을뿐더러, 학습시간이 증가하게 된다
	
	- `max_features`: 고려할 피처들을 정한다. 단 디폴트는 None이 아니라 $sqrt$ (전체 피처갯수)이다	

## GBM(Gradient Boosting Machine)

- GBM이란?
	- 여러개의 약한 학습기weak learner를 순차적으로 학습-예측하면서 잘못 예측한 데이터에 가중치를 부여하면서 오류를 개선 및 학습해 나가는 방식이다
	- GBM에 대표적으로 AdaBoosting(Adaptive Boosting)과 Gradient Boosting이 존재한다
	- ![img](Adaboost.md)
		- 첫번째 학습에서 하단 원 두개가 잘못 분류되어 두번째 학습에서 가중치가 가해진다.
		- 두번째 학습에서 오른쪽 원 3개가 잘못 분류되어 세번째 학습에서 가중치가 가해진다
		- 마지막으로 모든 분류기준을 결합하여 예측을 시행한다
- GBMB
	- 에이다부스트와 유사하나 가중치 업데이트로 경사하강법Gradient Descent를 활용한다
	- GBM의 하이퍼 파라미터
		- `loss`: 경사 하강법에서 사용할 비용함수를 지정한다. default로 `deviance`를 사용한다
		- `learning_rate`: GBM이 학습을 진행할 때 마다 적용하는 학습률을 지정한다. 0~1사이를 적용시킬 수 있으며, 기본값은 0.1이 된다. 너무 작은 값을 적용시키면 수행시간이 길어지게 되고, 너무 큰 값을 적용하게 되면 최소 오류값을 찾지 못해 예측 성능이 떨어질 가능성이 높아진다.
		- `n_estimators`: `weak_learner`의 개수를 지정한다. `weak_learner`가 순차적으로 오류를 보정하므로 개수가 많을수록 예측 성능이 일정 수준까지 좋아질 수 있다. 하지만 개수가 많아질수록 수행시간이 길어진다. default로 100개이다
		- `subsample`: `weak_learner`가 사용하는 데이터의 샘플링 비율이다. 기본값은 1이며, 이는 전체 학습 데이터를 기반으로 학습한다는 의미이다.과적합이 염려되는 경우 `subsmaple`를 1보다 작은 값으로 설정한다

  

## XGBoost

- XGBoost란?
	- 트리 기반 앙상블 학습에서 각광받고 있는 알고리즘중 하나로, Kaggle 상위권 데이터 과학자들이 많이 활용하게 되면서 유명해지게 되었다	
	- 표준 GBM과 달리 과적합 규제기능이 존재한다
	- 지정된 반복횟수가 아니라 교차 검증을 통해 평가 데이터 셋의 평가값이 최적화 되면 반복을 중간에 멈출수 있는 조기 중단 기능이 존재한다
	- 별도로 존재하는 프레임워크이나 사이킷런과 연동하기 위해 전용 래퍼 클래스를 개발하였고, 사이킷런에서 활용가능한 메소드 XGBClassifier는 기존 사이킷런의 하이퍼파라미터명과의 일관성을 위해 다음과 같이 변경되었다
		- `eta` -> `learning_rate`
		- `sub_sample` -> `subsample`
		- `lambda` -> `reg_lambda`
		- `alpha` -> `reg_alpha
- XGBoost의 하이퍼파라미터
	- 일반 파라미터
		- `booster`: booster를 결정한다 gbtree(tree based model) 또는 gblinear(linear model)로 결정한다
		- `silent` : 출력 메세지를 나타내고 싶으면 1 , 아니면 0으로 한다. 디폴트는 0이다
		- `nthrread`: CPU의 실행 스레드 개수를 조정한다. 디폴트는 CPU의 전체 스레드를 사용한다
	- 부스터 파라미터
		- `eta[default=0.3,alias:learning_rate]`: GBM의 학습률과 같은 파라미터. 0과 1사이 값을 지정하며, 부스팅 스텝을 반복적으로 수행할 때에 업데이트 되는 학습률 값이다.
		- `num_boost_rounds`: GBM의 n_estimators와 같은 파라미터이다
		- `min_child_weigh[default=1]`: 트리에서 추가적으로 가지를 나눌지를 결정하기 위한 데이터 weigh의 총합
		- `gamma['default=0, alias:min_split_loss']`:트리의 리프 노드를 추가적으로 나눌지를 결정하는 최소 손실 감소 값. 해당 값보다 loss가 클 경우에 리프 노드를 분리한다
		- `max_depth[default=6]`: 트리 기반 알고리즘의 `max_depth`와 같다. 0을 지정하면 깊이 제한이 없는 것으로 정하는 것이다. 이 값이 클수록 과적합 가능성이 높아지며 일반적으로 3~10 사이 값을 사용한다
		- `sub_sample[default=1']`:GBM의 `subsample`과 같다. 트리가 커져 과적합되는 것을 방지하기 위해 데이터 샘플링하는 비율을 지정한다. 0과 1사이 값을 지정할 수 있다
		- `colsample_bytree[default=1]`:GBM의 max_features와 유사하다. 트리 샘플에 필요한 피처를 임의로 샘플링 하는데 사용한다
		- `lambda[default=1 alias:reg_lambda]`: L2 regularization의 적용값을 지정한다. 값이 클수록 과적합 감소 효과가 있다
		- `alpha[default=0, alias:reg_alpha]`: L1 regularization 적용값이다. 값이 클수록 과적합 감소 효과가 존재한다
		- `scale_pos_weight[default=1]`: 특정 값으로 치우친 비대칭한 클래스로 구성된 데이터 셋의 균형을 유지하기 위한 하이퍼파라미터이다
	- 학습 태스트 파라미터
		- `objective`: 최솟값을 가져야 할 손실함수를 정의한다
		- `binary:logistic`:이진분류일 때 적용한다
		- `multi:softmax` : 다중분류일 떄 적용한다
		- `multi:softprob`: 개별 레이블 클래스에 해당되는 예측 확률을 반환한다
		- `eval_metric`:검증에 사용되는 함수를 정의한다. 기본값은 회귀읜 경우 `rmse`, 분류인 경우 `error`이다.
		- `rmse`: Root Mean Square Error
		- `mae` : Mean Absolute Error
		- `logloss`:Negative log-likelihood
		- `error`:Binary classification error rate(0.5 threshold)
		- `merror`: Multiclass classification error rate
		- `mlogloss`: Multiclass logloss
		- `auc`: Area under Curve

## LightGBM

- LightGMB이란 무엇인가?
	- XGBoost보다 학습에 걸리는 시간이 훨씬 적고, 메모리 사용량도 상대적으로 적다
	- 그럼에도 예측성능은 XGboost와 비슷하며, 기능상의 다양성은 LightBGM이 더 많이 존재한다
	- 다만 10,000건 이하의 적은 데이터셋의 경우 과적합 문제가 발생하기 쉬울 수 있다
	- 어떻게 이런 기술이 가능했는가?
	- 기존 GBM 계열의 트리 분할 방식은 리프 중심 분할(Leaf Wise) 방식이라 하여, 최대한 트리의 균형을 맞추며 분할하기 때문에 트리의 깊이를 최소화되고, 오버피팅을 예방한다 -> 그러나 이 방식은 균형을 맞추는데 시간이 필요하단 단점이 존재한다
	- 그러나 LightGBM은 최대 손실 값(max delta loss)을 가진 리프 노드를 지속적으로 분할하면서 비대칭적인 트리가 생성된다
	
- LightGBM의 하이퍼파라미터
	- `num_iteration[default=100]`:반복 수행하려는 트리의 개수를 지정한다. 크게 지정할수록 예측 성능이 높아질 수 있으나, 너무 크면 과적합으로 성능이 저하될 수 있다
	- `learning_rate[default=0.1]`: 0 ~ 1 사이 값을 지정한다. 부스텡 스텝을 반복적으로 수행할 때 업데이트 되는 학습률 값이다.
	- `max_depth[default=-1]`: 트리 기반 알고리즘의 `max_depth`와 동일하다.
	- `min_data_in_leaf[default=20]`: 최종 결정 클래스인 리프 노드가 되기 위해 최소한으로 필요한 데이터의 수. 과적합을 제어하기 위한 파라미터이다
	- `num_leaves[default=31]`: 하나의 트리가 가질 수 있는 최대 리프 개수
	- `boosting`: 부스팅 트리를 생성할 수 있는 알고리즘
	- `gbdt`:일반적인 그레디언트 부스팅 결정 트리
	- `rf`: 랜덤 포레스트
	- `bagging_fraction[default=1.0]`:트리가 과적합되는 것을 제어하기 위해 데이터를 샘링하는 비율.
	- `feature_fraction['default=1.0']`: 개별 트리를 학습할 때마다 무작위로 선택하는 피처의 비율
	- `lambda_l2[default=0.0]`: L2_regularization 제어를 위한 값, 과적합 제어를 위하여 사용한다
	- `lambda_l1[default=0.0]`: L1_regularization 제어를 위한 값. 과적합 제어를 위하여 사용한다
	- [LightGBM 학습 후 plot_importance()를 이용한 중요도 시각화](https://embed.deepnote.com/b04f8dfd-b00d-42eb-95d2-206667455040/f4cd2ad2-fb90-4aa8-85a9-e83c1ffe6d7c/1abe05ae8d124bc78afb4670e8fe605a?height=965)

## HyperOpt(베이지안 최적화)을 이용한 하이퍼 파라미터 튜닝

- 하이퍼파라미터 튜닝을 위해 Grid_Search(하이퍼파라미터값을 리스트에 담아서, 각각 하나씩 대입하여 최적의 값을 찾는 방법)은 튜닝해야될 하이퍼 파라미터의 개수가 많아질경우 최적화 수행시간이 오래 걸리게 될 수 밖에 없다
- XGBoost, LightGBM은 성능이 매우 뛰어난 알고리즘이지만 하이퍼 파라미터 개수가 다른 알고리즘에 비해 매우 많다. 때문에 실무의 대용량 학습 데이터에 Grid_Search 방식으로 최적 하이퍼파라미터를 찾으려 할경우 많은 시간이 소비될 수 있다
- 베이지안 최적화란 무엇인가?
	- 베이지안 최적화란 함수 모델을 제대로 알 수 없는 블랙박스 형태의 함수에서, 최대 또는 최소 함수 반환값을 만드는 최적 입력값을 가능한 적은 시도를 통해 빠르고 효과적으로 찾는 방식이다
	- 베이지안 확률론에 기반을 둔 최적화 기법으로서, 대체 모델 Surrogate Model과 획득 함수 Acquisition Fucntion으로 구성된다.
	- 대체 모델 Surrogate Model:현재까지 조사된 입력값-함숫결과값 데이터 포인터들 (x_1, f(x1)),...,(x_t, f(xt))을 바탕으로, 미지의 목적 함수의 형태에 대한 확률적인 추정을 수행하는 모델
	- 획득 함수 Acquisition Function:목적 함수에 대한 현재까지의 확률적 추정 결과를 바탕으로, ‘최적 입력값을 찾는 데 있어 가장 유용할 만한’ 다음 입력값 후보를 추천해 주는 함수
- 베이지안 최적화의 단계
	- Step1 : 최초 랜덤하게 하이퍼 파라미터를 샘플링하고 성능결과를 관측한다
	- Step2 : 관측된 값을 기반으로 대체 모델은 최적함수를 추정한다
	- Step3 : 최적함수를 기반으로 획득 함수가 다음으로 관측할 하이퍼 파라미터를 대체모델에 전달한다
	- Step4 : 획득 함수로부터 전달된 하이퍼파라미터를 활용하여 다시 최적 함수를 예측 추정한다
- hyperopt.hp의 argument
	- `hp.quniform(label,low,high,q)`: label로 지정된 입력값 변수를 최솟값 `low`부터 최댓값 `high`까지 `q`간격으로 검색한다
	- `hp.uniform(label,low,high)`: 최솟값 `low`에서 최댓값 `high`까지 정규 분포 형태의 검색 공간 설정
	- `hp.randint(label,upper)`: 0부터 최댓값 `upper`까지 random한 정수값으로 검색 공간을 설정
	- `hp.loguniform(label,low,high)':exp(uniform(low,high)) 값을 반환하며, 반환 값의 log 변환 된 값은 정규분포를 띈다
	- `hyperopt.fmin` : 최솟값의 목적함수를 만드는 하이퍼파라미터를 찾는 메소드.
	- `fn`: 목적함수
	- `space`:`search_space`와 같은 검색 공간 딕셔너리
	- `algo`:베이지안 최적화 적용 알고리즘. default로 tpe.suggest를 사용한다
	- `max_evals`: 최적 입력값을 찾기 위한 입력값 시도 횟수
	- `trials`:최적 입력값을 찾기 위한 시도
	- `rstate`:난수의 seed
	
	- `HyperOpt`를 이용한 `XGBoost` 하이퍼 파라미터 최적화

- 검색 공간에서 목적 함수로 사용되는 모든 인자들은 실수형이고, XGBOostClassifier에 정수형 파라미터값으로 사용하기 위해선 정수형으로 형변환하는 과정이 필요하다


## 4. 언더샘플링과 오버샘플링
- 레이블이 불균형한 분포를 가진 데이터셋을 학습시키는 경우 이상 레이블에 대해 제대로 학습하지 못하기 때문에 제대로된 데이터 검출을 하기 어려울 가능성이 높다.
- 지도학습에서 불균형한 레이블값 분포로 인한 문제점을 해결하기 위해 적절한 학습데이터를 확보하기 위한 방법을 사용한다.
	- 언더 샘플링 oversampling
		- 많은 데이터 셋(정상 레이블 셋)을 적은 데이터 셋(비정상 레이블 셋) 수준으로 수를 줄이는 방식이다. 이 경우 불균형 분포 문제를 확실하게 개선할 수 있겠지만, 너무 많은 정상 레이블 데이터를 감소시켜, 제대로된 학습을 수행하기 어려울 수 있다
	- 오버 샘플링 oversampling
		- 적은 데이터를 증식시켜 학습을 위한 충북한 데이터를 확보하는 방법이다. 동일값을 단순 복제하면 과적합을 야기하므로, SMOTE라는 방법에선 개별 데이터들의 K 최근접 이웃을 찾아, 그 이웃들간 차이를 일정 값으로 만들어서 기존 데이터들과 약간씩 차이가 나는 새로운 데이터를 생성한다


## 5.  스태킹 앙상블 Stacking Ensemble
- 개별 알고리즘이 예측한 데이터를 기존의 데이터와 추가하여 다시 예측하는데 사용한다
- 이론적 설명
	- $m$ 개의 row와 $n$ 개의 피처를 갖는 $\textbf{M}_{m \times n}$ 의 데이터셋이 있다고 하자. 
	- 개별적 모델로 학습을 시키면 $y_{m \times 1}$ 의 레이블 값을 반환할 것이다
	- 각 모델별로 도출된 예측 레이블값을 합친다. $(y_{(1),m \times 1},y_{(2),m \times 1},...,y_{(),m \times 1})$
	- 이렇게 합쳐진 새로운 데이터 셋을 가지고 최종 예측을 한다
- CV 셋 기반의 스태킹
	- K-Fold의 Validation-Fold의 에측값을 메타 모델을 위한 학습데이터로, 학습 데이터로 학습된 개별 모델과 테스트 데이터를 활용한 예측값을 메타 모델을 위한 테스트 데이터로 사용한다
	
	

