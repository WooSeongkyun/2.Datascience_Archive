## CH2 데이터 전처리

## 1. 붓꽃 품종 예측하기
- 분류 classification은 대표적인 지도학습 supervised learning 방법이다
- 지도학습 방법은  정답 데이터를 제공하면서 모델을 학습시키는 방법이다

- 이번 예제에서 사용되는  `sklearn` 라이브러리속 모듈
	- `sklearn.datasets`: 사이킷런에서 제공하는 데이터셋이 저장되어 있는 모듈
	- `sklearn.model_selection`: 데이터를 학습 데이터와 검증 데이터, 예측 데이터로 분리하거나 최적의 하이퍼 파라미터로 평가하기 위해 사용되는 함수들이 저장되어 있는 모듈
		- `sklearn.model_selection import train_test_split` : 데이터를 학습용과 테스트용으로 분리시켜주는 함수
		- `shuffle=True`: 데이터를 분리하기전에 데이터를 미리 섞을 지 정하는 옵션
	-   `sklearn.tree`: 트리 기반 ML 알고리즘이 저장되어 있는 모듈
		- `sklearn.tree.DecisionTreeClassifier`: 의사결정 트리 알고리즘 메소드

### 1. 데이터 전처리
- `sklearn`에서 `iris` 데이터 불러오기
	- `iris.data` : `iris`의 독립변수 데이터
	- `iris.target`: `iris`의 종속변수 데이터(정수로 표기됨)
	- `iris.target_names`: 종속변수 데이터(문자열로 표기됨)
	- `iris.feature_names`: 피쳐명(수학-계수/데이터프레임-컬럼명)
- 가져온 데이터를 데이터프레임에 넣어 정리하기
```python
iris_df= pd.DataFrame(data=iris_data,columns=iris.feature_names)  
iris_df['label']=iris.target
```
- 데이터를 학습용 데이터와 테스트용 데이터로 분리하기
```python
X_train, X_test, y_train, y_test = train_test_split(iris_data, iris_label,test_size=size_val,random_state=11)
```
- 
	- `train_test_split`의 `argument`로 독립변수,종속변수,나누어질 테스트 데이터셋의 비율을 필요로 한다

### 2. 머신러닝 알고리즘을 활용하여 학습 및 예측하기
- DecisionTreeClassifier 알고리즘 활용하기
	-  모델 객체 생성$\rightarrow$ 학습 수행 $\rightarrow$ 예측 수행 $\rightarrow$ 정확도 평가
```python 
#모델 생성하기  
DTC= DecisionTreeClassifier(random_state=11)  
#모델 학습시키기  
DTC.fit(X_train,y_train)  
#학습이 완료된 모델 객체와 테스트 데이터셋을 활용하여 예측하기  
pred= DTC.predict(X_test)  
#정확도 평가하기  
print(f'예측 정확도: {accuracy_score(y_test,pred):.3f}')

________________________________________
result)
예측 정확도: 0.933
```

## 2. 사이킷런의 기반 프레임워크 
- Estimator
	-  Classifier : 분류 알고리즘을 구현한 클래스
	-  Regressor: 회귀 알고리즘을 구현한 클래스 
	- Estimator는 학습을 위하여 fit(), 예측을 위하여 predict() 메소드를 사용하도록 통일하였다
- 관련 용어들
	- Hyperparameter : 학습 이전에 조정되며, 모델 학습에 영향을 주는 변수
	- parameter: 학습에 의해 결정되어 예측에 활용되는 변수

### 사이킷런의 주요 모듈
- 예제 데이터 
	- `sklearn.datasets`: 사이킷런에 내장되어 있는 데이터셋을 제공한다. 데이터셋은 딕셔너리 형태로 주어진다
- 피처 처리
	- `sklearn.preprocessing`: 데이터 전처리에 필요한 다양한 가공 기능을 제공한다
		- 문자열을 숫자형 코드로 인코딩
		- 정규화
		- 스케일링
	- `sklearn.feature_selection` : 영향력이 큰 피쳐가 무엇인지 특정 기준으로 판별하여 취사선택할 수 있게 해주는 기능을 제공한다
- 차원 축소
	- `sklearn.decomposition` : 차원 축소와 관련된 알고리즘을 제공한다
- 데이터 분리/검증/파라미터 튜닝
	- `sklearn.model_selection` : 교차 검증을 위한 학습/테스트용 데이터 분리, 최적 파라미터 추출을 위한 API등을 제공한다
- 평가
	- `sklearn.metrics` : 학습된 모델의 예측 성능 측정 방법을 제공한다
- 머신러닝 알고리즘
	- `sklearn.ensemble` : 앙상블 알고리즘 제공
	- `sklearn.linear_model` : 회귀 관련 알고리즘을 제공
		- 선형 회귀
		- 릿지 Ridge 회귀
		- 라쏘 Lasso 회귀
		- 로지스틱 회귀 
	- `sklearn.naive_bayes` : 나이브 베이즈 알고리즘 제공
	- `sklearn.neighbors` : 최근접 이웃 알고리즘 제공
	- `sklearn.svm` : 서포트 벡터 머신 알고리즘 제공
	- `sklearn.tree` : 의사 결정 트리 알고리즘 제공
	- `sklearn.cluster` : 비지도 클러스터링 알고리즘 제공


## 3.`model_selection` 모듈 
- 과적합overfitting
	- 모델이 학습 데이터에 과도하게 최적화되어, 학습 데이터 외의 다른 데이터로 예측할 때엔 예측 성능이 과도하게 떨어지는 현상. 
	- 만약 학습 데이터와 테스트 데이터를 고정한 채로 평가 및 수정하다보면 테스트 데이터에만 최적의 성능을 발휘할 수 있도록 편향되게 모델을 유도하는 경향이 생긴다. 이런 경우 제 3의 데이터에 대한 과적합 문제가 발생하는 것이다
	- 이러한 문제점을 해결하기 위해 교차검증을 통해 다양한 학습과 평가를 수행한다
- 교차 검증
	- 데이터를 훈련/ 테스트 데이터로 나누고 훈련 데이터의 일부를 검증 데이터로 할당한다. 테스트 데이터는 모든 학습/검증 과정이 완료된 이후 최종적으로 성능 평가를 위해 사용하는 데이터 셋이 된다
- K-폴드 교차 검증
	- 우선 훈련 데이터를 5등분 한다. 4조각 중 하나를 훈련 데이터로, 한조각을 검증 데이터로 하는데 한번의 평가를 마칠때마다 검증 데이터를 다음 조각으로 바꾼다
	- 예시)
```python
#전체 데이터는 150 => 학습은 150*4/5=120, 검증은 30n_iter=0  
# KFold 객체의 split()을 호출하면 폴드용 학습용, 검증용 테스트의 로우 인덱스를 array로 반환한다  
for train_index, test_index in kfold.split(features):  
    #kfold.split()으로 반환된 인덱스를 이용해 학습용, 검증용 테스트 데이터 추출  
    X_train, X_test = features[train_index], features[test_index]  
    y_train, y_test= label[train_index], label[test_index]  
    #학습 및 예측  
    DTC.fit(X_train,y_train)  
    pred=DTC.predict(X_test)  
    n_iter= n_iter+1  
    #반복시마다 정확도를 측정한다  
    accuracy= np.round(accuracy_score(y_test,pred),4)  
    train_size= X_train.shape[0]  
    test_size= X_test.shape[0]  
    print(f'{n_iter} 교차 검증 정확도: {accuracy} 학습 데이터 크기: {train_size} 검증 데이터 크기{test_size}')  
    print(f'{n_iter} 검증 세트 인덱스{test_index}')  
    cv_accuracy.append(accuracy)  
  
#반복할 때마다 계산된 정확도를 평균한다  
print(f'평균 검증 정확도{np.mean(cv_accuracy)}')

__________________________________________________________
result)
1 교차 검증 정확도: 1.0 학습 데이터 크기: 120 검증 데이터 크기30
1 검증 세트 인덱스[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29]
2 교차 검증 정확도: 0.9667 학습 데이터 크기: 120 검증 데이터 크기30
2 검증 세트 인덱스[30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53
 54 55 56 57 58 59]
3 교차 검증 정확도: 0.8667 학습 데이터 크기: 120 검증 데이터 크기30
3 검증 세트 인덱스[60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83
 84 85 86 87 88 89]
 4 교차 검증 정확도: 0.9333 학습 데이터 크기: 120 검증 데이터 크기30
4 검증 세트 인덱스[ 90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107
 108 109 110 111 112 113 114 115 116 117 118 119]
5 교차 검증 정확도: 0.7333 학습 데이터 크기: 120 검증 데이터 크기30
5 검증 세트 인덱스[120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137
 138 139 140 141 142 143 144 145 146 147 148 149]
평균 검증 정확도0.9

``` 
- Stratified K 폴드 검증 
	- 일반적으로 각 카테고리별 데이터의 수가 균질하지 않기 때문에 범용적으로 사용되는 
```python
from sklearn.model_selection import StratifiedKFold    
skf= StratifiedKFold(n_splits=3)  
n_iter=0  
  
#레이블 분포도에 따라 학습/검증 데이터를 나누기 때문에 split argument에 레이블 데이터도 필요하다  
for train_index, test_index in skf.split(iris_df, iris_df['label']):  
    n_iter = n_iter +1  
    label_train = iris_df['label'].iloc[train_index]  
    label_test = iris_df['label'].iloc[test_index]  
    print(f'교차 검증: {n_iter}')  
    print(f'학습 레이블 데이터 분포\n', label_train.value_counts())  
    print(f'검증 레이블 데이터 분포\n', label_test.value_counts())

_____________________________________________________
result)
교차 검증: 1
학습 레이블 데이터 분포
 2    34
0    33
1    33
Name: label, dtype: int64
검증 레이블 데이터 분포
 0    17
1    17
2    16
Name: label, dtype: int64
교차 검증: 2
학습 레이블 데이터 분포
 1    34
0    33
2    33
Name: label, dtype: int64
검증 레이블 데이터 분포
 0    17
2    17
1    16
Name: label, dtype: int64
교차 검증: 3
학습 레이블 데이터 분포
 0    34
1    33
2    33
Name: label, dtype: int64
검증 레이블 데이터 분포
 1    17
2    17
0    16
Name: label, dtype: int64
```
- 교차 검증 활용 예시
``` python
from sklearn.model_selection import StratifiedKFold  
  
skf= StratifiedKFold(n_splits=3)  
n_iter=0  
cv_accuracy=[]  
  
#레이블 분포도에 따라 학습/검증 데이터를 나누기 때문에 split argument에 레이블 데이터도 필요하다  
for train_index, test_index in skf.split(features,label):  
    #split()으로 반환된 인덱스를 이용해 학습용, 검증용 테스트 데이터를 추출하기  
    X_train, X_test= features[train_index], features[test_index]  
    y_train, y_test = label[train_index], label[test_index]  
    #학습 및 예측  
    DTC.fit(X_train, y_train)  
    pred= DTC.predict(X_test)  
  
    #반복시마다 정확도 측정  
    n_iter=n_iter +1  
    accuracy= np.round(accuracy_score(y_test,pred),4)  
    train_size= X_train.shape[0]  
    test_size= X_test.shape[0]  
    print(f'\n {n_iter} 교차 검증 정확도:{accuracy}, 학습 데이터 크기: {train_size}, 검증 데이터 크기:{test_size}')  
    print(f'{n_iter} 검증 세트 인덱스 {test_index}')  
    cv_accuracy.append(accuracy)  
  
#교차 검증별 정확도 및 평균 정확도 계산  
print('\n 교차 검증별 정확도:' ,np.round(cv_accuracy,4))  
print('평균 검증 정확도:',np.round(np.mean(cv_accuracy),4))

___________________________________________________________
result)
1 교차 검증 정확도:0.98, 학습 데이터 크기: 100, 검증 데이터 크기:50
1 검증 세트 인덱스 [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  50
  51  52  53  54  55  56  57  58  59  60  61  62  63  64  65  66 100 101
 102 103 104 105 106 107 108 109 110 111 112 113 114 115]

 2 교차 검증 정확도:0.94, 학습 데이터 크기: 100, 검증 데이터 크기:50
2 검증 세트 인덱스 [ 17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  67
  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82 116 117 118
 119 120 121 122 123 124 125 126 127 128 129 130 131 132]

3 교차 검증 정확도:0.98, 학습 데이터 크기: 100, 검증 데이터 크기:50
3 검증 세트 인덱스 [ 34  35  36  37  38  39  40  41  42  43  44  45  46  47  48  49  83  84
  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 133 134 135
 136 137 138 139 140 141 142 143 144 145 146 147 148 149]

 교차 검증별 정확도: [0.98 0.94 0.98]
평균 검증 정확도: 0.9667
```

## 4. 교차검증 모듈 `cross_val_score`
- 일반적으로 교차검증을 시행하기 위해선 
	1. 폴드 세트를 설정한다
	2. for 루프에서 반복으로 훈련 및 테스트 데이터의 인덱스를 추출한다
	3. 반복적으로 모델 학습과 예측을 수행하고 예측 성능을 반환한다
- `cross_val_score` 는 이런 데이터 분리, 모델 학습, 정확도 평가의 과정을 한꺼번에 시행해준다
```python
iris_data= load_iris()  
DTC= DecisionTreeClassifier(random_state=156)  
  
data= iris_data.data  
label= iris_data.target  
  
#성능 지표는 정확도(accuracy), 교차 검증 세트는 3개로 정한다  
scores=cross_val_score(DTC,data,label,scoring='accuracy',cv=3)  
print('교차 검증별 정확도:',np.round(scores,4))  
print('평균 검증 정확도',np.round(np.mean(scores),4))
___________________________
result)
교차 검증별 정확도: [0.98 0.94 0.98]
평균 검증 정확도 0.9667
```

## 5.`GridSearchCV`- 교차검증과 최적 하이퍼파라미터 튜닝을 한번에 하기 
- 사이킷런은 `GridSearchCV `API를 통하여 머신러닝 알고리즘에 사용되는 하이퍼 파라미터를 순차적으로 입력하면서 편리하게 최적의 파라미터를 탐색할 수 있는 방법을 제공한다. 
``` python
grid_parameters= {'hyper_par1':[x_1,x_2,...,x_n],
				  'hyper_par2':[y_1,y_2,...,y_m]}
```
- 이라 설정할 경우 `hypter_par1`과 `hyper_par2`의 이중 for문 형식으로 데이터가 차례대로 대입되게 된다
-  그리드서치 예시
- 학습시킨 `grid_dtree`의 `.cv_results_`를 데이터프레임으로 만들었을 때
	- `params` 열은 수행에 적용된 개별 하이퍼 파라미터이다
	- `rank_test_score` 는 하이퍼 파라미터별로 성능이 좋은 score의 순위이다
	- `mean_test_score`는 개별 하이퍼 파라미터별로 CV 폴딩 테스트셋에 대한 평가 평균값이다
```python
from sklearn.datasets import load_iris  
from sklearn.tree import DecisionTreeClassifier  
from sklearn.model_selection import GridSearchCV  
  
#데이터를 로딩하고 학습 데이터와 테스트 데이터를 분리시킨다  
iris_data= load_iris()  
X_train, X_test, y_train, y_test = train_test_split(iris_data.data,iris_data.target,test_size=0.2,random_state=121)  
  
dtree=DecisionTreeClassifier()  
  
#하이퍼 파라미터를 딕셔너리 형태로 만든다  
parameters= {'max_depth':[1,2,3],'min_samples_split':[2,3]}  
  
#parameter_grid의 하이퍼 파라미터를 3개의 train<test set fold로 나누어 테스트를 수행한다  
grid_dtree= GridSearchCV(dtree,param_grid=parameters, cv=3 ,refit=True)  
  
#붓꽃 학습 데이터로 param_grid의 하이퍼 파라미터를 순차적으로 학습 및 평가  
grid_dtree.fit(X_train,y_train)  
  
#GridsearchCV 결과를 추출하여 DataFrame으로 변환시킨다  
scores_df= pd.DataFrame(grid_dtree.cv_results_)  
scores_df[['params','mean_test_score','rank_test_score','split0_test_score','split1_test_score','split2_test_score']]

______________________________________________________
result)
params/mean_test_score/rank_test_score/split0_test_score/split1_test_score/split2_test_score
0,"{'max_depth': 1, 'min_samples_split': 2}",0.700000,5,0.700,0.7,0.70
1,"{'max_depth': 1, 'min_samples_split': 3}",0.700000,5,0.700,0.7,0.70
2,"{'max_depth': 2, 'min_samples_split': 2}",0.958333,3,0.925,1.0,0.95
3,"{'max_depth': 2, 'min_samples_split': 3}",0.958333,3,0.925,1.0,0.95
4,"{'max_depth': 3, 'min_samples_split': 2}",0.975000,1,0.975,1.0,0.95
5,"{'max_depth': 3, 'min_samples_split': 3}",0.975000,1,0.975,1.0,0.95
```
- 최적 파라미터와 최고 정확도 출력하기
```python
print('GridSearchCV 최적 파라미터는 ',grid_dtree.best_params_)  
print(f'GridSearchCV 최고 정확도는 {grid_dtree.best_score_:.3f}')

________________________________________________________
result)
GridSearchCV 최적 파라미터는  {'max_depth': 3, 'min_samples_split': 2}
GridSearchCV 최고 정확도는 0.975
```
- 최적 학습된 모델을 반환받아 테스트 셋으로 예측 및 성능 평가해보기
```python

#GridSearchCV의 reft으로 이미 학습된 estimator 반환시키기  
estimator = grid_dtree.best_estimator_  
  
#GridSearchCV의 best_estimator_는 이미 최적 학습이 되었으므로 별도의 학습이 필요없다  
pred= estimator.predict(X_test)  
print(f'테스트 데이터 셋의 정확도는 {accuracy_score(y_test,pred):.4f} 입니다')

________________________________________________________________________
result)
테스트 데이터 셋의 정확도는 0.9667 입니다
```

## 6.  데이터 전처리

### 결손값 처리
- 결손값 NaN의 존재는 허용되지 않는다
- 결손값의 존재가 얼마 되지 않는다면 해당 열의 평균 값으로 대체하는 등의 방식이 존재한다
- 그러나 결손값이 대부분이라면 차라리 해당 열을 드롭하는 것이 나을 수도 있다

### 범주형 독립변수
- 사이킷런 머신러닝 알고리즘은 문자열 값을 입력값으로 허용하지 않는다
- 그러므로 모든 문자열은 인코딩 되어 숫자형으로 변환되어야 한다
- 레이블 인코딩
	- 각각의 문자열 카테고리를 정수 카테고리 에 대응시켜 변환시킨다.
	- `preprocessing.LabelEncoder`를 사용한다
		- `encoder=LabelEncoder()` 선언
		- `encoder.fit(encoding_lst)`  대입
		- `labels=encoder.transform(encoding_lst)` 을 통해 변환된 정수 카테고리값을 `label`에 저장한다
		- `encoded_class=encoder.classes_` 를 통해 인코딩 되기전 문자열 카테고리명을 정수 카테고리 순서대로로 불러온다. 
		- `encoder.inverse_transform`을 이용하면 정수값에 대응되는 문자열 카테고리를 불러온다
	-  **명심할 사항: 정수형 카테고리의 정숫값의 크기가 알고리즘의 영향을 주는 일이 발생해선 안된다. 이러한 특성으로 레이블 인코딩은 회귀 알고리즘(로지스틱회귀,서포트벡터머신,신경망)에 사용되지 않는다. 그러나 트리 계열 알고리즘은 이런 문제가 발생하지 않으므로 사용할 수 있다 
	- 원 핫 인코딩 One-Hot Encoding
		- 피처의 갯수만큼의 길이를 가진 튜플을 만들고, 각 카테고리를 원소 위치와 대응시킨다. 해당 카테고리의 특성을 만족시키면 1, 아니면 0으로 표기한다
		- `preprocessing.OneHotEncoder`를 사용한다
			- `encoding_lst=np.array(encoding_lst).reshape(-1,1)` 를 통해 2차원 배열을 만들어 줘야 한다
			- `oh_encoder=OneHotEncoder()` 선언
			- `oh_encdoer.fit(encoding_lst)` 대입
			- `labels=oh_encoder,transform(encoding_lst)` 를 통해 변환된 희소행렬 카테고리 값을 `label`에 저장한다 이후 `labels.toarray()`를 통해 밀집행렬로 출력한다

### 연속형 변수 
- 피쳐 스케일링 feature scaling
	- 서로 다른 변수 값 범위를 일정한 수준으로 맞춰주는 작업
	- 대표적으로 표준화 standardzation과 정규화 normalization가  존재한다
	- 표준화 standardzation
		- 각 열의 데이터값을 평균이 0, 분산이 1인 가우시안 정규 분포를 가지는 값으로 변환시키는 것
		- $z_{i}=\cfrac{x_i-\bar{x}}{\sigma_x}$
	- 정규화 Normalization
		- 각 열의 데이터값을 최소 0~ 최대 1의 값으로 변환하는 것
			- $z_i=\cfrac{x_i}{max(x)-min(x)}$
	- 벡터 정규화 vector nomalization
		- 사이킷런에서 제공하는 Normalizaer모듈은 일반적 정의의 정규화와는 정의가 다르다. 이는 선형대수의 정규화와 대응되는 것으로 개별 벡터를 모든 피쳐 벡터의 크기로 나누어준다. *이를 벡터 정규화*로 따로 구분하여 부르기로 약속한다
		- 원소가 서로 다른 단위를 가지고 있으면 사용하지 않는다. 일반적으론 딥러닝의 학습벡터에 사용된다
		-  $\tilde{x_i}=\cfrac{x_i}{\sqrt{x_i^2+y_i^2+z_i^2}}$
- 표준화 예시)
```python
#StandardScaler  
from sklearn.datasets import load_iris  
import pandas as pd  
#붓꽃 데이터를 로딩하고 Dataframe으로 변환한다  
iris= load_iris()  
iris_data= iris.data  
iris_df = pd.DataFrame(data=iris_data,columns=iris.feature_names)  
  
print(f'feature들의 평균값은 \n{iris_df.mean()}')  
print(f'\nfeature들의 분산값은 \n{iris_df.var()}')

#이제 StandardScaler를 활용하여 정규화하기  
from sklearn.preprocessing import StandardScaler  
  
#StandardScaler 객체 생성하기  
scaler= StandardScaler()  
#StandardScaler로 데이터 세트 변환. fit() 과 trnasformation() 호출  
scaler.fit(iris_df)  
iris_scaled=scaler.transform(iris_df)  
  
#transform()시 스케일 변환된 데이터 셋이 넘파이 배열로 변환되어 이를 데이터프레임으로 변환한다  
iris_df_scaled= pd.DataFrame(data=iris_scaled,columns=iris.feature_names)  
print(f'features의 평군값은 \n{iris_df_scaled.mean()}\n이다')  
print(f'feature들의 분산 값은 \n{iris_df_scaled.var()}\n이다')
____________________________________________________
result)
features의 평군값은 
sepal length (cm)   -1.690315e-15
sepal width (cm)    -1.842970e-15
petal length (cm)   -1.698641e-15
petal width (cm)    -1.409243e-15
dtype: float64
이다
feature들의 분산 값은 
sepal length (cm)    1.006711
sepal width (cm)     1.006711
petal length (cm)    1.006711
petal width (cm)     1.006711
dtype: float64
이다

```
- 정규화 예시
``` python
from sklearn.preprocessing import MinMaxScaler  
  
#MinMaxScaler객체 생성하기  
scaler= MinMaxScaler()  
#MinMaxSclaer로 데이터 세트 변환. fit()과 transformation() 호출하기  
scaler.fit(iris_df)  
iris_scaled= scaler.transform(iris_df)  
  
#transform()시 스케일 변환된 데이터 셋이 넘파이 배열로 반환되어 이를 데이터프레임으로 변환시킨다  
iris_df_scaled= pd.DataFrame(data=iris_scaled,columns=iris.feature_names)  
print(f'feature들의 최솟값은 \n{iris_df_scaled.min()}\n입니다')  
print(f'feature들의 최댓값은 \n{iris_df_scaled.max()}\n입니다')

result)
_______________________________________________
feature들의 최솟값은 
sepal length (cm)    0.0
sepal width (cm)     0.0
petal length (cm)    0.0
petal width (cm)     0.0
dtype: float64
입니다
feature들의 최댓값은 
sepal length (cm)    1.0
sepal width (cm)     1.0
petal length (cm)    1.0
petal width (cm)     1.0
dtype: float64
입니다
```
- *피쳐 스케일링 시 유의점*
	- fit()은 데이터 변환을 위한 기준정보를 설정하고, transformation()은 이렇게 설정된 정보를 이용해 데이터를 변환한다.
	- 만약 학습 데이터셋과 테스트 데이터셋에 피쳐 스케일링을 적용시킨다면, 테스트 데이터셋에 transform()을 시행시킬 때 학습 데이터에 적용시킨 fit() 을 사용해야 한다.
	- 만약 그렇게 안한다면 같은 데이터값을 가지고 있는데도, 다른 fit()함수를 가지고 있어 변환값이 다르고, 그 결과 예측값이 다르게 생기는 문제가 발생할 것이다


## 7. 타이타닉 예제- 데이터 일괄 전처리 모듈 만들기
```python
#데이터 전처리: Null값 처리/ 불필요 피처 제거/ 문자열-카테고리 인코딩을 일괄적으로 수행할 수 있겠끔 함수로 만들기  
  
# 결측치 처리하기  
def fillna(df):  
    # 이 책에선 age는 열의 평균으로, 나머지 변수들은 'N'값으로 체우기로 결정한다  
    df['Age'].fillna(df['Age'].mean(),inplace=True)  
    df['Cabin'].fillna('N',inplace=True)  
    df['Embarked'].fillna('N',inplace=True)  
    return df  
  
#불필요해보이는 피처 제거하기  
def drop_features(df):  
    df.drop(['PassengerId','Name','Ticket'],axis=1,inplace=True)  
    return df  
  
#문자열- 카테고리 피처 Sex,Cabin,Embarked를 인코딩하기  
def encoding_features(df):  
    #한번만 인코딩해야됨. 두번하면 안됨  
    categories_backup=[]  
    features=['Cabin','Sex','Embarked']  
    for feature in features:  
        LE= LabelEncoder()  
        LE= LE.fit(df[feature])  
        #카테고리 백업해두기  
        categories_backup.append(list(LE.classes_))  
        #인코딩하기  
        df[feature]= LE.transform(df[feature])  
    return df  
  
#앞에서 정의한 전처리 함수들을 호출하여 한번에 전처리를 완료하기  
def preprocessing_feature(df):  
    df= fillna(df)  
    df= drop_features(df)  
    df= encoding_features(df)  
    return df  
  
  
df_titanic= pd.read_csv('./train.csv')  
y_titanic_df= df_titanic['Survived']  
X_titanic_df= df_titanic.drop('Survived',axis=1)  
X_titanic_df= preprocessing_feature(X_titanic_df)
```
