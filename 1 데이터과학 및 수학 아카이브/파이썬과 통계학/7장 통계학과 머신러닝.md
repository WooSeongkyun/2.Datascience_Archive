# 7장 통계학과 머신러닝

## 1.머신러닝이란?

- 데이터를 학습하여 자동으로 성능을 개선하는 컴퓨터 알고리즘과 그것을 연구하는 분야
    - 지도학습: 정답을 포함한 데이터를 활용하는 학습방법
    - 비지도학습:정답을 포함하지 않는 데이터를 활용하는 학습방법
    - 강화학습: 주어진 상황내에 최대의 이익이 되는 행동을 찾아내는 알고리즘을 활용하는 학습방법

### 2.용어들

- 정규화: 파라미터를 추정할 때 손실함수에 벌칙항을 도입함으로써 계수가 큰 값이 되는 것을 막는 기법

- 리지 회귀($L_2$ 정규화):정규화항으로 계수의 제곱합을 이용한 회귀모델
    - $S(\beta)=\sum_{i=1}^{I}(y_i-\sum_{j=1}^{J}x_{ij}\beta_{j})^2$ : 전차제곱합
    - 기존의 최소제곱법이 $S(\beta)$를 최소화하는 계수 $\beta$를 추정하는 것이였다면
    - $L_2(\beta)=\sum_{i=1}^{I}[(y_i-\sum_{j=1}^{J}x_{ij}\beta_j)^2+\alpha\sum_{j=1}^{J}\beta_j^2]$
    - 리지 회귀는 $L(\beta)$를 최소화하는 $\beta$를 추정한다.
    
- 라소 회귀($L_1$ 정규화):정규화 항으로 계수의 절대값을 이용한 회귀모델
    - $L_1(\beta)=[(y_i-\sum_{j=1}^{J}x_{ij}\beta_j)^2 +\alpha\sum_{j=1}^{J}|\beta_j| ]$

### 독립변수의 표준화

- 리지 회귀나 라소회귀를 실행하기 전 미리 독립변수의 평균을 0, 표준편차 1로 표준화 해야 한다
- 독립변수의 단위가 파라미터 추정에 영향에 과다하게 영향을 주는 것을 막기 위해서이다

### 리지 회귀와 라소 회귀 추정의 차이

- 리지 회귀는 전체적으로 절대치가 작은 회귀 계수를 얻을 수 있다
- 라소 회귀는 대부분 파라미터가 0이 되고 일부 파라미터만 $non-zero$일 가능성이 높다
- 이유
    - 독립변수가 2개 있다고 하자. 각각에 대응되는 계수를 $\beta_1,\beta_2$라 하자
    - $\alpha=1$이라 가정할 때
        - $case\,\,1)\,\,\beta_1=1,\beta_2=0:$ 리지회귀값 1, 라소회귀값 1
        - $case\,\,2)\,\,\beta_1=0.5,\beta_2=0.5:$ 리지회귀값 0.5, 라소회귀값 1

## 3.파이썬을 이용한 리지 회귀/ 라소 회귀

```python
#정규분포를 따르는 노이즈
np.random.seed(1)
noise= sp.stats.norm.rvs(loc=0, scale=1, size=X.shape[0])
#올바른 계수를 5라고 설정하여 종속변수 작성
y= X.X_1 *5 +noise
#종속변수와 독립변수 합치기
large_data= pd.concat([pd.DataFrame({'y':y}),X], axis=1)
#그래프 그리기
sns.jointplot(y='y', x='X_1', data=large_data, color='black')
```

![Untitled](1%20데이터과학%20및%20수학%20아카이브/파이썬과%20통계학/레퍼런스_7장%20통계학과%20머신러닝/Untitled.png)

### 일반적인 최소제곱법 적용하기

```python
lm_statsmodels= sm.OLS(endog=y, exog=X_norm).fit()
print(lm_statsmodels.params)
# parameter가 5로 추정되어야 하는데 제법 이상한 값으로 추정되고 있는 상황이다

result)
X_1       14.755018
X_2      -87.462851
X_3      211.743136
X_4      -94.153420
X_5      -68.174976
            ...    
X_96      -2.478914
X_97     -13.059330
X_98      -5.544929
X_99      -2.923188
X_100      2.945163
Length: 100, dtype: float64
```

### sklearn을 이용한 선형회귀

```python
#어떤 모델을 만들지 지정
lm_sklearn = linear_model.LinearRegression()
#데이터를 지정하여 모델을 추정
lm_sklearn.fit(X_norm,y)
#추정된 파라미터
lm_sklearn.coef_

result)
#이것도 잘 안들어맞음 => 리지 회귀나 라쏘 회귀법을 사용
array([ 1.476e+01, -8.746e+01,  2.117e+02, -9.415e+01, -6.817e+01,
       -9.284e+01,  1.761e+00,  8.170e+01,  6.680e+01,  2.788e+01,
       -3.288e+01,  6.818e+01, -1.699e+01,  2.208e+01, -4.855e+01,
       -3.390e+01, -4.536e+01,  9.728e+00,  2.481e+01,  1.858e+00,
        1.599e+00, -2.838e+01,  6.477e+00, -5.980e+01,  3.532e+01,
       -1.302e+01,  4.108e+01, -4.303e+01,  5.247e+01,  1.920e+00,
        3.342e+01,  8.490e+00,  8.329e+00, -2.293e+01, -3.577e+01,
       -1.337e+01,  3.178e+01, -2.018e+01, -2.654e+01, -3.523e+01,
        1.927e+01,  4.823e+01, -1.655e+00,  5.749e-01, -1.893e+01,
        7.370e-01,  1.047e+01, -7.732e+00, -1.854e+00, -2.559e-01,
       -5.045e+00, -4.227e+00,  2.523e+01,  1.616e+01, -1.310e+01,
        6.455e+00, -1.332e+01, -2.291e+01, -1.202e+01,  7.745e+00,
        1.755e+01,  5.997e+00,  4.648e+00,  4.120e+00, -1.825e+00,
        5.366e+00,  4.546e+00, -2.598e+00, -4.091e+00,  7.496e+00,
        3.100e+01,  1.530e+01, -5.323e+00, -1.509e+01, -3.801e+00,
       -6.584e+00, -7.203e-02, -4.237e+00,  3.355e+00,  3.176e+00,
        6.025e+00, -2.422e-01,  2.628e+00, -2.131e-02, -8.349e+00,
       -5.091e+00, -5.060e+00, -4.779e+00,  7.645e+00,  1.797e+00,
        4.276e+00,  1.418e-01,  2.800e+00,  3.146e+00, -5.188e+00,
       -2.479e+00, -1.306e+01, -5.545e+00, -2.923e+00,  2.945e+00])
```

### 리지 회귀법 사용

```python
#그래프 그리기
#alpha를 변환
log_alphas = -np.log10(ridge_alphas)
#가로축을 -log10(alpha), 세로축을 계수로 하는 그래프
plt.plot(log_alphas, ridge_coefs, color='black')
#독립변수 X_1의 계수를 알기 쉽게 표시
plt.text(max(log_alphas)+ 0.1, np.array(ridge_coefs)[0,0],'X_1')
#X축의 범위
plt.xlim([min(log_alphas)-0.1, max(log_alphas)+0.3])
#축 테이블
plt.title('Ridge')
plt.xlabel('-log10(alpha)')
plt.ylabel('coefficients')
plt.savefig('Ridge')

#오른쪽으로 갈수록 페널티가 완화되므로 계수의 절대치가 크게 추정되기 쉬워진다
```

![Untitled](1%20데이터과학%20및%20수학%20아카이브/파이썬과%20통계학/레퍼런스_7장%20통계학과%20머신러닝/Untitled%201.png)

### 리지 회귀: 최적의 $\alpha$값 찾기

```python
#CV에서 최적의 alpha 구함
ridge_best = linear_model.RidgeCV(cv=10, alphas=ridge_alphas, fit_intercept=False)
ridge_best.fit(X_norm,y)
#최적의 -log10(alpha)
print(-np.log10(ridge_best.alpha_))

result)
0.23673469387755094

print(ridge_best.coef_)

result)
[ 4.463  1.288  0.293 -0.091 -0.201 -0.233 -0.215 -0.206 -0.145 -0.135
 -0.155 -0.046 -0.097 -0.017 -0.11  -0.012 -0.094  0.013 -0.018 -0.031
  0.025 -0.029  0.043 -0.087  0.127  0.021  0.055 -0.077  0.141 -0.007
  0.099  0.116 -0.044  0.037 -0.034  0.015  0.123 -0.171 -0.007 -0.182
  0.09   0.222  0.035 -0.032 -0.008  0.025  0.338 -0.193 -0.108  0.212
 -0.128 -0.246  0.249  0.128 -0.155  0.27   0.03  -0.165 -0.178  0.158
 -0.011  0.013  0.194  0.134 -0.156 -0.018  0.256  0.223 -0.185  0.006
  0.535  0.181 -0.348 -0.123  0.226 -0.043 -0.115 -0.053  0.209  0.189
 -0.042 -0.205 -0.099  0.059 -0.224  0.15  -0.039 -0.113  0.21   0.012
  0.129 -0.032 -0.022 -0.229 -0.198  0.245 -0.305 -0.396 -0.163  0.164]
X_1의 계수가 4.463으로 정답과 가깝다

```

### 라소 그래프

```python
lasso_alphas, lasso_coefs,_ = linear_model.lasso_path( X_norm,y,fit_intercept= False)
#alpha를 변환
log_alphas= -np.log10(lasso_alphas)
#가로축을 -log10(alpha), 세로축을 계수로 하는 그래프
plt.plot(log_alphas, lasso_coefs.T, color='black')
#독립변수 X_1의 계수를 알기 쉽게 표시
plt.text(max(log_alphas)+0.1,lasso_coefs[0,-1],'x_1')
#축의 범위
plt.xlim([min(log_alphas)-0.1,max(log_alphas)+0.3])
#축 테이블
plt.title('Lasso')
plt.xlabel('-log10(alpha)')
plt.ylabel('coefficient')
plt.savefig('Lasso그래프')
```

![Untitled](1%20데이터과학%20및%20수학%20아카이브/파이썬과%20통계학/레퍼런스_7장%20통계학과%20머신러닝/Untitled%202.png)

### 라소 회귀- 최적의 정규화 강도 결정

```python
#CV로 최적의 alpha 구함
lasso_best = linear_model.LassoCV(cv=10, alphas=lasso_alphas, fit_intercept= False)
lasso_best.fit(X_norm,y)
#최적의 -log(alpha)
print(-np.log10(lasso_best.alpha_))

result)
2.301043177767326
```

### 추정된 계수

```python
lasso_best.coef_

result)
array([ 5.336e+00, -0.000e+00, -0.000e+00, -3.043e-01, -4.121e-02,
       -0.000e+00, -0.000e+00, -0.000e+00, -0.000e+00, -0.000e+00,
       -0.000e+00, -0.000e+00, -0.000e+00, -0.000e+00, -0.000e+00,
       -0.000e+00, -0.000e+00, -0.000e+00, -0.000e+00, -0.000e+00,
       -0.000e+00, -0.000e+00, -0.000e+00, -0.000e+00, -0.000e+00,
       -0.000e+00, -0.000e+00, -0.000e+00,  0.000e+00, -0.000e+00,
        0.000e+00,  0.000e+00, -0.000e+00,  0.000e+00,  0.000e+00,
        0.000e+00,  0.000e+00, -0.000e+00,  0.000e+00,  0.000e+00,
        0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,
        0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,
        0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,
        0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,
        0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,
        0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,
        8.425e-03,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,
        0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,
        0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,
        0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,
        0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,
        7.192e-04,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00])
```

## 4. 선형모델과 신경망

- 입력 벡터: 머신 러닝에서 독립변수를 지칭하는 말
- 목표 벡터: 머신러닝에서 종속변수를 지칭하는 말
- 가중치: 머신러닝에서 계수를 지칭하는 말
- 편향: 머신러닝에서 절편을 지칭하는 말, 값이 1인 독립변수로 바라볼 수 있다

### 단순 퍼셉트론

![Untitled](1%20데이터과학%20및%20수학%20아카이브/파이썬과%20통계학/레퍼런스_7장%20통계학과%20머신러닝/Untitled%203.png)

- 입력 벡터에 가중치가 반영치가 반영된 값을 합하여 하나의 출력으로 나오게 하는 것

### 활성화함수

- 입력벡터에 가중치를 적용한 값을 출력으로 변환하는 함수
- 예: $ReLU(x) \equiv max(0,x)$

### 신경망의 $L_2$ 정규화

- 딥러닝의 경우 가중치의 결정방법, 중간층의 개수, 중간층의 종류, 손실함수의 종류, 활성화함수 선택, $L_2$ 정규화 강도등 많은 요소에 의해 최적화 정도가 다를 수 있다. 이러한 것들은 모델을 추정하기 전 정해야 하기 때문에 ***하이퍼파라미터 라고*** 부른다

### 예시: 붓꽃 줄기 폭/길이로 종류 맞추기

```python
#데이터의 feature: 꽃받침 조각 길이/폭, 꽃잎 길이/폭
iris = load_iris()
print(iris.feature_names)
#데이터의 종속변수(꽃의 종류): setosa/ versicolor/ virginica
print(iris.target_names)

#이번 예제에선 독립변수를 2개만 사용하고, 붗꽃도 2종류만 사용한다. 데이터 50번째부터 붓꽃의 종류가 바뀌므로
#row는 50:150, column은 0:2
X= iris.data[50:150, 0:2]
y=iris.target[50:150]

print('독립변수의 행 수, 열 수:',X.shape)
print('종속변수의 행 수, 열 수:',y.shape)

#데이터 셋을 훈련 데이터셋과 테스트 데이터셋으로 분리한다
#이떄 훈련 데이터는 전체 데이터중 75% 가 되게 한다
X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=2)
print('독립변수 행 수, 열 수', X_train.shape)
print('종속변수 행 수, 열 수', y_train.shape)

#데이터 정리
#독립변수 데이터프레임
X_train_df= pd.DataFrame(X_train, columns=['sepal_len','sepal_wid'])
#종속변수 데이터프레임
y_train_df=pd.DataFrame({'species': y_train-1})
#데이터 프레임 결합
iris_train_df= pd.concat([y_train_df, X_train_df],axis=1)
#결과 출력
print(iris_train_df)

result)
species  sepal_len  sepal_wid
0         0        5.7        2.8
1         0        6.6        3.0
2         1        6.1        3.0
3         1        6.7        3.3
4         1        6.8        3.0
..      ...        ...        ...
70        0        5.0        2.3
71        0        6.3        2.5
72        1        7.7        2.8
73        0        6.7        3.1
74        0        5.5        2.6
```

### 로지스틱 회귀

```python
#모델로 만들기
#모든 변수를 넣은 모델
logi_mod_full = smf.glm('species~ sepal_len + sepal_wid',data = iris_train_df, family=sm.families.Binomial()).fit()
#길이만
logi_mod_len = smf.glm('species~ sepal_len',data=iris_train_df, family= sm.families.Binomial()).fit()
#폭만
logi_mod_wid= smf.glm('species~ sepal_wid', data=iris_train_df, family= sm.families.Binomial()).fit()
#Null 모델
logi_mod_null= smf.glm('species~1', data=iris_train_df, family=sm.families.Binomial()).fit()
#AIC비교

print('full',logi_mod_full.aic.round(3))
print('len',logi_mod_len.aic.round(3))
print('wid',logi_mod_wid.aic.round(3))
print('null',logi_mod_null.aic.round(3))
# 길이를 이용한 1변수가 가장 예측을 잘한다고 나왔다
result)
full 76.813
len 76.234
wid 92.768
null 105.318

logi_mod_len.summary().tables[1]
result)
```

|  | coef | std err | z | p>|z| | [0.025 | 0.975] |
| --- | --- | --- | --- | --- | --- | --- |
| Intercept | -16.4152 | 4.000 | -4.104 | 0.000 | -24.256 | -8.575 |
| sepal_len | 2.6478 | 0.639 | 4.142 | 0.000 | 1.395 | .3901 |

### 적중률 확인

```python
#데이터 정리
X_test_df= pd.DataFrame(X_test,columns=['sepal_len', 'sepal_wid'])
#피팅 예측
logi_fit = logi_mod_len.fittedvalues.round(0)
logi_pred= logi_mod_len.predict(X_test_df).round(0)
#정답 수
true_train =np. sum(logi_fit == (y_train-1))
true_test= np.sum(logi_pred==(y_test-1))
#적중률
result_train = true_train/len(y_train)
result_test= true_test/len(y_test)
#결과 출력
print('훈련 데이터 적중률', result_train)
print('테스트 데이터 적중률',result_test)

result)
훈련 데이터 적중률 0.7466666666666667
테스트 데이터 적중률 0.68
```

### 신경망 전 데이터 전처리: 표준화

```python
#표준화를 위한 준비
scaler= StandardScaler()
scaler.fit(X_train)

#표준화
X_train_scaled= scaler.transform(X_train)
X_test_scaled = scaler.transform(X_test)

#여기서 표준화란 훈련 데이터셋의 표준편차는 1, 평균을 0으로 만드는 것이지 
#테스트셋을 그렇게 만드는게 아님
print(np.std(X_train_scaled,axis=0))
print(np.std(X_test_scaled,axis=0))

result)
[1. 1.]
[0.74  0.679]

```

### 신경망

```python
nnet=MLPClassifier(hidden_layer_sizes=(100,100), alpha=0.07, 
max_iter=10000, random_state=0)
nnet.fit(X_train_scaled, y_train)
#정답률
print('훈련 데이터 적중률:', nnet.score(X_train_scaled,y_train))
print('테스트 데이터 적중률:',nnet.score(X_test_scaled,y_test))

result)
훈련 데이터 적중률: 0.8933333333333333
테스트 데이터 적중률: 0.68

```