# 6장 일반선형모델

## 1.여러가지 확률 분포

### 1.이항확률분포

- 2개의 값만을 갖는 확률변수
    - 예: 있다와 없다/ 앞 뒤/ 0과 1 등
- 베르누이 시행: 2종류의 결과 중 하나만 발생시키는 시행
    - 예: 동전 던졌을 때 앞면혹은 뒷면 나오는 사건
- 성공확률: 2 종류의 결과중 어느 한 쪽의 결과를 얻을 확률
- 베르누이 분포: 베르누이 시행을 1번 했을 때의 확률분포
    - $P(X=1)=p ,\,\,P(X=0)=1-p,p$는 성공확률
- **이항분포**
    - 성공확률 $p$ 인 시행을 $N$회 독립 시행했을 때 성공한 횟수 $k$가 따르는 확률분포
    - 기댓값은 $Np$, 분산은 $Np(1-p)$가 유도된다
    - $Notation:\begin{pmatrix} N \\k   \end{pmatrix}={}_NC_kp^k(1-p)^{N-k}$
        - $Interpretation$
            - 이항분포는 정의상 $x=1,x=2,...,x=N$일 때의 확률을 다 더하면 1이 되어야 한다
            - $(p+(1-p))^N=1$과 비교하자.
            - ${}_NC_k$는 순서를 고려하지 않음으로서 곱해지는 조합의 수다
            - 이항분포 히스토그램

```python
#n=10, p=0.2의 이항분포
binomial= sp.stats.binom(n=10,p=0.2)
#난수
np.random.seed(1)
rvs_binomial = binomial.rvs(size=10000)
#확률밀도함수: 이론적으로 예측된 값
m= np.arange(0,10,1)
pmf_binomial = binomial.pmf(k=m)
#난수의 히스토그램과 확률질량함수를 겹쳐서 그리기
sns.distplot(rvs_binomial, bins=m, kde=False, norm_hist=True, color='gray')
plt.plot(m,pmf_binomial,color='black')
plt.xlim(0,10)
plt.show()
```

![Untitled](1.%20수학%20아카이브/0.미완성_수학/파이썬과%20통계학/레퍼런스_6장%20일반선형모델/Untitled.png)

### 2. 푸아송 분포

- 단위시간안에 몇번의 사건이 일어날지를 나타내는 확률분포
- 이항분포 모델에서
    - 시행횟수 $N$과
    - 정해진 구간내 사건이 일어나는 횟수의 기댓값 $\lambda$이 있고
    - $N\gg\lambda$ 이면 푸아송 분포로 근사된다
    - 증명:
    
    [이항 분포의 극한→ 푸아송 분포 증명](이항%20분포의%20극한→%20푸아송%20분포%20증명%20ab21f58f40ab4949a6d6344c917531a5.md)
    
- $Pois(x| \lambda)=\cfrac{e^{-\lambda }\cdot \lambda^x}{x!}$

```python
# 람다가 2인 푸아송 분포
poisson= sp.stats.poisson(mu=2)
#시행횟수가 10**8이고 성공확률이 2* 10**(-8)인 경우의 이항분포와 비교하기
N= 10 **8
p= 2* 10**(-8)
binomial_2= sp.stats.binom(n=N,p=p)
#확률질량함수
pmf_binomial_2= binomial_2.pmf(k=m)
#확률질량의 그래프
plt.plot(m,pmf_poisson,color='gray')
plt.plot(m,pmf_binomial_2,color='black',linestyle='dotted')
```

![Untitled](1.%20수학%20아카이브/0.미완성_수학/파이썬과%20통계학/레퍼런스_6장%20일반선형모델/Untitled%201.png)

### 3.지수형 분포

- $f(x|\theta)=exp[a(x)b(\theta)+c(\theta)+d(x)]$ 꼴로 정의되는 함수
- $a(x)=x$ 의 꼴은 정준형$canonical$이라 부른다
- $b(\theta)$는 자연 파라미터라 부른다

### 4.그외의 확률 분포

- 음이항분포
- 감마분포

## 2. 일반선형모델의 기본

- 모집단이 있다/없다등의 두개의 값을 취하거나 1,2,3등의 정수의 값을 취하는 등의 데이터라면 정규분포로 가정하는 것은 무리가 있다
- 일반선형모델은 그렇기에 도입되었다

### 선형예측자 $linear \,\,Predictor$

- 독립변수를 선형관계식으로 표현한 것
    - 회귀계수$regression\,\,coefficient$ $\beta\equiv[\beta_0,\beta_1,\beta_2,....]$
    - 설계행렬$design\,\,matrix\,\, \bold{X}$$:\,\,\,\,x_{ij}$는 $i$번째 데이터의 $j$번째 속성 값을 담은 행렬
    - 의 행렬곱으로 정의된다
    

### 링크함수 $link\,\,function$

- 선형예측자와 기댓값 사이를 잇는 함수
- $g(E(y))=g(\mu)=\bold{X}\beta$ 의 관계를 갖는다
    - 단조성$monotonicity$
    - 미분가능성$differentiability$ 를 요구한다

### 로짓 함수$logit\,\,function$

- $f(x)= log(\cfrac{x}{1-x})$

### 로지스틱함수 $logistic\,\,function$

- $g(y)= \cfrac{1}{1+exp(-y)}$
- 로짓 함수의 역함수이다 ($g(f(x))=x)$
- 특징
    - $y \rightarrow \infin \,\,g(y)\rightarrow1$
    - $y \rightarrow -\infin \,\,g(y)\rightarrow0$
    - $g(y=0)=0.5$

## 3.로지스틱 회귀 $logistic \,\,regression$

- 확률분포에 이항분포를 사용하고, 링크함수에 로짓함수를 사용한 일반선형모델
- 예시: 시험의 합격
    - 시험의 합격을 1, 실패를 0으로 표현한다고 하자. 시험시간을 독립변수로 하였을 때 가장 단순한 형태의 식은 아래와 같다
    - $f(t)=\beta_0+\beta_1\cdot t$, $t$는 시험시간은 실수이므로 $f(t)$ 또한 실수값을 가질 수 있다. 그러므로 합격1, 실패0으로 표현하는 것은 불가능하다.
    - 대신 $f(t)$를 시험의 합격률로 해석한다고 하자. 그렇다면 $0\le f(t)\le1$의 값을 가져야 한다
    - 따라서 이러한 이유로 $link \,\,function$을 사용한다
    - 모델링
        - $let\,\,p\,\,be \,\,probability \,\,of\,\,success\,\,\,$
        - $let \,\, log(\cfrac{p}{1-p})=\beta_0+\beta_1\cdot t$
        - $then\,\,p=\cfrac{1}{1+exp[-(\beta_0+\beta_1\cdot t)]}$

### 로지스틱 회귀의 우도함수

- 우도함수의 정의
    - $L(\theta|x)=P(x|\theta)$
- 예시 :공부시간-합격률 모델의 $\beta_0,\beta_1$의 추정
    - $given\,\,model: p(t)=\cfrac{1}{1+exp[-(\beta_0+\beta_1\cdot t)]}$
    - $data$
        - 공부시간이 3시간인 학생 9명중 4명이 합격
        - 공부시간이 5명인 8명 중 6명이 합격
        - 공부시간 8시간인 학생 1명중 1명이 합격
    - $Inference$
        - 시행횟수를 $N$, 합격자수를 $m$으로 표기하자
        - $L(\beta_0,\beta_1|N,m)$
        - $=Bin(4|9,\cfrac{1}{1+exp[-(\beta_0+\beta_1\cdot3)]}\cdot\,Bin(6|8,\cfrac{1}{1+exp[-(\beta_0+\beta_1\cdot5)]}\cdot\,Bin(1|1,\cfrac{1}{1+exp[-(\beta_0+\beta_1\cdot8)]}$
        
        ```python
        #독립변수 hours, 종속변수 result
        #확률분포를 Binomial로 정함. 만약 푸아송으로 지정하고 싶은 경우 sm.families.Poisson()을 입력하면 된다
        #link function는 명시하지 않을 시 자동으로 logit으로 지정된다
        mod_glm= smf.glm(formula= 'result~hours', data=test_result, family=sm.families.Binomial()).fit()
        mod_glm.summary()
        ```
        

### Null 모델과 1변수 모델의 AIC 비교

```python
mod_glm_null = smf.glm('result ~1', data=test_result, family=sm.families.Binomial()).fit()
print('Null 모델:', mod_glm_null.aic.round(3))
print('1변수 모델',mod_glm.aic.round(3))
```

### 로지스틱 회귀곡선 그래프

```python
sns.lmplot(x='hours',y='result',data=test_result, logistic=True, scatter_kws={'color':'black'},
          line_kws={'color':'black'}, x_jitter=0.1, y_jitter=0.02)
plt.savefig('로지스틱 회귀 그래프')
```

![Untitled](1.%20수학%20아카이브/0.미완성_수학/파이썬과%20통계학/레퍼런스_6장%20일반선형모델/Untitled%202.png)

```python
#0~9까지 1씩 증가시키는 배열
exp_val= pd.DataFrame({
    'hours':np.arange(0,10,1)
})
#하루 평 균공부시간에 따른 합격확률
pred= mod_glm.predict(exp_val)
print(pred)

result)
0    0.010367
1    0.025836
2    0.062920
3    0.145291
4    0.300876
5    0.521427
6    0.733929
7    0.874741
8    0.946467
9    0.978147
```

### 로지스틱 회귀 해석에 필요한 용어들

- 오즈$odds$
    - 실패하는 것보다 성공하는 것이 몇배 더 많이 발생하는 가를 나타내는 비율
    - $odds=\cfrac{p}{1-p}$
    - 로그오즈: 오즈에 로그를 취한 것
    - 오즈비: 오즈간에 비율을 취한 것
    - 로그오즈비:오즈비에 로그를 취한 것

### 로지스틱 회귀계수와 오즈비의 관계

```python
#공부시간이 1시간일 때의 합격률
exp_val_1= pd.DataFrame({'hours':[1]})
pred_1 = mod_glm.predict(exp_val_1)
#공부시간이 2시간일 때의 합격률
exp_val_2= pd.DataFrame({'hours':[2]})
pred_2 = mod_glm.predict(exp_val_2)

#오즈
odds_1 = pred_1/(1-pred_1)
odds_2= pred_2/(1-pred_2)
print(odds_1,odds_2)
#로그오즈비
sp.log(odds_2/odds_1)

result)
array([0.929])
```

- 로그오즈비는 공부시간의 계수와 일치한다. → 계수에 $exp$를 취한 것이 오즈비이다

## 4.일반선형모델의 평가

- $Pearson\,\,Residual$: 측정된 데이터값에서 추정된 값을 빼고 그것을 표준편차로 나눈 값
    - $\cfrac{y-N\hat{p}}{\sqrt{N\hat{p}(1-\hat{p})}}$
    - 여기서 하나하나의 예측 결과의 시행횟수는 1이 되기 때문에 실질적으로 피어슨 잔차는 다음과 같이 계산된다
    - $\cfrac{y-\hat{p}}{\sqrt{\hat{p}(1-\hat{p})}}$
    - $N$을 고정하였을 때 $Np(1-p)$가 가장 클 때는 $p=0.5$일 때이다

### 피어슨 잔차

```python
#예측한 성공확률
pred= mod_glm.predict()
#종속변수(시험 합격여부)
y= test_result.result
#피어슨 잔차
pearson_resid=(y-pred)/np.sqrt(pred*(1-pred))
print(pearson_resid)

#피어슨 잔차제곱합= 피어슨 카이제곱통계랑
pearson_chi_square= np.dot(pearson_resid,pearson_resid)
print(pearson_chi_square)

result)
0    -0.102351
1    -0.102351
2    -0.102351
3    -0.102351
4    -0.102351
        ...   
95    0.149470
96    0.149470
97    0.149470
98    0.149470
99    0.149470
Name: result, Length: 100, dtype: float64
84.91138782569983
```

- $deviance$
    - 모델의 적합도를 평가하는 지표 / 클수록 모델이 적합하지 않다고 평가할 수 있다
    - $deviance= 2[logL(\beta_{max}|y)-logL(\beta_{glm}|y)]$
    - 이때 $\beta_{max}$는 종속변수를 완전히 예측할 수 있을 때의 회귀계수를 의미한다

```python
#deviance 잔차 계산하기

#예측한 성공확률
pred= mod_glm.predict()
#종속변수(테스트 합격 여부)
y= test_result.result
#합격여부를 완전히 예측할 수 있을 때의 로그우도와 잔차
resid_tmp=0-np.log(sp.stats.binom.pmf(k=y,n=1,p=pred))
#deviance 잔파
deviance_resid= np.sqrt(2*resid_tmp) *np.sign(y-pred) #np.sign 함수는 플러스마이너스 부호를 돌려주는 함수
#결과 확인
deviance_resid.head(3)
```

### 교차 엔트로피

- 두 확률분포 $p,q$가 있을 때 둘을 구분하기 위해 필요한 평균 비트 수
- 머신러닝에선 로지스틱 회귀를 교차 엔트로피 오차의 최소화란 관점으로 설명하는 경우도 있다
    - $Bin(m|N,p)={}_NC_m\cdot p^m(1-p)^{N-m}$ 이때 $N=1\,\, ,m=0\,\,or\,\,1$ : 각 하나하나의 데이터에 대한 시행
    - $Bin(m|1,p)=p^m(1-p)^{1-m}$
    - 이때 $y=0,1$을 각각 실패와 성공, $\hat{p}$을 예측한 합격률이라고 하자
    - $Bin(y|1,\hat{p})=$$\hat{p}^y(1-\hat{p})^{1-y}$
    - $likelihood=\prod_{i=1}^{T}\hat{p}_{i}^{y_i}(1-\hat{p_i})^{1-y_i}$ 이때 $T$는 샘플사이즈
    - 이 우도함수에 $log$를 취하고 $-1\cdot$을 곱하면
    - $**cross\,\,entrophy=-\sum_{i=1}^{T}[y_ilog\hat{p_i}+(1-y_i)log(1-\hat{p_i})]**$

## 6.푸아송 회귀

- 확률분포에 푸아송 분포를 사용, 링크함수에 로그함수를 사용한 일반선형모델
- 예시: 기온- 맥주판매 예시
    - $t$는 온도, $y$는 맥주 판매량,$\lambda$는 강도(특정 기간내 사건이 발생한 횟수) 으로 표기한다
    - 선형예측자: $\beta_0+\beta_1\cdot t$
    - $log(y)=\beta_0+\beta_1\cdot t$
    - $y=exp[\beta_0+\beta_1\cdot t]$
    - 이때 맥주 판매갯수 $y$는 푸아송 분포에 따른다고 가정하면
    - $y \sim Pois(y|exp[\beta_0+\beta_1\cdot t])$ $\,\,\,(ref. Pois(y|\lambda)=\cfrac{\exp^{-\lambda}\cdot\lambda^y}{y!})$
    
    ```python
    mod_pois_null = smf.glm('beer_number ~ 1', data=beer, family= sm.families.Poisson()).fit()
    print('Null 모델:',mod_pois_null.aic.round(3))
    print('1변수 모델',mod_pois.aic.round(3))
    #1변수 모델이 aic가 더 적음 > 더 합리적인 모델
    
    #회귀곡선 그래프 그리기
    #푸아송 회귀의 경우엔 seaborn 함수로 그릴 수 없다
    x_plot = np.arange(0,37)
    pred= mod_pois.predict(pd.DataFrame({'temperature':x_plot}))
    #회귀곡선을 그리지 않은 lmplot
    sns.lmplot(y='beer_number', x='temperature', data=beer, fit_reg=False, scatter_kws={'color':'black'})
    #회귀곡선을 덧그리기
    plt.plot(x_plot,pred,color='black')
    ```
    

![Untitled](1.%20수학%20아카이브/0.미완성_수학/파이썬과%20통계학/레퍼런스_6장%20일반선형모델/Untitled%203.png)

### 회귀계수 해석

```python
#기온이 1도일 때의 판매 개수의 기댓값
exp_val_1= pd.DataFrame({'temperature':[1]})
pred_1 = mod_pois.predict(exp_val_1)
#기온이 2도일 때의 판매 개수의 기댓값
exp_val_2= pd.DataFrame({'temperature':[2]})
pred_2 = mod_pois.predict(exp_val_2)

#기온이 1도에서 1도가 오르면 판매 개수는 몇배가 되는가?
#회귀계수에 exponetial 취한 값과 동일하다
print(pred_2/pred_1)
print(np.exp(mod_pois.params['temperature']))

result)
0    1.079045
1.079045054586893
```