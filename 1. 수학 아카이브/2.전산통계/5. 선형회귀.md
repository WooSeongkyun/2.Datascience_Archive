### 선형회귀
- 선형회귀 의 정의
- 회귀계수의 이론적 추정
- 선형회귀식의 변형

## 1. $Linear\,\,Regression$은 무엇인가?

- 특징
    - 정량적인 종속변수를 예측하는데 쓰이는 도구이다.
    - 현대 통계학습중 일반화된 선형회귀법이 많은 부분을 차지한다
- 정의
    - 독립변수 $X_1,X_2,...,X_n$과 종속변수 $Y$, 계수 $\beta_0,\beta_1,...,\beta_n$ 오차항 $\epsilon_1,\epsilon_2,...,\epsilon_n$이 있다하자
    - 다음과 같은 가정이 성립한다 하자
        - 비확률변수nonstochastic :독립변수 $X_1,X_2,...,X_n$는 비확률변수이다(상수취급한다)
        - 선형성linearity: 계수와 종속변수 사이 선형성을 갖는다
        - 독립성:independence 독립변수가 다른 독립변수에 영향을 주지 않는다
        - 등분산성homoskedasticity: $Var(\epsilon_i)=\sigma^2$$\,\,for\,\,i=1,2,...$
        - 정규성normality: $\epsilon\sim \mathcal{N}(0,\sigma^2)$
        - $i\ne j$일때 $Cov(\epsilon_i,\epsilon_j)=0$ 이다
- 정의
    - 모델형태가  $Y=\sum_{i=1}^{n}\beta_{i}X_i+\beta_0+\epsilon$ 이라고 가정하자. 그 계수값 $\beta_0,\beta_1,\beta_2,...,\beta_n$ 을 구한뒤, 해당 모델로 종속변수를 예측하는 일련의 방법론을 선형회귀라고 부른다


## 2. 회귀계수의 이론적 추정

### 단순회귀계수의 이론적 추정

- 조건
    - 독립변수 $X$와 종속변수 $Y$, 계수(파라미터) $\beta_0,\beta_1$이 있다하자
    - 훈련 데이터셋 $\{(x_1,y_1),(x_2,y_2),...,(x_n,y_n)\}$이 있다하자
    - 예측값 $\hat{y_i}=\hat{\beta_0}+\hat{\beta_1}x_i$라고 하자
    
- 논리
    - $y_i-\hat{y_i}$가 작을수록 $\hat{\beta_0},\hat{\beta_1}$이 잘 추정되었다고 볼 수 있다
    - 그렇기 때문에 각 $i$에 대하여 $e_i=y_i-\hat{y_i}$ 값을 측정하자. 그리고 제곱을 하면 $e_i^2=|y_i-\hat{y_i}|^2$, 종속변수의 관측치와 예측치 사이 거리의 제곱이 된다.
    - 이 값을 $i$에 대하여 모으자. 모은값이 작으면 작을수록 계수 추정이 잘 되었다고 해석할 수 있다(이를 잔차제곱합Residual Sum of Square라 부른다)
    
- 순서
    - $RSS=\sum_{i=1}^n|y_i-\hat{y_i}|^2=\sum_{i=1}^n(y_i-(\hat{\beta_0}+\hat{\beta_1}x_i))^2$
    - 미분을 이용하여 $RSS$를 최소화하는 계수값을 찾기
        - $\displaystyle\frac{\partial{(RSS)}}{\partial{\hat{\beta_0}}}=-2\sum_{i}(y_i-\hat{\beta_0}-\hat{\beta_1}x_i)=0$
        - $\displaystyle\frac{\partial{(RSS)}}{\partial{\hat{\beta_1}}}=-2\sum_{i}(y_i-\beta_0-\beta_1x_i)x_i=0$
        - $n\hat{\beta_0}+\hat{\beta_1}\sum_{i}x_i=\sum_{i}y_i$
        - $\hat{\beta_0}\sum_{i}x_i+\hat{\beta_1}\sum_{i}x_i^2=\sum_{i}x_iy_i$
        - $\begin{bmatrix} \hat{\beta_0} \\ \hat{\beta_1} \end{bmatrix}=\displaystyle\frac{1}{n\sum_{i}x_i^2-(\sum_{i}x_i)^2} \begin{bmatrix} \sum_{i}x_i^2 & -\sum_{i}x_i \\ -\sum_{i}x_i &n \end{bmatrix}\begin{bmatrix} \sum_{i}y_i \\ \sum_{i} x_iy_i\end{bmatrix}$
        - $n\bar{x}=\sum_{i}x_i, n\bar{y}=\sum_{i}y_i$로 두자
        - $\begin{bmatrix} \hat{\beta_0} \\ \hat{\beta_1} \end{bmatrix}=\displaystyle\frac{1}{n(\sum_{i}x_i^2-n\bar{x}^2)} \begin{bmatrix} \sum_{i}x_i^2 & -n\bar{x}\\ -n\bar{x} &n \end{bmatrix}\begin{bmatrix} n\bar{y}\\ \sum_{i} x_iy_i\end{bmatrix}=\displaystyle\frac{1}{n(\sum_{i}(x_i-\bar{x_i})^2)} \begin{bmatrix} n\bar{y}\sum_{i}x_i^2 -n\bar{x}\sum_{i}x_iy_i\\ -n^2\bar{x}\bar{y} + n\sum_{i}x_iy_i \end{bmatrix}$
        - $\begin{bmatrix} \hat{\beta_0} \\ \hat{\beta_1} \end{bmatrix}=\displaystyle\frac{1}{\sum_{i}(x_i-\bar{x})^2}\begin{bmatrix} \bar{y}\sum_{i}x_i^2 -\bar{x}\sum_{i}x_iy_i\\   \sum_{i}x_iy_i-n\bar{x}\bar{y} \end{bmatrix}$
        - $\begin{bmatrix} \hat{\beta_0} \\ \hat{\beta_1} \end{bmatrix}=\displaystyle\frac{1}{\sum_{i}(x_i-\bar{x})^2}\begin{bmatrix} \bar{y}\sum_{i}x_i^2 -\bar{x}\sum_{i}x_iy_i\\   \sum_{i}(x_i-\bar{x})(y_i-\bar{y}) \end{bmatrix}$
        - $\sum_{i}(x_i-\bar{x})^2\hat{\beta_1}+n\bar{x}\bar{y}=\sum_{i}x_iy_i$
        - $\bar{y}\sum_{i}x_i^2-\bar{x}(\sum_{i}(x_i-\bar{x})^2\hat{\beta_1}+n\bar{x}\bar{y})$
        - $=\bar{y}(\sum_{i}x_i^2-n\bar{x}^2)-\hat{\beta_1}\bar{x}\sum_{i}(x_i-\bar{x})^2$
        - $\therefore \hat{\beta_0}=\bar{y}-\hat{\beta_1}\bar{x}$
- 계수 계산하기(최종 요약)
    - $\begin{bmatrix} \hat{\beta_0} \\ \hat{\beta_1} \end{bmatrix}= \begin{bmatrix} \bar{y}-\hat{\beta_1}\bar{x}\\ \displaystyle\frac{\sum_{i=1}^{n}(x_i-\bar{x})(y_i-\bar{y})}{\sum_{i=1}^{n}(x_i-\bar{x})^2}:\displaystyle\frac{S(x,y)}{S(x,x)} \end{bmatrix}$

### 다중선형회귀의 이론적 추정
- 순서
	- $RSS=|\mathbf{Y}-\mathbf{X}\beta|^2=\langle \mathbf{Y}-\mathbf{X}\beta,\mathbf{Y}-\mathbf{X}\beta\rangle=$$(\mathbf{Y}-\mathbf{X}\beta)^T(\mathbf{Y}-\mathbf{X}\beta)=(\mathbf{Y}^T-\beta^T\mathbf{X}^T)(\mathbf{Y}-\mathbf{X}\beta)=\mathbf{Y}^T\mathbf{Y}-\mathbf{Y}^T\mathbf{X}\beta-\beta^T\mathbf{X}^T\mathbf{Y}+\beta^T\mathbf{X}^T\mathbf{X}\beta$
	 - $=\mathbf{Y}^T\mathbf{Y}-2\beta^T\mathbf{X}^T\mathbf{Y}+\beta^T\mathbf{X}^T\mathbf{X}\beta$
		 - ($\displaystyle\frac{\partial {\mathbf{A}\mathbf{x}}}{\partial {\mathbf{x}}}=\mathbf{A}$, $\displaystyle\frac{\partial {\mathbf{x}\mathbf{A}\mathbf{x}}}{\partial {\mathbf{x}}}=2\mathbf{A}\mathbf{x}$ 을 활용함)
	- 미분을 이용하여 $RSS$를 최소화하는 계수값을 찾자
	- $\displaystyle\frac{\partial{(RSS)}}{\partial\beta}=-2(\mathbf{X}^T\mathbf{Y})^T+2\beta^T(\mathbf{X}^T\mathbf{X})=-2\mathbf{Y}^T\mathbf{X}+2\mathbf{X}^T\mathbf{X}\beta=0$
	- $\mathbf{X}^T\mathbf{X}\beta=\mathbf{Y}^T\mathbf{X}$
	- $\beta=(\mathbf{X}^T\mathbf{X})^{-1}(\mathbf{Y}^T\mathbf{X})$ : 추정된 $\beta$값


## 3. 계수의 평균과 표준오차

- 모회귀선population regression line 
	- 모집단 데이터에 학습된 회귀선
- 최소제곱선 least square line
	- 훈련 데이터셋에 학습된 회귀선
	
- 계수의 기댓값: 편향되지 않는다
	- $E(\hat{\beta})=\beta$
	- 증명
		- $E(\hat{\beta})=E((\mathbb{X}^{T}\mathbb{X}) ^{-1}(\mathbb{X}^{T}\mathbb{Y})$
		- $=(\mathbb{X}^{T}\mathbb{X}) ^{-1}(\mathbb{X}^{T}E(\mathbb{Y}))$
		- $=(\mathbb{X}^{T}\mathbb{X}) ^{-1}\mathbb{X}^{T}\mathbb{X}{\beta}$
		- $=\mathbb{I}_n\beta=\beta$
		
- 단순선형회귀 계수의 표준오차
	- 표준오차 Standrd Error: 표본 분포의 표준편차.
	- $SE(\hat{\beta_{0}})^2=\sigma^2[\displaystyle\frac{1}{n}+\displaystyle\frac{\bar{x}^2}{\displaystyle\sum_{i=1}^{n}{(x_{i}-\bar{x})^2}}]$
	- $SE(\hat{\beta_{1}})^2=\displaystyle\frac{\sigma^2}{\displaystyle\sum_{i=1}^{n}{(x_{i}-\bar{x})^2}}$
	- 이때 $\sigma^2=Var(\epsilon)=\sqrt{\displaystyle\frac{\displaystyle\sum_{i=1}^{n}{(y_{i}-\hat{y}_{i})}}{d.o.f}}=\sqrt{\displaystyle\frac{\displaystyle\sum_{i=1}^{n}{(y_{i}-\hat{y}_{i})}}{n-2}}$, Residual Standard Error라 불린다.
		- $d.o.f=n-2$인 이유는 $\beta_0,\beta_1$ 두개의 파라미터가 사용되기 때문이다
		- 만약 $y=\beta_{0}+\beta_{1}x_{1}+\beta_{2}x_{2}+\epsilon$이라면 $d.o.f=n-3$ 이 된다
		- 종속변수와 회귀선사이의 벗어나는 정도를 측정한 것이다

### 4. 결정계수 $R^2$  와 상관계수 $r_{xy}$ 
#### Coefficient of Determination and Coefficient of Correlation

- 결정계수 $R^2=\displaystyle\frac{TSS-RSS}{TSS}=1-\displaystyle\frac{RSS}{TSS}$
	- $TSS= \displaystyle\sum_{i}^{}{(y_{i}-\bar{y})^2}$ : Total Sum of Square : 종속변수의 변동성을 측정한 것
	- $RSS=\displaystyle\sum_{i}^{}{(y_{i}-\hat{y}_{i})^2}$ : Residual Sum of Suare: 회귀를 수행한 후에도 설명되지 않는 변동성의 양을 측정한 것이다
	- $TSS-RSS$ 는 회귀를 수행하여 설명되는 종속변수의 변동성을 측정한 것이다
	- 즉 $R^2$는 회귀분석을 통하여 설명되는 종속변수의 변동성의 비율이다. 0에 가까울 경우 회귀분석이 종속변수의 변동성을 많이 설명할 수 없다는 것이고, 1의 경우는 반대로 해석할 수 있다
	- ![](https://upload.wikimedia.org/wikipedia/commons/thumb/8/86/Coefficient_of_Determination.svg/800px-Coefficient_of_Determination.svg.png)
	- 각각 $TSS$와 $RSS$ 측정하는 것을 보여주고 있다. 오른쪽 선형회귀 그래프가 왼쪽 그래프와 비교하여 데이터에 더 적합할 수록 $R^2$ 값이 1에 가까워진다
	
- 상관계수 $r_{xy}=\displaystyle\frac{\displaystyle\sum_{i=1}^{n}{(x_{i}-\bar{x})(y_{i}-\bar{y})}}{\sqrt{\displaystyle\sum_{i=1}^{n}{(x_{i}-\bar{x})^2}}\sqrt{\displaystyle\sum_{i=1}^{n}{(y_{i}-\bar{y})^2}}}$
	
	- 두 변수 $x,y$ 가 얼마나 선형적 상관관계를 갖는지를 측정한 수치다. 

### 5.가설검정: 독립변수와 종속변수 사이의 관계
- 다중선형회귀 모델에서 회귀계수 $\beta_{1},\beta_{2},...,\beta_{p}$가 있다고 하자
- 귀무가설 $H_{0}:\beta_{1},\beta_{2},...,\beta_{p}=0$
- 대립가설 $H_{a}:$ at least one $\beta_j$ is non-zero for $1\le{j}\le{p}$
- 해당 가설 검정은 $F-Statistic$을 통해 이루어진다
	- $F=\displaystyle\frac{(TSS-RSS)/p}{RSS/(n-p-1)}$

- 변수 선택: 종속변수와 관련되는 일부 독립변수집합을 찾기
	- 다중선형회귀모델을 세울 때 각 변수에 대한 $p$값을 측정한다고 하자. 이때 $p$값이 매우 작은 변수는 종속변수와 관계성이 있을 가능성이 높다. 그러나 독립변수의 수를 증가시킨다면, 그에 따라 실제 종속변수와의 관련성이 크게 없을지라도 $p$값이 작은 독립변수의 수가 늘어나게 된다
	- 만약 추정할 계수보다도 관측값이 많다면 최소 제곱을 활용한 다중선형회귀모델을 사용할 수 없고, $F$ 통계량 또한 사용할 수 없다. 이 경우 다른 방법을 사용해야한다
	- 확인법
		-  $\beta$ 의 부분 집합에 대해서 다시 귀무가설을 세울 수 있고 이는 
			- $H_{0}=\beta_{p-q+1}=\beta_{p-q+2}=...=\beta_p=0$ 으로 표현된다
			- $F=\displaystyle\frac{(RSS_{0}-RSS)/q}{RSS/(n-p-1)}$
		- AIC, 베이지안 정보기준 등의 방법들이 더 활용될 수 있다
	- 선택방법
		- Foward Selection: 처음 null 모델부터 시작한다. 맨 처음 $p$개의 독립변수에 대한 단순선형회귀식을 세우고, 그 중 RSS가 낮은 변수를 모델에 추가한다. 그 다음 규칙이 충족될 때 까지 변수를 추가한다
		- Backward Selection: 처음 모든 변수로 모델을 설정한다. 맨 처음 $p$개의 독립변수중 가장 $p$값이 큰 변수를 제거한다. 그 다음 중지 규칙이 충족될 때 까지 계속한다
		- Mixed Selection: 처음 null 모델에서 시작한다. 계속 변수를 추가하다, 변수에대한 $p$값이 임계치 이상 증가하면 해당 변수를 제거한다. 모든 변수의 모델이 충분히 낮은 $p$ 값을 갖고, 모델에 속하지 않느 변수가 높은 $p$값을 가질 때 까지 이러한 전진 및 후진을 계속 수행한다.


## 3. 선형회귀식 변형: 상호작용항 추가
- 예: $\beta_1$는 TV광고 효과, $\beta_2$는 라디오 단독 효과, $\beta_3$는 라디오와 광고의 상호작용 효과
	- $y=\beta_{0}+(\beta_{1}+\beta_{3}x_{2})x_{1}+\beta_{2}x_{2}+\epsilon$
	- $y=\beta_{0}+\tilde{\beta_{1}}x_{1}+\beta_{2}x_{2}+\epsilon$ , $\tilde{\beta_1}=\beta_{1}+\beta_{3}x_{2}$
- 이 가정이 맞는지 확인하기 위하여
	- $H_{0}:\beta_{3}=0$ 귀무가설을 세워 검증한다
	- $R^2$ 를 측정한다
	 
- 선형모델 가정시의 잠재적 문제
	1. Non-linearity of response-predictor relationships
	2. Correlation of error terms
	3. Non-constant variance of error terms
	4. Outliers
	5. High-leverage points
	6. Colinearity

### 4. 이상치 처리
- Leverage 
	- 선형회귀에서 관찰된 독립변수 값이 다른 독립변수와 얼마나 떨어져 있는지 측정하는 척도
	- 실제 종속변수 $y$가 예측치 $\hat{y}$에 미치는 영향력을 나타내는 척도로 해석할 수도 있다
	- 정의
		- 모델의 정의는  $\mathbf{y}=\mathbf{X}\mathbf{\beta}+\mathbf{\epsilon}$ , $y_{i}=\mathbf{x}_{i}^{T}\mathbf{\beta}+\mathbf{\epsilon}_{i}$ 과 같다하자
		-  $i^{th}$ 관측치에 대한 leverage 스코어는 $h_{ii}=[\mathbf{H}] _{ii}=\mathbf{x} _{i}(\mathbf{X^{T}X}) ^{-1}\mathbf{x} _{i}^T$이다 
			- 이때 $\mathbf{H}=\mathbf{X}\mathbf{(X^{T}X) ^{-1}\mathbf{X}^T}$ 는 Ortho-projection matrix이며 영향도 행렬 influence matrix 또는 hat matrix라고 부른다
		- $h_{ii}=\cfrac{\partial {\hat{y}_i}}{\partial {y_{i}}}$
			- (${\mathbf{\hat{y}}=\mathbf{H}\mathbf{y}}$ 는 $\mathbf{X}$ 의 range 공간에 대한 ortho-projection이다.)

### 5. 공선성
- 공선성 Colinearity는 독립 변수 두 개가 서로 선형 관계를 갖고 있는 상태를 의미한다
	- $x_j=\lambda_{0}+\lambda_{1}x_{k}$ for $j \neq{k}$  
- 다중공선성은Multicolinearity 2개 이상의 설명변수가 선형 관계를 갖고 있는 상태를 의미한다
	- $\lambda_{0}+\lambda_{1}x_{1}+\lambda_{2}x_{2}+...+\lambda_{k}x_{k}=0$
- 공선성의 문제
	- 수학적 추론
		- $\mathbf{y}=\mathbf{X}\mathbf{\beta}+\mathbf{\epsilon}$ 이라고 하자 이때 
		- $\mathbf{X}=\begin{bmatrix} 1 & x_{11} & x_{12} & ... & x_{1k} \\ \cdots  & \cdots  &&& \cdots \\ 1 & x_{n1} & \cdots  && x_{nk}  \end{bmatrix}$ 이라 하자
		- 만약 다중공선성의 관계가 존재한다면 $rank(\mathbf{X})<{k+1}$ 이고,  $rank(\mathbf{X^{T}X})$ 는 비가역적이다
		- $\hat{\beta}=(\mathbf{X^{T}X})^{-1}\mathbf{X^T}\mathbf{y}$ 는 비가역적인 $\mathbf{X^{T}X}$ 의 존재로 인해 사용할 수 없게 된다
	- 공선성이 존재하는 변수는 개별 효과를 분리하여 해석하기 어렵다.
	- 공선성은 회귀계수 추정의 정확도를 감소시킨다
- 공선성의 문제해결법
	- 공선성을 갖는 변수 중 하나를 삭제한다. 삭제되는 변수가 제공하는 정보가 다른 변수가 제공하는 정보와 중복되어 있기 때문에, 일반적으로 회귀 적합도를 크게 헤치지 않는다
	- 공선성을 갖는 변수들을 하나의 단일 예측 변수로 결합시킨다